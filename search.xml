<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>使用wfdb来处理心电信号</title>
      <link href="/2019/01/07/%E4%BD%BF%E7%94%A8wfdb%E6%9D%A5%E5%A4%84%E7%90%86%E5%BF%83%E7%94%B5%E4%BF%A1%E5%8F%B7/"/>
      <url>/2019/01/07/%E4%BD%BF%E7%94%A8wfdb%E6%9D%A5%E5%A4%84%E7%90%86%E5%BF%83%E7%94%B5%E4%BF%A1%E5%8F%B7/</url>
      <content type="html"><![CDATA[<p>涉及到项目组设备的更新，近期在探究通过分析短时段心电信号来预测病人的病型，之后成功的话会把成果放上来。<br>因为公开的心电信号数据集中多使用.data的数据格式，一般来说需要matlab处理，但是这样的话不利于程序的一致性和整体性，<br>后来听马组长说有一个python module可以用来进行处理，所以今天记录下wfdb来处理心电信号。(注：使用的数据均为physionet 2017)</p>]]></content>
      
      <categories>
          
          <category> 信号处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号处理 </tag>
            
            <tag> 慢病管理 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2019冲鸭~</title>
      <link href="/2019/01/01/2019%E5%86%B2%E9%B8%AD/"/>
      <url>/2019/01/01/2019%E5%86%B2%E9%B8%AD/</url>
      <content type="html"><![CDATA[<p>2018年起起伏伏，总算有一个不错的结局。2019不多说，冲鸭~</p>]]></content>
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>用PaddlePaddle实现目标检测任务——Paddle Fluid v1.1深度测评</title>
      <link href="/2018/12/12/%E7%94%A8PaddlePaddle%E5%AE%9E%E7%8E%B0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1%E2%80%94%E2%80%94Paddle-Fluid-v1-1%E6%B7%B1%E5%BA%A6%E6%B5%8B%E8%AF%84/"/>
      <url>/2018/12/12/%E7%94%A8PaddlePaddle%E5%AE%9E%E7%8E%B0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1%E2%80%94%E2%80%94Paddle-Fluid-v1-1%E6%B7%B1%E5%BA%A6%E6%B5%8B%E8%AF%84/</url>
      <content type="html"><![CDATA[<p>近期师兄叫我一起评测百度之前发布的PaddlePaddle深度学习框架，我一起划了划水，评测结果如下。</p><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>11月1日，百度发布了Paddle Fluid的1.1版本，作为国内首个深度学习框架，PaddlePaddle对中文社区非常友好，有完善的中文社区、项目为导向的中文教程，<br>可以让更多中文使用者更方便地进行深度学习、机器学习相关的研究和实践。我本人也非常希望PaddlePaddle能够不断发展壮大，毕竟这是国内公司为开源社区做出的一项非常有意义的贡献。<br>为了一探Paddle Fluid 1.1版本究竟做了哪些方面的更新，笔者第一时间安装了新发布的版本，用一个基于SSD的目标检测任务来测试一下新版PaddlePaddle的表现。<br><a id="more"></a></p><h1 id="2-什么是目标检测"><a href="#2-什么是目标检测" class="headerlink" title="2. 什么是目标检测"></a>2. 什么是目标检测</h1><p>图像识别对于做视觉的同学来说应该是一个非常熟悉的任务了，最初深度学习就是是应用于图像识别任务的，举例来说，给计算机一张汽车图片，让它判断这图片里有没有汽车。</p><p><img src="/img/paddle-car.jpg" alt=""></p><p>对于背景干净的图片来说，这样做很有意义也比较容易。但是如果是一张包含丰富元素的图片，不仅识别难度大大提高，仅仅判断出里面有没有图片的意义也不大了，<br>我们需要找到到底在读片的什么位置出现了一辆汽车，这就提出了一个新的任务和需求——目标检测。</p><p><img src="/img/paddle-car1.jpg" alt=""></p><p>我们的任务就是给定一张图像或是一个视频帧，让计算机找出其中所有目标的位置，并给出每个目标的具体类别。对于人类来说，目标检测是一个非常简单的任务。然而，<br>计算机能够“看到”的是图像被编码之后的数字，很难解图像或是视频帧中出现了人或是物体这样的高层语义概念，也就更加难以定位目标出现在图像中哪个区域。</p><p>与此同时，由于目标会出现在图像或是视频帧中的任何位置，目标的形态千变万化，图像或是视频帧的背景千差万别，诸多因素都使得目标检测对计算机来说是一个具有挑战性的问题。<br>目前主流的方法是FasterRCNN、YOLO和SSD，本文使用SSD进行实验。</p><h1 id="3-PaddlePaddle简介"><a href="#3-PaddlePaddle简介" class="headerlink" title="3. PaddlePaddle简介"></a>3. PaddlePaddle简介</h1><p>第一次听到PaddlePaddle是在CCF前线研讨会上，当时几个人聊起来关于机器学习算法平台的事情，有一位小伙伴提起了这个名字，所以一段时间以来我一直认为这是一个机器学习算法平台。<br>直到16年百度开源了PaddlePaddle我才知道，原来这是一个可以跟TensorFlow媲美的深度学习框架，主打“易用、高效、灵活、可扩展”。所以，简单来说，PaddlePaddle就是百度自研的一套深度学习框架(看过发布会后了解到，百度为此建立了一套覆盖面非常广的生态，包括金融、推荐、决策等，但笔者主要是对PaddlePaddle的核心框架进行测评，不在此浪费过多笔墨了)。</p><h2 id="3-1-如何安装"><a href="#3-1-如何安装" class="headerlink" title="3.1 如何安装"></a>3.1 如何安装</h2><p>笔者的工作站是Ubuntu 16.04系统，PaddlePaddle在CentOS和Ubuntu都支持pip安装和docker安装，GPU版本在Linux下也可以完美适配。下面来看一下具体的安装步骤。</p><p>首先我们使用cat /proc/cpuinfo | grep avx2来查看我们的Ubuntu系统是否支持avx2指令集，如果发现系统返回了如下一系列信息，就说明系统是支持avx2指令集的，可以放心进行后续安装。如果不支持也没关系，在官网上可以直接下载no_avx的whl包进行安装。</p><p><img src="/img/paddle_2.jpg" alt=""></p><p>接下来使用pip安装最新的Fluid v1.1版本的PaddlePaddle(GPU)，在安装前注意，需要在机器上安装python3.5-dev才可以用pip安装PaddlePaddle。下载速度会比较慢，需要20分钟左右的下载时间。</p><p>安装完成后，在python里import paddle测试一下，如果成功导入则说明安装成功！</p><p>在更新的Paddle Fluid v1.1版本中还特意优化了对MacOS的支持，可以直接通过pip安装，也可以用源码编译安装。具体细节可参考：<a href="http://www.paddlepaddle.org/documentation/docs/zh/1.1/beginners_guide/install/Start.html" target="_blank" rel="noopener">paddlepaddle</a></p><h2 id="3-2-PaddlePaddle的计算描述方式"><a href="#3-2-PaddlePaddle的计算描述方式" class="headerlink" title="3.2 PaddlePaddle的计算描述方式"></a>3.2 PaddlePaddle的计算描述方式</h2><p>框架的计算描述方式是深度学习项目开发者非常关注的一个问题。计算的描述方式经历了从Caffe1.0时代的一组连续执行的layers到TensorFlow的变量和操作构成的计算图再到PaddlePaddle Fluid<sup>[1]</sup>提出不再有模型的概念一系列的演变。<br>那么PaddlePaddle现在是怎么描述计算的呢？</p><p>PaddlePaddle使用Program来描述模型和优化过程，可以把它简单理解为数据流的控制过程。Program由Block、Operator和Variable构成，variable和operator被组织成为多个可以嵌套的block。<br>具体的，如果要实现一个神经网络，我们只需要通过添加必要的variable、operator来定义网络的前向计算，而反向计算、内存管理、block创建都由框架来完成。下面展示一下如何在PaddlePaddle中定义program：</p><p>以一个简单的线性回归为例，我们这样定义前向计算逻辑：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义输入数据类型</span></span><br><span class="line">x = fluid.layers.data(name=<span class="string">"x"</span>,shape=[<span class="number">1</span>],dtype=<span class="string">'float32'</span>)</span><br><span class="line"><span class="comment">#搭建全连接网络</span></span><br><span class="line">y_predict = fluid.layers.fc(input=x,size=<span class="number">1</span>,act=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p><p>定义好计算逻辑后，与TensorFlow一样，下一步就需要定义损失函数，feed数据，开始训练，feed数据也是在执行运算的时候进行，我们先定义一下数据，这里train_data 就是我们的输入数据，y_true是label：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data=numpy.array([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>],[<span class="number">4.0</span>]]).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_true = numpy.array([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>],[<span class="number">8.0</span>]]).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure></p><p>添加均方误差损失函数(MSE)，框架会自动完成反向计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cost = fluid.layers.square_error_cost(input=y_predict,label=y)</span><br><span class="line">avg_cost = fluid.layers.mean(cost)</span><br></pre></td></tr></table></figure></p><p>执行我们定义的上述Program：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cpu = fluid.core.CPUPlace()</span><br><span class="line">exe = fluid.Executor(cpu)</span><br><span class="line">exe.run(fluid.default_startup_program())</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line">outs = exe.run(feed=&#123;<span class="string">'x'</span>:train_data,<span class="string">'y'</span>:y_true&#125;,</span><br><span class="line">               fetch_list=[y_predict.name,avg_cost.name])</span><br><span class="line"></span><br><span class="line"><span class="comment">#观察结果</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> outs</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[array([[<span class="number">0.9010564</span>],</span><br><span class="line">[<span class="number">1.8021128</span>], </span><br><span class="line">[<span class="number">2.7031693</span>],</span><br><span class="line">[<span class="number">3.6042256</span>]], dtype=float32), array([<span class="number">9.057577</span>], dtype=float32)]</span><br></pre></td></tr></table></figure></p><p>这样就用PaddlePaddle实现了简单的计算流程，个人感觉使用起来跟TensorFlow的相似度较高，习惯在TensorFlow上跑模型的小伙伴应该很容易适应PaddlePaddle的这一套生态。</p><p>关于PaddlePaddle计算描述的详情可以参考Fluid编程指南：<a href="http://www.paddlepaddle.org/documentation/docs/zh/1.1/beginners_guide/programming_guide/programming_guide.html" target="_blank" rel="noopener">fluid</a></p><h2 id="3-3-PaddlePaddle的模型库简介"><a href="#3-3-PaddlePaddle的模型库简介" class="headerlink" title="3.3 PaddlePaddle的模型库简介"></a>3.3 PaddlePaddle的模型库简介</h2><p>PaddlePaddle的核心框架内置了非常多的经典模型和网络，涵盖了几乎所有主流的机器学习/深度学习任务，包括图像、语音、自然语言处理、推荐等诸多方面。<br>因为本文是做目标检测，所以主要调研了一下图像方面的模型库，在此大致介绍一下。</p><h3 id="3-3-1-分类"><a href="#3-3-1-分类" class="headerlink" title="3.3.1 分类"></a>3.3.1 分类</h3><p>分类任务中的模型库是最全面的，AlexNet、VGG、GoogleNet、ResNet、Inception、MobileNet、Dual Path Network以及SE-ResNeXt，2012年以来的经典图像识别网络都包含其中，每个网络模型是一个独立的py文件，里面是这个网络模型的类，类里面公用的方法是net()，在调用时初始化对应的类之后调用.net()方法，就可以得到对应网络的Program描述，<br>之后只需要给网络feed数据、定义损失函数、优化方法等就可以轻松使用了。分类模型作为图像任务的基础任务，在目标检测、语义分割等任务中都会重复利用这些模型，<br>所以这样一个模型库可以为大大简化后续任务的开发工作。这部分的模型库里的写法比较统一，只要了解网络结构，用.net()方法调用就可以，这里就不一一介绍了，具体可以参考：<a href="https://github.com/PaddlePaddle/models/tree/develop/fluid/PaddleCV/image_classification/models" target="_blank" rel="noopener">PaddleCV</a></p><h3 id="3-3-2-目标检测"><a href="#3-3-2-目标检测" class="headerlink" title="3.3.2 目标检测"></a>3.3.2 目标检测</h3><p><strong>SSD</strong></p><p>Single Shot MultiBox Detector (SSD) 是一种单阶段的目标检测器。与两阶段的检测方法不同，单阶段目标检测并不进行区域推荐，而是直接从特征图回归出目标的边界框和分类概率。<br>SSD 运用了这种单阶段检测的思想，并且对其进行改进：在不同尺度的特征图上检测对应尺度的目标。如下图所示，SSD 在六个尺度的特征图上进行了不同层级的预测。每个层级由两个3x3卷积分别对目标类别和边界框偏移进行回归。因此对于每个类别，SSD 的六个层级一共会产生 38x38x4 + 19x19x6 + 10x10x6 + 5x5x6 + 3x3x4 + 1x1x4 = 8732 个检测结果。</p><p><img src="/img/paddle_3.jpg" alt="SSD目标检测模型"></p><p>SSD 可以方便地插入到任何一种标准卷积网络中，比如 VGG、ResNet 或者 MobileNet，这些网络被称作检测器的基网络。PaddlePaddle里的SSD使用Google的MobileNet作为基网络。</p><p>目标检测模型库不同于分类模型库，PaddlePaddle是以一个工程的形式提供SSD的模型库。工程里面包含如下文件：</p><p><img src="/img/paddle_4.jpg" alt=""></p><p>其中，train.py、reader.py、mobilenet_ssd.py是与网络训练相关的文件，包括数据读取、网络结构、训练参数等过程的定义都在这3个文件中；eval.py、eval_coco_map.py是网络预测评估相关文件；<br>infer.py是可视化预测结果相关文件。Data文件夹用于存储数据集，使用时可以把训练集、测试集、验证集放在data目录下，reader会在data目录下寻找图片数据加载；pretrained目录存放预训练模型，如果不想从头训练一个SSD，可以把预训练好的模型放在这个目录下，方便进行迁移学习。</p><h1 id="4-PaddlePaddle实现SSD的目标检测"><a href="#4-PaddlePaddle实现SSD的目标检测" class="headerlink" title="4. PaddlePaddle实现SSD的目标检测"></a>4. PaddlePaddle实现SSD的目标检测</h1><p>有了上述的一些基础，我们就可以轻松使用PaddlePaddle上手一些项目了。现在我们就来实现一个基于SSD的目标检测任务。</p><h2 id="4-1-服务器配置"><a href="#4-1-服务器配置" class="headerlink" title="4.1 服务器配置"></a>4.1 服务器配置</h2><p>系统：Ubuntu 16.04</p><p>GPU：NVIDIA GTX 1080*4 </p><p>环境：python3.5</p><h2 id="4-2-框架配置"><a href="#4-2-框架配置" class="headerlink" title="4.2 框架配置"></a>4.2 框架配置</h2><p>Paddle Fluid v1.1 GPU版本</p><h2 id="4-3-数据准备"><a href="#4-3-数据准备" class="headerlink" title="4.3 数据准备"></a>4.3 数据准备</h2><p>我们使用微软的COCO2017数据集来预训练模型(PaddlePaddle提供了一个基于COCO的预训练模型，可以直接使用)，COCO数据集是微软团队获取的一个可以用来图像recognition+segmentation+captioning 数据集，其官方说明网址：<a href="http://mscoco.org/" target="_blank" rel="noopener">COCO</a>。微软在ECCV Workshops里发表文章《Microsoft COCO: Common Objects in Context》更充分地介绍了该数据集。COCO以场景理解为目标，从复杂场景中截取了28,000张影像，包括了91类目标和2,500,000个label。整个COCO2017数据集20G，官网下载非常慢，可以在国内找一些镜像站下载，数据集里分好了训练集、测试集和验证集，标注和file_list用json文件保存。</p><p><img src="/img/paddle_5.jpg" alt=""></p><p>拿到预训练数据集后，我们在Pascal VOC数据集上对模型进行进一步训练，做一下微调。Pascal VOC数据集相较COCO数据集来说图片数量和种类小很多，共计20类，11540张训练图片，标注采用xml格式文件保存。</p><h2 id="4-4-数据读取"><a href="#4-4-数据读取" class="headerlink" title="4.4 数据读取"></a>4.4 数据读取</h2><p>图片格式为jpg，需要对图像进行转码读取，SSD中的reader.py文件帮助我们实现了这个功能，内置的数据读取使用了一个生成器来逐个batch读取图片并转码，这样内存占用率非常低。由于我们机器内存不大，设置的batch为32，在此情况下load十万张图片的annotation只需要17秒左右，每一个batch的load+train时间只需要0.3秒左右。</p><p><img src="/img/paddle_6.jpg" alt=""></p><p>可以看一下这个reader的核心代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reader</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span> <span class="keyword">and</span> shuffle:</span><br><span class="line">        np.random.shuffle(images)</span><br><span class="line">    batch_out = []</span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> images:</span><br><span class="line">        image_name = image[<span class="string">'file_name'</span>]</span><br><span class="line">        image_path = os.path.join(settings.data_dir, image_name)</span><br><span class="line"></span><br><span class="line">        im = Image.open(image_path)</span><br><span class="line">        <span class="keyword">if</span> im.mode == <span class="string">'L'</span>:</span><br><span class="line">            im = im.convert(<span class="string">'RGB'</span>)</span><br><span class="line">        im_width, im_height = im.size</span><br><span class="line">        im_id = image[<span class="string">'id'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layout: category_id | xmin | ymin | xmax | ymax | iscrowd</span></span><br><span class="line">        bbox_labels = []</span><br><span class="line">        annIds = coco.getAnnIds(imgIds=image[<span class="string">'id'</span>])</span><br><span class="line">        anns = coco.loadAnns(annIds)</span><br><span class="line">        <span class="keyword">for</span> ann <span class="keyword">in</span> anns:</span><br><span class="line">            bbox_sample = []</span><br><span class="line">            <span class="comment"># start from 1, leave 0 to background</span></span><br><span class="line">            bbox_sample.append(float(ann[<span class="string">'category_id'</span>]))</span><br><span class="line">            bbox = ann[<span class="string">'bbox'</span>]</span><br><span class="line">            xmin, ymin, w, h = bbox</span><br><span class="line">            xmax = xmin + w</span><br><span class="line">            ymax = ymin + h</span><br><span class="line">            bbox_sample.append(float(xmin) / im_width)</span><br><span class="line">            bbox_sample.append(float(ymin) / im_height)</span><br><span class="line">            bbox_sample.append(float(xmax) / im_width)</span><br><span class="line">            bbox_sample.append(float(ymax) / im_height)</span><br><span class="line">            bbox_sample.append(float(ann[<span class="string">'iscrowd'</span>]))</span><br><span class="line">            bbox_labels.append(bbox_sample)</span><br><span class="line">        im, sample_labels = preprocess(im, bbox_labels, mode, settings)</span><br><span class="line">        sample_labels = np.array(sample_labels)</span><br><span class="line">        <span class="keyword">if</span> len(sample_labels) == <span class="number">0</span>: <span class="keyword">continue</span></span><br><span class="line">        im = im.astype(<span class="string">'float32'</span>)</span><br><span class="line">        boxes = sample_labels[:, <span class="number">1</span>:<span class="number">5</span>]</span><br><span class="line">        lbls = sample_labels[:, <span class="number">0</span>].astype(<span class="string">'int32'</span>)</span><br><span class="line">        iscrowd = sample_labels[:, <span class="number">-1</span>].astype(<span class="string">'int32'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'cocoMAP'</span> <span class="keyword">in</span> settings.ap_version:</span><br><span class="line">            batch_out.append((im, boxes, lbls, iscrowd,</span><br><span class="line">                              [im_id, im_width, im_height]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch_out.append((im, boxes, lbls, iscrowd))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(batch_out) == batch_size:</span><br><span class="line">            <span class="keyword">yield</span> batch_out</span><br><span class="line">            batch_out = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'test'</span> <span class="keyword">and</span> len(batch_out) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">yield</span> batch_out</span><br><span class="line">        batch_out = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure></p><p>可以看到，这里的reader是一个生成器，逐个batch把数据load进内存。在数据读取过程中，需要注意一下几点：</p><ol><li>数据集需要放在项目的data目录下，reader通过annotations下的instances_train2017.json文件区分训练集和验证集，不需要在data目录下用文件夹区分训练集和验证集。</li><li>如果数据没有按要求保存，则需要在reader.py中修改数据路径</li><li>如果遇到NoneType is not iterable的错误，一般是由于数据读取错误导致的，仔细检查文件路径应该可以解决。</li><li>读取PascalVOC数据集用reader.py文件中的pascalvoc()函数，两个数据集的文件结构和标注不太一样，Paddle为我们写好了两个版本数据集的读取方法，可以直接调用。</li></ol><h2 id="4-5-模型训练"><a href="#4-5-模型训练" class="headerlink" title="4.5 模型训练"></a>4.5 模型训练</h2><p>数据读取完成后，就可以着手开始模型的训练了，这里直接使用PaddlePaddle SSD model里面的train.py进行训练：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -u train.py</span><br></pre></td></tr></table></figure></p><p>train.py里为所有的超参数都设置了缺省值，不熟悉PaddlePaddle参数调整的工程师可以直接用缺省参数进行训练，非常方便。如果需要，可以根据下表进行对应超参数的修改：</p><table><thead><tr><th style="text-align:center">参数名</th><th style="text-align:center">类型</th><th style="text-align:center">意义</th></tr></thead><tbody><tr><td style="text-align:center">learning_rate</td><td style="text-align:center">Float</td><td style="text-align:center">学习率</td></tr><tr><td style="text-align:center">batch_size</td><td style="text-align:center">Int</td><td style="text-align:center">Batch大小</td></tr><tr><td style="text-align:center">epoc_num</td><td style="text-align:center">Int</td><td style="text-align:center">迭代次数</td></tr><tr><td style="text-align:center">use_gpu</td><td style="text-align:center">Bool</td><td style="text-align:center">是否使用GPU训练</td></tr><tr><td style="text-align:center">parallel</td><td style="text-align:center">Bool</td><td style="text-align:center">是否使用多卡</td></tr><tr><td style="text-align:center">dataset</td><td style="text-align:center">Str</td><td style="text-align:center">数据集名称</td></tr><tr><td style="text-align:center">model_save_dir</td><td style="text-align:center">Str</td><td style="text-align:center">模型保存路径</td></tr><tr><td style="text-align:center">pretrained_model</td><td style="text-align:center">Str</td><td style="text-align:center">预训练模型路径(如果使用)</td></tr><tr><td style="text-align:center">image_shape</td><td style="text-align:center">Str</td><td style="text-align:center">输入图片尺寸</td></tr><tr><td style="text-align:center">data_dir</td><td style="text-align:center">Str</td><td style="text-align:center">数据集路径</td></tr></tbody></table><p>在执行脚本时，传入相应的参数值即可，例如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python -u train.py --batch_size=16 \</span><br><span class="line">                   --epoc_num=1 \</span><br><span class="line">                   --dataset=<span class="string">'pascalvoc'</span> \</span><br><span class="line">                   --pretrained_model=<span class="string">'pretrain/ssd_mobilenet_v1_coco/'</span></span><br></pre></td></tr></table></figure></p><p><img src="/img/paddle_7.jpg" alt=""></p><h3 id="4-5-1-单机多卡配置"><a href="#4-5-1-单机多卡配置" class="headerlink" title="4.5.1 单机多卡配置"></a>4.5.1 单机多卡配置</h3><p>单机多卡的配置相较于多机多卡配置较为简单，参数需要先在GPU0上初始化，再经由fluid.ParallelExecutor() 分发到多张显卡上。<br>这里可以使用fluid.core.get_cuda_device_count()得到可用显卡数量，也可以自己定义用几张显卡。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_exe = fluid.ParallelExecutor(use_cuda=<span class="keyword">True</span>, loss_name=loss.name,</span><br><span class="line">                                   main_program=fluid.default_main_program())</span><br><span class="line">train_exe.run(fetch_list=[loss.name], feed=&#123;...&#125;)</span><br></pre></td></tr></table></figure></p><h3 id="4-5-2-参数调整"><a href="#4-5-2-参数调整" class="headerlink" title="4.5.2 参数调整"></a>4.5.2 参数调整</h3><p>PaddlePaddle这一套SSD模型给了使用者非常大的自由度，可以对网络结构、损失函数、优化方法等多个角度对模型进行调整。本文采用的是基于MobileNet的SSD，如果想使用基于VGG的SSD，可以自己修改工程中的mobilenet_ssd.py文件，把里面定义的MobileNet Program更改为VGG的Program描述就可以了；如果需要修改损失函数或优化方法，则在train.py中找到build_program()函数，在<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> fluid.unique_name.guard(<span class="string">"train"</span>):</span><br><span class="line">    loss = fluid.layers.ssd_loss(locs, confs, gt_box, gt_label, box,</span><br><span class="line">                                 box_var)</span><br><span class="line">    loss = fluid.layers.reduce_sum(loss)</span><br><span class="line">    optimizer = optimizer_setting(train_params)</span><br><span class="line">    optimizer.minimize(loss)</span><br></pre></td></tr></table></figure></p><p>里修改损失函数或优化器即可；修改batch_num、epoch_num、learning rate等参数可以直接在train.py传入参数中进行。</p><h3 id="4-5-3-模型保存"><a href="#4-5-3-模型保存" class="headerlink" title="4.5.3 模型保存"></a>4.5.3 模型保存</h3><p>模型在COCO数据集上训练完后，可以用fluid.io.save_persistables()方法将模型保存下来，我们实现了如下save_model()函数来将模型保存到指定路径。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(postfix, main_prog, model_path)</span>:</span></span><br><span class="line">    model_path = os.path.join(model_save_dir, postfix)</span><br><span class="line">    <span class="keyword">if</span> os.path.isdir(model_path):</span><br><span class="line">        shutil.rmtree(model_path)</span><br><span class="line">        print(<span class="string">'save models to %s'</span> % (model_path))</span><br><span class="line">        fluid.io.save_persistables(exe, model_path, main_program=main_prog)</span><br></pre></td></tr></table></figure></p><h3 id="4-5-4-继续训练"><a href="#4-5-4-继续训练" class="headerlink" title="4.5.4 继续训练"></a>4.5.4 继续训练</h3><p>训练过程有时候会被打断，只要每个过几个batch保存一下模型，我们就可以通过load_vars()方法来恢复已经保存的模型来继续训练或者用于预测。文中提到的这些API，<br>大家可以去PaddlePaddle的官网教程上进行更系统的学习和查看，PaddlePaddle提供了大量的中文文档和使用教程，对中文使用者可以说是非常友好的了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fluid.io.load_vars(exe, pretrained_model, main_program=train_prog, predicate=if_exist)</span><br></pre></td></tr></table></figure></p><h3 id="4-5-5-性能参数"><a href="#4-5-5-性能参数" class="headerlink" title="4.5.5 性能参数"></a>4.5.5 性能参数</h3><p>训练速度：在COCO2017数据集上单卡训练，迭代1个epoch耗时3 min33s；单机4卡训练，迭代1个epoch耗时1min02s。</p><p>CPU/GPU占用率：正常训练情况下CPU占用率在40%-60%之间，GPU占用率稳定在50%左右。</p><p><img src="/img/paddle_8.jpg" alt=""></p><p><img src="/img/paddle_9.jpg" alt="CPU/GPU使用情况"></p><h2 id="4-6-模型评估"><a href="#4-6-模型评估" class="headerlink" title="4.6 模型评估"></a>4.6 模型评估</h2><p>在PaddlePaddle的SSD模型中，可以使用eval.py脚本进行模型评估，可以选择11point、integral等方法来计算模型在验证集上的mAP。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python eval.py --dataset=<span class="string">'pascalvoc'</span> \</span><br><span class="line">               --model_dir=<span class="string">'train_pascal_model/best_model'</span> \</span><br><span class="line">               --data_dir=<span class="string">'data/pascalvoc'</span> \</span><br><span class="line">               --test_list=<span class="string">'test.txt'</span> \</span><br><span class="line">               --ap_version=<span class="string">'11point'</span> \</span><br><span class="line">               --nms_threshold=0.45</span><br></pre></td></tr></table></figure></p><p>其中，model_dir是我们训练好的模型的保存目录，data_dir是数据集目录，test_list是作为验证集的文件列表(txt文件)，前提是这些文件必须要有对应的标签文件，ap_version是计算mAP的方法，nms_threshold是分类阈值。最后我们得到PaddlePaddle SSD模型在Pascal VOC数据集上的mAP为73.32%<sup>[2]</sup></p><table><thead><tr><th style="text-align:center">模型</th><th style="text-align:center">预训练模型</th><th style="text-align:center">训练数据</th><th style="text-align:center">测试数据</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">MobileNet-v1-SSD 300x300</td><td style="text-align:center">COCO SSD</td><td style="text-align:center">MobileNet VOC07+12 trainval</td><td style="text-align:center">VOC07 test</td><td style="text-align:center">73.32%</td></tr></tbody></table><h2 id="4-7-模型预测及可视化"><a href="#4-7-模型预测及可视化" class="headerlink" title="4.7 模型预测及可视化"></a>4.7 模型预测及可视化</h2><h3 id="4-7-1-模型预测"><a href="#4-7-1-模型预测" class="headerlink" title="4.7.1 模型预测"></a>4.7.1 模型预测</h3><p>模型训练完成后，用test_program = fluid.default_main_program().clone(for_test=True)将Program转换到test模式，然后把要预测的数据feed进Executor执行Program就可以计算得到图像的分类标签、<br>目标框的得分、xmin、ymin、xmax、ymax。具体过程如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># switch network to test mode (i.e. batch norm test mode)</span></span><br><span class="line">test_program = fluid.default_main_program().clone(for_test=<span class="keyword">True</span>)</span><br><span class="line">nmsed_out_v, = exe.run(test_program,</span><br><span class="line">                       feed=feeder.feed([[data]]),</span><br><span class="line">                       fetch_list=[nmsed_out],</span><br><span class="line">                       return_numpy=<span class="keyword">False</span>)</span><br><span class="line">nmsed_out_v = np.array(nmsed_out_v)</span><br></pre></td></tr></table></figure></p><h3 id="4-7-2-预测结果可视化"><a href="#4-7-2-预测结果可视化" class="headerlink" title="4.7.2 预测结果可视化"></a>4.7.2 预测结果可视化</h3><p>对于目标检测任务，我们通常需要对预测结果进行可视化进而获得对结果的感性认识。我们可以编写一个程序，让它在原图像上画出预测框，核心代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_bounding_box_on_image</span><span class="params">(image_path, nms_out, confs_threshold,</span></span></span><br><span class="line"><span class="function"><span class="params">                               label_list)</span>:</span></span><br><span class="line">    image = Image.open(image_path)</span><br><span class="line">    draw = ImageDraw.Draw(image)</span><br><span class="line">    im_width, im_height = image.size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> dt <span class="keyword">in</span> nms_out:</span><br><span class="line">        <span class="keyword">if</span> dt[<span class="number">1</span>] &lt; confs_threshold:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        category_id = dt[<span class="number">0</span>]</span><br><span class="line">        bbox = dt[<span class="number">2</span>:]</span><br><span class="line">        xmin, ymin, xmax, ymax = clip_bbox(dt[<span class="number">2</span>:])</span><br><span class="line">        (left, right, top, bottom) = (xmin * im_width, xmax * im_width,</span><br><span class="line">                                      ymin * im_height, ymax * im_height)</span><br><span class="line">        draw.line(</span><br><span class="line">            [(left, top), (left, bottom), (right, bottom), (right, top),</span><br><span class="line">             (left, top)],</span><br><span class="line">            width=<span class="number">4</span>,</span><br><span class="line">            fill=<span class="string">'red'</span>)</span><br><span class="line">        <span class="keyword">if</span> image.mode == <span class="string">'RGB'</span>:</span><br><span class="line">            draw.text((left, top), label_list[int(category_id)], (<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>))</span><br><span class="line">    image_name = image_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">    print(<span class="string">"image with bbox drawed saved as &#123;&#125;"</span>.format(image_name))</span><br><span class="line">    image.save(image_name)</span><br></pre></td></tr></table></figure></p><p>这样，我们可以很直观的看到预测结果：</p><p><img src="/img/paddle_10.jpg" alt=""></p><p>令人欣喜的是，PaddlePaddle的SSD模型中帮我们实现了完整的一套预测流程，我们可以直接运行SSD model下的infer.py脚本使用训练好的模型对图片进行预测：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python infer.py --dataset=<span class="string">'coco'</span> \</span><br><span class="line">                --nms_threshold=0.45 \</span><br><span class="line">                --model_dir=<span class="string">'pretrained/ssd_mobilenet_v1_coco'</span> \</span><br><span class="line">                --image_path=<span class="string">'./data/ pascalvoc/VOCdevkit/VOC2012/JPEGImages/2007_002216.jpg'</span></span><br></pre></td></tr></table></figure></p><h2 id="4-8-模型部署"><a href="#4-8-模型部署" class="headerlink" title="4.8 模型部署"></a>4.8 模型部署</h2><p>PaddlePaddle的模型部署需要先安装编译C++预测库，可以在<a href="http://www.paddlepaddle.org/documentation/docs/zh/1.1/user_guides/howto/inference/build_and_install_lib_cn.html" target="_blank" rel="noopener">C++</a>下载安装。<br>预测库中提供了Paddle的预测API，预测部署过程大致分为三个步骤：</p><ul><li>创建PaddlePredictor；</li><li>创建PaddleTensor传入PaddlePredictor中；</li><li>获取输出 PaddleTensor，输出结果。<br>这部分操作也并不复杂，而且Paddle的教程中也提供了一份部署详细代码参考，大家可以很快地利用这个模板完成模型部署<a href="https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/inference/api/demo_ci" target="_blank" rel="noopener">Paddle教程</a></li></ul><h1 id="5-使用感受"><a href="#5-使用感受" class="headerlink" title="5. 使用感受"></a>5. 使用感受</h1><ul><li>中文社区支持好</li></ul><p>在搭建SSD过程中，遇到了一些问题，例如segmentation fault、NoneType等，笔者直接在paddle的GitHub上提了相关issue，很快就得到了contributor的回复，问题很快得到了解决。</p><ul><li>教程完善</li></ul><p>PaddlePaddle的官网上提供了非常详尽的中英文教程，相较于之前学TensorFlow的时候经常看文档看半天才能理解其中的意思，PaddlePaddle对于中文使用者真是一大福音。</p><ul><li>相比较于TensorFlow，整体架构简明清晰，没有太多难以理解的概念</li><li>模型库丰富</li></ul><p>内置了CV、NLP、Recommendation等多种任务常用经典的模型，可以快速开发迭代AI产品。</p><ul><li>性能优越，生态完整</li></ul><p>从这次实验的结果来看，PaddlePaddle在性能上与TensorFlow等主流框架的性能差别不大，训练速度、CPU/GPU占用率等方面均表现优异，而且PaddlePaddle已经布局了一套完整的生态，前景非常好。</p><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h1><p>整体来说，PaddlePaddle是一个不错的框架。由于设计简洁加之文档、社区做的很好，非常容易上手，在使用过程中也没有非常难理解的概念，用fluid Program定义网络结构很方便，对于之前使用过TensorFlow的工程师来说可以比较快速的迁移到PaddlePaddle上。这次实验过程中，还是发现了一些PaddlePaddle的问题，训练过程如果意外终止，Paddle的训练任务并没有被完全kill掉，依然会占用CPU和GPU大量资源，内存和显存的管理还需要进一步的提高。不过，实验也证实了，正常情况下PaddlePaddle在SSD模型上的精度、速度等性能与TensorFlow差不多，在数据读取操作上比TensorFlow要更加简洁明了。</p><p>[1]. PaddlePaddle Fluid是2016年百度对原有PaddlePaddle的重构版本，如无特殊说明，本文中所述PaddlePaddle均指PaddlePaddle Fluid。</p><p>[2]. 此处引用了官方的评估结果，数据来源：<a href="https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/object_detection/README_cn.md#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0" target="_blank" rel="noopener">PaddleCV</a></p>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> PaddlePaddle </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>semantic-segmentation Implementation</title>
      <link href="/2018/11/23/semantic-segmentation-Implementation/"/>
      <url>/2018/11/23/semantic-segmentation-Implementation/</url>
      <content type="html"><![CDATA[<p>实习结束啦，终于有时间来整理下这几个月做的事情，主要是针对语义分割方向的一些模型复现和改进。<br><a id="more"></a></p><h1 id="Deeplab-v3"><a href="#Deeplab-v3" class="headerlink" title="Deeplab-v3"></a>Deeplab-v3</h1><p>原文地址：<a href="http://arxiv.org/abs/1706.05587" target="_blank" rel="noopener">DeepLabv3</a><br>代码：<a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener">deeplab</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>DeepLabv3结构采用了空洞卷积级联或不同采样率空洞卷积并行的架构，以解决多尺度下的目标分割问题。<br>此外，运用了ASPP(Atrous Spatial Pyramid Pooling)模块，它可以获取多个尺度上的卷积特征，从而进一步提升性能。</p><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p><img src="/img/deeplabv3-1.jpg" alt="summary"></p><ul><li>a. Image Pyramid：将输入图片放缩成不同比例，分别应用到DCNN上，将预测结果融合得到最终输出</li><li>b. Encoder-Decoder：利用Encoder阶段的多尺度特征，运用到Decoder阶段上恢复空间分辨率(代表工作有FCN、SegNet、PSPNet等工作)</li><li>c. Deeper w.Atroous Convolution：在原始模型的顶端增加额外的模块，例如DenseCRF，捕捉像素间长距离信息</li><li>d. Spatial Pyramid Pooling：空间金字塔池化具有不同采样率和多种视野的卷积核，能够以多尺度捕捉对象</li></ul><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p><img src="/img/deeplabv3-2.jpg" alt="deeplabv3架构"><br>Deeplab-v3用ResNet来当作主要提取特征网络，在block4的地方采用了ASPP模块(这个选择在论文中有论述，<br>分别对比了截取到不同的block的准确度，最终选择了block4)，当output_stride=16时，改进后的ASPP包括<br>一个1x1 convolution和三个3x3 convolutions，其中3x3 convolutions的atrous rates=(6,12,18)，(所有的filter个数为256，并加入batch normalization)当output_stride=8<br>时，rates将加倍。然后连接所有分支的最终特征，输入到另一个1x1 convolution(所有的filter个数也为256，然后进行batch normalization)，<br>再进入最终的1x1 convolution，得到logits结果。</p><h2 id="realization"><a href="#realization" class="headerlink" title="realization"></a>realization</h2><p>ASPP的模块如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atrous_spatial_pyramid_pooling</span><span class="params">(inputs, output_stride, batch_norm_decay, is_training, depth=<span class="number">256</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Atrous Spatial Pyramid Pooling.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    inputs: A tensor of size [batch, height, width, channels].</span></span><br><span class="line"><span class="string">    output_stride: The ResNet unit's stride. Determines the rates for atrous convolution.</span></span><br><span class="line"><span class="string">      the rates are (6, 12, 18) when the stride is 16, and doubled when 8.</span></span><br><span class="line"><span class="string">    batch_norm_decay: The moving average decay when estimating layer activation</span></span><br><span class="line"><span class="string">      statistics in batch normalization.</span></span><br><span class="line"><span class="string">    is_training: A boolean denoting whether the input is for training.</span></span><br><span class="line"><span class="string">    depth: The depth of the ResNet unit output.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    The atrous spatial pyramid pooling output.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"aspp"</span>):</span><br><span class="line">    <span class="keyword">if</span> output_stride <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">8</span>, <span class="number">16</span>]:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'output_stride must be either 8 or 16.'</span>)</span><br><span class="line"></span><br><span class="line">    atrous_rates = [<span class="number">6</span>, <span class="number">12</span>, <span class="number">18</span>]</span><br><span class="line">    <span class="keyword">if</span> output_stride == <span class="number">8</span>:</span><br><span class="line">      atrous_rates = [<span class="number">2</span>*rate <span class="keyword">for</span> rate <span class="keyword">in</span> atrous_rates]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):</span><br><span class="line">      <span class="keyword">with</span> arg_scope([layers.batch_norm], is_training=is_training):</span><br><span class="line">        inputs_size = tf.shape(inputs)[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># (a) one 1x1 convolution and three 3x3 convolutions with rates = (6, 12, 18) when output stride = 16.</span></span><br><span class="line">        <span class="comment"># the rates are doubled when output stride = 8.</span></span><br><span class="line">        conv_1x1 = layers_lib.conv2d(inputs, depth, [<span class="number">1</span>, <span class="number">1</span>], stride=<span class="number">1</span>, scope=<span class="string">"conv_1x1"</span>)</span><br><span class="line">        conv_3x3_1 = resnet_utils.conv2d_same(inputs, depth, <span class="number">3</span>, stride=<span class="number">1</span>, rate=atrous_rates[<span class="number">0</span>], scope=<span class="string">'conv_3x3_1'</span>)</span><br><span class="line">        conv_3x3_2 = resnet_utils.conv2d_same(inputs, depth, <span class="number">3</span>, stride=<span class="number">1</span>, rate=atrous_rates[<span class="number">1</span>], scope=<span class="string">'conv_3x3_2'</span>)</span><br><span class="line">        conv_3x3_3 = resnet_utils.conv2d_same(inputs, depth, <span class="number">3</span>, stride=<span class="number">1</span>, rate=atrous_rates[<span class="number">2</span>], scope=<span class="string">'conv_3x3_3'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (b) the image-level features</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"image_level_features"</span>):</span><br><span class="line">          <span class="comment"># global average pooling</span></span><br><span class="line">          image_level_features = tf.reduce_mean(inputs, [<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">'global_average_pooling'</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">          <span class="comment"># 1x1 convolution with 256 filters( and batch normalization)</span></span><br><span class="line">          image_level_features = layers_lib.conv2d(image_level_features, depth, [<span class="number">1</span>, <span class="number">1</span>], stride=<span class="number">1</span>, scope=<span class="string">'conv_1x1'</span>)</span><br><span class="line">          <span class="comment"># bilinearly upsample features</span></span><br><span class="line">          image_level_features = tf.image.resize_bilinear(image_level_features, inputs_size, name=<span class="string">'upsample'</span>)</span><br><span class="line"></span><br><span class="line">        net = tf.concat([conv_1x1, conv_3x3_1, conv_3x3_2, conv_3x3_3, image_level_features], axis=<span class="number">3</span>, name=<span class="string">'concat'</span>)</span><br><span class="line">        net = layers_lib.conv2d(net, depth, [<span class="number">1</span>, <span class="number">1</span>], stride=<span class="number">1</span>, scope=<span class="string">'conv_1x1_concat'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><h2 id="pretrained-on-MS-COCO"><a href="#pretrained-on-MS-COCO" class="headerlink" title="pretrained on MS-COCO"></a>pretrained on MS-COCO</h2><p>paper上提到了pretrained on MS-COCO的结果，但是我看了下github上好像没有类似实现，所以自己实现了下。</p><p>在准备数据集的时候遇到了很多坑，具体的解决放在上一篇<a href="https://www.wuxiaochun.cn/2018/09/17/MS-COCO-dataset/">post</a>里面了.</p><p>后来训练了模型，训练的时候正确率突然就增加到mIOU=1.0，pixel accuracy=1.0，evaluate的时候发现所有的分类都是underground，<br>采用自己的图片发现生成图片为全黑，于是对比MS-COCO和VOC的label，发现是准备数据集的时候出了差错，提取成了stuff segmentation的label，遂纠正。<br>先是训练了coco数据集中label为PASCAL VOC那20类中的图片，大概有4w张，训练出来的mIOU为69.11，<br>然后又训练了所有coco数据集中的图片，然后单单计算pascal那20类的得分，得到的mIOU为56.34<br>具体得分和效果图如下：</p><table><thead><tr><th style="text-align:center">assessment criteria</th><th style="text-align:center">Deeplabv3_pascal_trainaug</th><th style="text-align:center">Deeplabv3_coco20_train</th><th style="text-align:center">Deeplabv3_coco90_train</th></tr></thead><tbody><tr><td style="text-align:center">mIOU</td><td style="text-align:center">76.42</td><td style="text-align:center">69.11</td><td style="text-align:center">56.34</td></tr><tr><td style="text-align:center">Pixel accuracy</td><td style="text-align:center">94.45</td><td style="text-align:center">95.31</td><td style="text-align:center">86.59</td></tr><tr><td style="text-align:center">test_mIOU</td><td style="text-align:center">75.72</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr></tbody></table><p><img src="/img/示意图.png" alt="从左到右分别是原图、groundtruth、deeplabv3_pascal、deeplabv3_coco20"></p><h1 id="Deeplab-v3-1"><a href="#Deeplab-v3-1" class="headerlink" title="Deeplab-v3+"></a>Deeplab-v3+</h1><h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="/img/deeplabv3+-1.jpg" alt="deeplabv3+"><br>Deeplab-v3+主要使用了一种新的编码-解码架构，然后检验了Xception作为backbone使用的效果</p><ul><li>(a). 即Deeplab-v3的结构，使用ASPP模块获取多尺度上下文信息，然后直接上采样得到预测结果</li><li>(b). encoder-decoder结构，高层特征提供语义，decoder逐步恢复边界信息</li><li>(c). Deeplab-v3+结构，以Deeplab-v3为encoder，decoder结构简单<br>然后该论文采用了更深的Xception结构，所有的最大池化操作替换成带下采样的深度分离卷积，改进后的Xception为encode网络主体，<br>替换原来的ResNet101，进一步提高模型的速度和性能。</li></ul><h2 id="Model-Architecture-1"><a href="#Model-Architecture-1" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p><img src="/img/deeplabv3+-2.jpg" alt="implementation"><br>可以从上图看出低级特征提供细节信息，高级特征提供语义信息。具体为什么要在图中位置融合，个人觉得是因为图片中的物体大小一般都占整个图片的1/4大小。</p><h2 id="realization-1"><a href="#realization-1" class="headerlink" title="realization"></a>realization</h2><p>这里我采用了官方放出的代码，官方代码比较晦涩，github地址为：<a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener">deeplab</a></p><p>在PASCAL VOC trainaug和trainval上分别进行训练，得分如下：</p><table><thead><tr><th style="text-align:center">assessment criteria</th><th style="text-align:center">Deeplabv3_pascal_trainaug</th><th style="text-align:center">Deeplabv3_coco20_train</th><th style="text-align:center">Deeplabv3_coco90_train</th><th style="text-align:center">Deeplabv3+_pascal_trainaug</th><th style="text-align:center">Deeplabv3+_pascal_trainval</th></tr></thead><tbody><tr><td style="text-align:center">mIOU</td><td style="text-align:center">76.42</td><td style="text-align:center">69.11</td><td style="text-align:center">56.34</td><td style="text-align:center">83.68</td><td style="text-align:center">93.58</td></tr><tr><td style="text-align:center">Pixel accuracy</td><td style="text-align:center">94.45</td><td style="text-align:center">95.31</td><td style="text-align:center">86.59</td><td style="text-align:center">96.65</td><td style="text-align:center">98.74</td></tr><tr><td style="text-align:center">test_mIOU</td><td style="text-align:center">75.72</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr></tbody></table><p>注：因为用trainval来进行训练的deeplabv3+存在数据泄露，所以得分不具有参考性</p><h1 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h1><p>在模型优化上面，我采用了stacking的方法。目前来说，有三种常见的集成学习框架：bagging，boosting和stacking。<br><img src="/img/bagging.jpg" alt="bagging"><br>bagging：从训练集进行抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果</p><p><img src="/img/boosting.jpg" alt="boosting"><br>boosting：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果</p><p><img src="/img/stacking.jpg" alt="stacking"><br>stacking：用所有数据训练基模型，将训练好的所有基模型对训练集进行预测，然后将训练集的预测值作为新的训练特征，label不变，再去训练一个比较强的模型，生成预测结果</p><p>已经完成的是用三种效果较好的模型生成的图片针对每一个pixel做一个投票，来生成最好的结果<br>更好的思路是将三个模型输出的logits作为输入来用一个简单的三层神经网络进行训练来生成最后的logits。</p><table><thead><tr><th style="text-align:center">assessment criteria</th><th style="text-align:center">Deeplabv3_pascal_trainaug</th><th style="text-align:center">Deeplabv3_coco20_train</th><th style="text-align:center">Deeplabv3_coco90_train</th><th style="text-align:center">Deeplabv3+_pascal_trainaug</th><th style="text-align:center">Deeplabv3+_pascal_trainval</th><th style="text-align:center">Stacking</th></tr></thead><tbody><tr><td style="text-align:center">mIOU</td><td style="text-align:center">76.42</td><td style="text-align:center">69.11</td><td style="text-align:center">56.34</td><td style="text-align:center">83.68</td><td style="text-align:center">93.58</td><td style="text-align:center">87.77</td></tr><tr><td style="text-align:center">Pixel accuracy</td><td style="text-align:center">94.45</td><td style="text-align:center">95.31</td><td style="text-align:center">86.59</td><td style="text-align:center">96.65</td><td style="text-align:center">98.74</td><td style="text-align:center">97.55</td></tr><tr><td style="text-align:center">test_mIOU</td><td style="text-align:center">75.72</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr></tbody></table><h2 id="DenseCRF"><a href="#DenseCRF" class="headerlink" title="DenseCRF"></a>DenseCRF</h2><p>DenseCRF(全连接条件随机场)，是在给定一组输入随机变量条件下另外一组输出随机变量的条件概率分布模型。全连接条件随机场模型能够将邻近结点耦合，有利于将相同标记分配给空间上接近的像素。定性的说，<br>这些短程条件随机场主函数会清楚构建在局部手动特征上弱分类器的错误预测。具体的原理介绍见这篇<a href="https://www.wuxiaochun.cn/2018/11/30/CRF/">post</a><br>由于时间原因作者只做了图片中单个物体的CRF处理，如图片中有多个物体则会分类为同个物体。效果图如下：<br><img src="/img/crf-1.png" alt="图1"><br><img src="/img/crf-2.png" alt="图2"><br><img src="/img/crf-3.png" alt="图3"><br>实现代码放github上：<a href="https://github.com/xiaochunWu/Semantic-Segmentation-CRF" target="_blank" rel="noopener">semantic-segmentation</a></p><h1 id="To-Be-Continued"><a href="#To-Be-Continued" class="headerlink" title="To Be Continued"></a>To Be Continued</h1><h2 id="Model-Level"><a href="#Model-Level" class="headerlink" title="Model Level"></a>Model Level</h2><ul><li>采用更多层次的融合，如果低级特征包含很少的语义信息，高级特征包含不够多的空间信息，那么只融合最低和最高的话效果不会达到最好。</li><li>我是在生成图片的基础上进行融合的，我觉得在logits的基础上融合的效果可能更好，而且可以详细设计下第二层的神经网络结构。</li><li>我觉得可以用两个模型来分别保留原图像的空间信息和语义信息，放一幅图来作为指引<br><img src="/img/continue.jpg" alt="头脑风暴"><h2 id="Data-Level"><a href="#Data-Level" class="headerlink" title="Data Level"></a>Data Level</h2></li><li>可以针对bad image和bad class增加一些图片和标注，主要是bicycle、chair、pottedplant、sofa类</li><li>可以对原训练集做更多augmentation的工作，比如锐化，模糊</li></ul><p>以上。</p>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> semantic-segmentation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>multi-gpu</title>
      <link href="/2018/11/01/multi-gpu/"/>
      <url>/2018/11/01/multi-gpu/</url>
      <content type="html"><![CDATA[<h1 id="multi-gpu-的原理"><a href="#multi-gpu-的原理" class="headerlink" title="multi-gpu 的原理"></a>multi-gpu 的原理</h1><p>数据并行的原理很简单，如下图，其中CPU主要负责梯度平均和参数更新，而GPU1和GPU2主要负责训练model replica。<br><a id="more"></a><br><img src="/img/multi-gpu.png" alt="训练步骤如图"></p><ol><li>在GPU1、GPU2上分别定义模型参数变量，网络结构</li><li>对于单独的GPU，分别从数据管道读取不同的数据块，然后做forward propagation来计算出loss，再计算在当前variables下的gradients</li><li>把所有GPU输出的梯度数据转移到CPU上，先进行梯度取平均操作，然后进行模型参数的更新</li><li>重复1-3，直到模型收敛</li></ol><p>在1中定义模型参数时，要考虑到不同model replica之间要能够share variables，因此要采用tf.get_variable()函数而不是直接tf.Variables()。另外，因为tensorflow和theano类似，都是先定义好tensor Graph，再基于已经定义好的Graph进行模型迭代式训练的。因此在每次迭代过程中，只会对当前的模型参数进行更新，而不会调用tf.get_variable()函数重新定义模型变量，因此变量共享只是存在于模型定义阶段的一个概念。</p><p>在实际使用过程中我发现其实不用在模型构建的时候考虑并行架构也可以实现multi-gpu，因为google有放出multi-gpu的api，详情见<a href="https://github.com/tensorflow/models/blob/master/research/slim/deployment/model_deploy.py" target="_blank" rel="noopener">multi-gpu</a></p><h1 id="Tensorflow切换使用CPU-GPU"><a href="#Tensorflow切换使用CPU-GPU" class="headerlink" title="Tensorflow切换使用CPU/GPU"></a>Tensorflow切换使用CPU/GPU</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto(device_count=&#123;<span class="string">'GPU'</span>: <span class="number">0</span>&#125;) <span class="comment"># 0表示使用CPU, 1则是GPU</span></span><br></pre></td></tr></table></figure><p>或者修改model_deploy.DeploymentConfig(clone_on_cpu=True)</p><h1 id="Tensorflow多GPU训练"><a href="#Tensorflow多GPU训练" class="headerlink" title="Tensorflow多GPU训练"></a>Tensorflow多GPU训练</h1><p>在/models/research/slim/depolyment/model_deploy.py中，有说明：</p><blockquote><p>DeploymentConfig parameters: </p><ul><li>num_clones: Number of model clones to deploy in each replica. </li><li>clone_on_cpu: True if clones should be placed on CPU. </li><li>replica_id: Integer. Index of the replica for which the model is deployed. Usually 0 for the chief replica. </li><li>num_replicas: Number of replicas to use. </li><li>num_ps_tasks: Number of tasks for the ps job. 0 to not use replicas. </li><li>worker_job_name: A name for the worker job. </li><li>ps_job_name: A name for the parameter server job.</li></ul></blockquote><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">config = model_deploy.DeploymentConfig(</span><br><span class="line">    num_clones=FLAGS.num_clones,</span><br><span class="line">    clone_on_cpu=FLAGS.clone_on_cpu,</span><br><span class="line">    replica_id=FLAGS.task,</span><br><span class="line">    num_replicas=FLAGS.num_replicas,</span><br><span class="line">    num_ps_tasks=FLAGS.num_ps_tasks)</span><br></pre></td></tr></table></figure><p>**在terminal还是要设置CUDA_VISIBLE_DEVICES=(),要不会占用剩下GPU的显存。</p>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 模型构建 </tag>
            
            <tag> tricks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>checkpoint</title>
      <link href="/2018/10/12/checkpoint/"/>
      <url>/2018/10/12/checkpoint/</url>
      <content type="html"><![CDATA[<h1 id="检查点"><a href="#检查点" class="headerlink" title="检查点"></a>检查点</h1><p>本文介绍如何保存和恢复编译有 Estimator 的 TensorFlow 模型。TensorFlow 提供两种模型格式：</p><ul><li>检查点（checkpoints）：这是一种依赖于创建模型代码的格式。</li><li>SavedModel：这是一种与创建模型代码无关的格式。<a id="more"></a><a href="//tensorflow.juejin.im/programmers_guide/saved_model.html">保存和恢复</a></li></ul><h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h2><p><a href="//tensorflow.juejin.im/get_started/premade_estimators.html">Premade Estimators</a></p><pre><code>git clone https://github.com/tensorflow/models/cd models/samples/core/get_started</code></pre><p>本文中大部分代码片断都是在 <code>premade_estimator.py</code> 基础上少量修改的版本。</p><h2 id="保存未训练完的模型"><a href="#保存未训练完的模型" class="headerlink" title="保存未训练完的模型"></a>保存未训练完的模型</h2><p>Estimators 自动将下列内容写到磁盘上：</p><ul><li><strong>检查点</strong>：训练过程中生成的不同版本的模型。</li><li><strong>事件文件</strong>：包含一些用于 <a href="https://developers.google.com/machine-learning/glossary/#TensorBoard" target="_blank" rel="noopener">TensorBoard</a> 可视化的信息</li></ul><p>为指定 Estimator 存储信息的顶层目录，将其赋值给任何一个 Estimator 的构造函数的可选参数 <code>model_dir</code>。比如 ，下列代码将 <code>model_dir</code> 参数设置为 <code>models/iris</code> 目录：</p><pre><code>classifier = tf.estimator.DNNClassifier(    feature_columns=my_feature_columns,    hidden_units=[10, 10],    n_classes=3,    model_dir=&apos;models/iris&apos;)</code></pre><p>假定你调用 Estimator 的 <code>train</code> 方法。比如：</p><pre><code>classifier.train(        input_fn=lambda:train_input_fn(train_x, train_y, batch_size=100),                steps=200)</code></pre><p>如下列图表所示，第一次调用 <code>train</code> 将检查点和其它文件添加到 <code>model_dir</code> 目录中：</p><p><img src="../images/first_train_calls.png" alt=""></p><p>第一次调用 train()。</p><p>在一个类 UNIX 系统中，可用命令 <code>ls</code> 来查看 <code>model_dir</code> 目录中的对象：</p><pre><code>$ ls -1 models/irischeckpointevents.out.tfevents.timestamp.hostnamegraph.pbtxtmodel.ckpt-1.data-00000-of-00001model.ckpt-1.indexmodel.ckpt-1.metamodel.ckpt-200.data-00000-of-00001model.ckpt-200.indexmodel.ckpt-200.meta</code></pre><p>上面的 <code>ls</code> 命令显示，此 Estimator 在第 1 步（训练开始时）和第 200 步（训练结束时）生成了检查点。</p><h3 id="默认检查点目录"><a href="#默认检查点目录" class="headerlink" title="默认检查点目录"></a>默认检查点目录</h3><p>如果你在一个 Estimator 构造函数中指定 <code>model_dir</code> 参数，此 Estimator 将检查点文件写到一个临时目录中，此目录由 Python 的 <a href="https://docs.python.org/3/library/tempfile.html#tempfile.mkdtemp" target="_blank" rel="noopener">tempfile.mkdtemp</a> 函数指定。比如，下面的 Estimator 构造函数并没有指定 <code>model_dir</code> 参数：</p><pre><code>classifier = tf.estimator.DNNClassifier(    feature_columns=my_feature_columns,    hidden_units=[10, 10],    n_classes=3)print(classifier.model_dir)</code></pre><p><code>tempfile.mkdtemp</code> 函数会为你在操作系统中选择一个安全的临时目录。比如，在 macOS 操作系统中，一个典型的临时目录为：</p><pre><code>/var/folders/0s/5q9kfzfj3gx2knj0vj8p68yc00dhcr/T/tmpYm1Rwa</code></pre><h3 id="检查点的保存频率"><a href="#检查点的保存频率" class="headerlink" title="检查点的保存频率"></a>检查点的保存频率</h3><p>默认情况下， Estimator 会在 <code>model_dir</code> 目录中保存 <a href="https://developers.google.com/machine-learning/glossary/#checkpoint" target="_blank" rel="noopener">检查点</a>，并且采用如下策略：</p><ul><li>每隔 10 分钟保存一个检查点（即 600 秒）。</li><li>当 <code>train</code> 方法开始执行（即第一次循环）和执行结束（最后一次循环）时，会各保存一个检查点。</li><li>保留目录中最近 5 个检查点。</li></ul><p>你可以用如下步骤改变上述默认策略：</p><ol><li><a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig" target="_blank" rel="noopener"><code>tf.estimator.RunConfig</code></a></li><li>当实例化 Estimator 时，将此 <code>RunConfig</code> 对象传递给 Estimator 的 <code>config</code> 参数。</li></ol><p>比如，下面的代码将检查点保存策略修改为每隔 20 分钟保存一次，且保留最近 10 个检查点：</p><pre><code>my_checkpointing_config = tf.estimator.RunConfig(    save_checkpoints_secs = 20*60,  # Save checkpoints every 20 minutes.    keep_checkpoint_max = 10,       # Retain the 10 most recent checkpoints.)classifier = tf.estimator.DNNClassifier(    feature_columns=my_feature_columns,    hidden_units=[10, 10],    n_classes=3,    model_dir=&apos;models/iris&apos;,    config=my_checkpointing_config)</code></pre><h2 id="恢复你的模型"><a href="#恢复你的模型" class="headerlink" title="恢复你的模型"></a>恢复你的模型</h2><p>当第一次调用一个 Estimator 的 <code>train</code> 方法时，TensorFlow 会在 <code>model_dir</code> 目录中保存一个检查点。后续每调用一次 Estimator 的 <code>train</code> 、 <code>evaluate</code> 或 <code>predict</code> 方法，都会发生如下的行为：</p><ol><li><a href="//tensorflow.juejin.im/get_started/custom_estimators.html">创建定制化 Estimator</a></li><li>这个 Estimator 从最近的检查点中恢复出数据，用于初始化新模型的权重值。</li></ol><p>换句话说，如下图所示，一旦检查点文件存在，TensorFlow 总会在你调用 <code>train()</code> 、 <code>evaluation()</code> 或 <code>predict()</code> 时重建模型。</p><p><img src="../images/subsequent_calls.png" alt=""></p><p>后续对 train()、evaluate() 或 predict() 的调用</p><h3 id="避免不好的恢复"><a href="#避免不好的恢复" class="headerlink" title="避免不好的恢复"></a>避免不好的恢复</h3><p>只有当模型与检查点兼容时，我们才可以从这个检查点中恢复出模型的状态。比如，假设你训练了一个称为 <code>DNNClassifier</code> 的 Estimator，它包含两个隐藏层，每个有 10 个结点：</p><pre><code>classifier = tf.estimator.DNNClassifier(    feature_columns=feature_columns,    hidden_units=[10, 10],    n_classes=3,    model_dir=&apos;models/iris&apos;)classifier.train(    input_fn=lambda:train_input_fn(train_x, train_y, batch_size=100),        steps=200)</code></pre><p>经过训练之后（当然，也会同时在 <code>models/iris</code> 目录中创建检查点），假如你将每个隐藏层中的 10 个结点改成 20 个，然后再尝试恢复模型：</p><pre><code>classifier2 = tf.estimator.DNNClassifier(    feature_columns=my_feature_columns,    hidden_units=[20, 20],  # 修改模型中的神经元个数    n_classes=3,    model_dir=&apos;models/iris&apos;)classifier.train(    input_fn=lambda:train_input_fn(train_x, train_y, batch_size=100),        steps=200)</code></pre><p>因为检查点的状态与 <code>classifier2</code> 所描述的模型的状态不兼容，恢复模型会失败，错误信息如下：</p><pre><code>...InvalidArgumentError (see above for traceback): tensor_name =dnn/hiddenlayer_1/bias/t_0/Adagrad; shape in shape_and_slice spec [10]does not match the shape stored in checkpoint: [20]</code></pre><p>当你在做实验时训练并比较版本稍有不同的模型时，记得保存创建每个 <code>model_dir</code> 的代码。比如，你可以为每个版本创建一个独立的 git 分支。这种分隔的做法可以保证你的检查点是可恢复的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>检查点提供了一种容易的保存和恢复由 Estimator 生成的模型的自动化机制。</p><p><a href="//tensorflow.juejin.im/programmers_guide/saved_model.html">保存和恢复</a></p><ul><li>使用底层 TensorFlow API 来保存和恢复模型。</li><li><p>在 SavedModel 模式中导出和导入模型，这是一种语言无关、可恢复、可序列化格式。</p></li><li><p><a href="#toc-0">检查点</a></p><ul><li><a href="#toc-1">示例代码</a></li><li><a href="#toc-2">保存未训练完的模型</a></li><li><a href="#toc-5">恢复你的模型</a></li><li><a href="#toc-7">总结</a></li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 训练过程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MS-COCO dataset</title>
      <link href="/2018/09/17/MS-COCO-dataset/"/>
      <url>/2018/09/17/MS-COCO-dataset/</url>
      <content type="html"><![CDATA[<p>最近在复现deeplab-v3时想在MS-COCO数据集上pretrain，所以鼓捣了很久的数据集，做个记录。</p><h1 id="数据集简介及下载"><a href="#数据集简介及下载" class="headerlink" title="数据集简介及下载"></a>数据集简介及下载</h1><p>MS-COCO是微软持续更新的一个可以用来图像recognition+segmentation+captioning的数据集，<br>其中训练集有118287张图片和测试集有5000张图片，其官方说明地址：<a href="http://mscoco.org/" target="_blank" rel="noopener">http://mscoco.org/</a><br><a id="more"></a><br>官网被墙了，所以直接从下载链接下载，速度还可以。linux下wget很方便，建议用断点续传命令 wget -c http<br><a href="http://images.cocodataset.org/zips/train2017.zip" target="_blank" rel="noopener">训练集</a><br><a href="http://images.cocodataset.org/annotations/annotations_trainval2017.zip" target="_blank" rel="noopener">训练集注释</a></p><p><a href="http://images.cocodataset.org/zips/val2017.zip" target="_blank" rel="noopener">验证集</a><br><a href="http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip" target="_blank" rel="noopener">验证集注释</a></p><p><a href="http://images.cocodataset.org/zips/test2017.zip" target="_blank" rel="noopener">测试集</a><br><a href="http://images.cocodataset.org/annotations/image_info_test2017.zip" target="_blank" rel="noopener">测试集注释</a></p><p>由于云服务器是内部网的(T_T)，所以先ssh到本地的服务器，然后通过堡垒机作为中转终于传到云服务器上了。</p><h1 id="cocoAPI"><a href="#cocoAPI" class="headerlink" title="cocoAPI"></a>cocoAPI</h1><p>coco数据集的注释是以json格式存储的，coco配置了数据读取的API，下载链接：<a href="https://github.com/nightrome/cocostuffapi" target="_blank" rel="noopener">https://github.com/nightrome/cocostuffapi</a></p><p>下载之后cd到PythonAPI路径下执行命令make就行</p><p>我在用的时候碰到了gcc错误</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python setup.py build_ext --inplace</span><br><span class="line">running build_ext</span><br><span class="line">building <span class="string">'pycocotools._mask'</span> extension</span><br><span class="line">gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I<span class="regexp">/home/</span>rizqi-okta<span class="regexp">/miniconda2/</span>lib<span class="regexp">/python2.7/</span>site-packages<span class="regexp">/numpy/</span>core<span class="regexp">/include -I../</span>common -I<span class="regexp">/home/</span>rizqi-okta<span class="regexp">/miniconda2/</span>include<span class="regexp">/python2.7 -c pycocotools/</span>_mask.c -o build<span class="regexp">/temp.linux-x86_64-2.7/</span>pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99</span><br><span class="line">pycocotools/_mask.<span class="string">c:</span><span class="number">547</span>:<span class="number">21</span>: fatal <span class="string">error:</span> maskApi.<span class="string">h:</span> No such file or directory</span><br><span class="line">compilation terminated.</span><br><span class="line"><span class="string">error:</span> command <span class="string">'gcc'</span> failed with exit status <span class="number">1</span></span><br><span class="line"><span class="string">Makefile:</span><span class="number">3</span>: recipe <span class="keyword">for</span> target <span class="string">'all'</span> failed</span><br><span class="line"><span class="string">make:</span> *** [all] Error <span class="number">1</span></span><br></pre></td></tr></table></figure><p>也重装了Cython，还是不行，最后发现是自己挪动了PythonAPI文件夹的位置，从而找不到_maskApi.c等配置文件，重新调整路径后终于搞定。<br>如果有遇到其他问题，可以参考这个issue：<a href="https://github.com/cocodataset/cocoapi/issues/141" target="_blank" rel="noopener">https://github.com/cocodataset/cocoapi/issues/141</a></p><h1 id="用cocoAPI来生成segmentation的label"><a href="#用cocoAPI来生成segmentation的label" class="headerlink" title="用cocoAPI来生成segmentation的label"></a>用cocoAPI来生成segmentation的label</h1><p>因为我的环境是python3，所以在以下地方有修改：</p><ol><li>在PythonAPI/cocostuff/cocoSegmentationToPngDemo.py中line.68 修改xrange为range</li><li>在PythonAPI/cocostuff/cocoSegmentationToPngDemo.py中注释掉line.62-64，这是示例限制生成label个数<br>再注释掉line.76-88</li><li>设置好annPath为自己的stuff_train/val2017.json路径，运行即可</li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://blog.csdn.net/qq_33000225/article/details/78985635" target="_blank" rel="noopener">MS-COCO数据集来做semantic segmentation</a></li><li><a href="https://arxiv.org/abs/1504.00325" target="_blank" rel="noopener">Microsoft COCO Captions: Data Collection and Evaluation Server</a></li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 数据集 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Semantic Segmentation</title>
      <link href="/2018/09/03/Semantic-Segmentation/"/>
      <url>/2018/09/03/Semantic-Segmentation/</url>
      <content type="html"><![CDATA[<p>最近在做语义分割的研究，就上周汇报做一个简单记录。</p><h1 id="What-exactly-is-Semantic-Segmentation"><a href="#What-exactly-is-Semantic-Segmentation" class="headerlink" title="What exactly is Semantic Segmentation?"></a>What exactly is Semantic Segmentation?</h1><p>所谓语义分割，就是在像素水平上理解一幅图，例如我们想要把图片中的每一个像素都给分类。</p><p>通过语义分割，我们可以把一幅图片中的不同类物体识别出来，并可以区分出边界，从而可以实现像素级别的预测<br><a id="more"></a></p><h1 id="What-are-the-different-approacher"><a href="#What-are-the-different-approacher" class="headerlink" title="What are the different approacher?"></a>What are the different approacher?</h1><ul><li>CNN：CNN来实现语义分割主要是通过对一个像素在图像中的patch来进行分类。对于CNN来说，直到第一个全连接层<br>之前，输入图片的大小可以是不固定的，但是有了全连接层之后，就要求输入大小保持一致。所以会有以下缺点：</li></ul><ol><li>存储开销大 </li><li>计算效率低</li><li>patch的大小限制了感受野的大小</li></ol><ul><li><p>FCN：将CNN的全连接层替换成卷积层就变成了全卷积神经网络，这也是现在语义分割领域大部分架构的基础结构。</p></li><li><p>encoder-decoder architecture：因为池化层的使用丢失了位置信息，使得分类结果不能够实现绝对匹配。</p></li></ul><p><img src="/img/u-net.png" alt="png"></p><ul><li><p>dilated/atrous convolutions：膨胀卷积可以不通过池化获得较大的感受野，减小信息损失。</p></li><li><p>CRF：条件随机场预处理常用于改善分割效果，它是一种基于底层图像像素强度进行平滑分割的图模型，工作原理是<br>灰度相近的像素易被标注为同一类别，通常可令分值提高1-2%。</p></li></ul><h1 id="Summaries-of-previous-works"><a href="#Summaries-of-previous-works" class="headerlink" title="Summaries of previous works"></a>Summaries of previous works</h1><h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><ul><li>End to end convolutional networks</li><li>Deconvolutional layers</li><li>Skip connections</li></ul><p><img src="/img/FCN.png" alt="png"></p><p>FCN是将全连接层改为卷积层之后实现端到端的卷积网络，它应用了反卷积层来进行上采样。由于池化过程造成信息丢失，<br>上采样生成的分割图较为粗糙，所以从高分辨率的特征图引入跳跃连接来改善上采样的粗糙程度。</p><h2 id="Dilated-Convolutions"><a href="#Dilated-Convolutions" class="headerlink" title="Dilated Convolutions"></a>Dilated Convolutions</h2><ul><li>Dilated convolutions</li><li>Context module</li></ul><p>除了全连接层，使用卷积神经网络来实现分割的另一个主要问题是池化层，池化层扩大了感受野，有效继承了上层信息，<br>但是同时丢弃了位置信息，然而，语义分割的要求是对于分类结果的绝对匹配，所以提出了膨胀卷积。<br>膨胀卷积能够极大的扩大感受野同时不减小空间维度。<br>Context module级联了不同膨胀系数的膨胀卷积层，输入输出同尺寸，能够提取不同规模特征中的信息，得到更精确的分割结果。</p><h2 id="DeepLab-v1-amp-v2"><a href="#DeepLab-v1-amp-v2" class="headerlink" title="DeepLab(v1&amp;v2)"></a>DeepLab(v1&amp;v2)</h2><ul><li>ASPP</li><li>Atrous convolutions</li><li>CRF</li></ul><p><img src="/img/deeplabv1.png" alt="png"></p><p>这是google推出的一系列架构，这两个版本较相似，就放在一块说了。<br>该架构是将原始图像的多个重新缩放版本传递到CNN的并行分支(图像金字塔)中，或者使用采样率不同的多个并行空洞卷积层(ASPP)，<br>实现多尺度处理，膨胀卷积不再另行介绍。<br>全连接CRF可以实现结构化预测，该部分的训练/微调需作为后处理的步骤单独进行。</p><h2 id="RefineNet"><a href="#RefineNet" class="headerlink" title="RefineNet"></a>RefineNet</h2><ul><li>Encoder-Decoder architecture</li><li>residual connection design</li></ul><p><img src="/img/refinenet.png" alt="png"></p><p>上文提到的为了解决池化层而提出的膨胀卷积也有弊端，其需要大量高分辨率特征图，计算成本高，占用内存大。<br>所以提出了RefineNet，它主要由三个部分构成：</p><ol><li>不同尺度（也可能只有一个输入尺度）的特征输入首先经过两个Residual模块的处理；</li><li>之后是不同尺寸的特征进行融合。当然如果只有一个输入尺度，该模块则可以省去。所有特征上采样至最大的输入尺寸，然后进行加和。<br>上采样之前的卷积模块是为了调整不同特征的数值尺度；</li><li>最后是一个链式的pooling模块。其设计本意是使用侧支上一系列的pooling来获取背景信息（通常尺寸较大）。直连通路上的ReLU可以在<br>不显著影响梯度流通的情况下提高后续pooling的性能，同时不让网络的训练对学习率很敏感。<br>最后再经过一个Residual模块即得RefineNet的输出。</li></ol><h2 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h2><ul><li>Pyramid pooling module</li><li>Use auxiliary loss</li></ul><p><img src="/img/PSPNet.png" alt="png"></p><p>该架构采用4层金字塔模块，该模块将ResNet的特征图与并行池化层的上采样输出结果连接起来，从而保留了位置信息。<br>另外，在主分支损失之外增加了附加损失，能够更快更有效的调校模型。</p><h2 id="DeepLab-v3"><a href="#DeepLab-v3" class="headerlink" title="DeepLab v3"></a>DeepLab v3</h2><ul><li>Improved ASPP</li><li>Atrous convolutions in cascade</li></ul><p><img src="/img/deeplabv3.png" alt="png"></p><p>在该架构中，具有不同atrous rates的ASPP能够有效的捕获多尺度信息。不过，随着采样率的增加，有效特征区域会逐渐变小。<br>当采用具有不同atrous rates的3x3 filter应用到65x65 featuremap上时，在rate值接近于feature map大小的极端情况，该3x3<br> filter不能捕获整个图像内容信息，而退化成了一个简单的1x1 filter，因为只有中心filter权重才是有效的。所以最后要对特征<br>进行双线性上采样到特定的空间维度。<br>该架构中的级联模块的逐步翻倍的atrous rates和ASPP模块增强图像级的特征，探讨了多采样率和有效感受野下的滤波器特性。</p><h2 id="UperNet"><a href="#UperNet" class="headerlink" title="UperNet"></a>UperNet</h2><ul><li>Unified Perceptual Parsing</li><li>Broden+</li></ul><p><img src="/img/UperNet.png" alt="png"></p><p>该架构要实现的是一个全新概念，统一感知解析(Unified Perceptual Parsing),即让机器系统尽可能多的识别出一副图像中的视觉概念，<br>所以重新构建了数据集Broden+。<br>为了使深度卷积网络的感受野足够大，本架构将PSPNet中的PPM用于骨干网络的最后一层。因为图像级信息更适合场景分类，PPM 模块之后的特征图被用来对scene分类<br>。来自 FPN 的所有层相融合的特征图被用来对Object和Part进行分类。FPN 中带有最高分辨率的特征图用来对Material进行分类。纹理特征是最简单的特征，最容易发现和辨别<br>，因此Texture 被附加到 ResNet 中的 Res-2 模块，并在整个网络完成其他任务的训练之后进行优化。</p><h2 id="DeepLab-v3-1"><a href="#DeepLab-v3-1" class="headerlink" title="DeepLab v3+"></a>DeepLab v3+</h2><ul><li>New encoder-decoder architecture</li><li>Xception</li></ul><p><img src="/img/deeplabv3+_0.png" alt="png"></p><p>该模型主要使用了一种全新的编码-解码架构，然后探索了Xception和深度分离卷积在模型上的使用<br>(a): 即DeepLabv3的结构，使用ASPP模块获取多尺度上下文信息，直接上采样得到预测结果<br>(b): encoder-decoder结构，高层特征提供语义，decoder逐步恢复边界信息<br>(c): DeepLabv3+结构，以DeepLabv3为encoder，decoder结构简单</p><p><img src="/img/deeplabv3+_1.png" alt="png"></p><p><img src="/img/deeplabv3+_2.png" alt="png"></p><p>模型改进了MSRA的Xception工作，采用了更深的Xception结构，不同的地方在于不修改entry flow network的结构，这样可以快速计算和有效的使用内存。<br>所有的最大池化操作替换成带下采样的深度分离卷积，这能够应用扩张分离卷积扩展feature的分辨率。<br>在每个3×33×3的深度卷积后增加BN层和ReLU。<br>改进后的Xception为encode网络主体，替换原本DeepLabv3的ResNet101，进一步提高模型的速度和性能。</p><h1 id="Datasets-comparison"><a href="#Datasets-comparison" class="headerlink" title="Datasets comparison"></a>Datasets comparison</h1><p><img src="/img/datasets.png" alt="png"></p><blockquote><p>最近在mentor的指导下尝试复现deeplab v3，之后如果复现成功的话会把github放上来~</p></blockquote><p>以上。</p>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Batch Normalization</title>
      <link href="/2018/08/20/Batch-Normalization/"/>
      <url>/2018/08/20/Batch-Normalization/</url>
      <content type="html"><![CDATA[<p>Batch Normalization作为最近一年来DL的重要成果，已经广泛被证明其有效性和重要性。</p><p>机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集<br>获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。<br><a id="more"></a></p><h2 id="一、”Internal-Covariate-Shift”问题"><a href="#一、”Internal-Covariate-Shift”问题" class="headerlink" title="一、”Internal Covariate Shift”问题"></a>一、”Internal Covariate Shift”问题</h2><p>首先说明Mini-batch SGD相对于One Example SGD的两个优势：</p><ul><li>梯度更新方向更准确</li><li>并行计算速度快</li></ul><p>那么所谓的covariate shift就是 如果ML系统实例集合&lt;X,Y&gt;中的输入值X的分布老是变，就不符合IID假设，神经网络模型很难稳定的学到规律。</p><p>对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就<br>是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部<br>的事情，而不是covariate shift问题只发生在输入层。</p><p>之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓白化，就是对输入数据分布变换到0均值，单位方差的正态分布——那么神经网络会较快收敛。<br>那么图像作为深度神经网络的输入层，通过白化能够加快收敛，对于深度网络来说，其中某个隐层的神经元是下一层的输入，可以把BN理解成为对深层神经网络每个隐层神经元的<br>激活值做简化版本的白化操作。</p><h2 id="二、-BatchNorm的本质思想"><a href="#二、-BatchNorm的本质思想" class="headerlink" title="二、 BatchNorm的本质思想"></a>二、 BatchNorm的本质思想</h2><p>因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体<br>分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失，<br>这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，<br>其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样<br>让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</p><p>总而言之，言而总之：对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，<br>使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</p><h2 id="三、-训练阶段如何做BatchNorm"><a href="#三、-训练阶段如何做BatchNorm" class="headerlink" title="三、 训练阶段如何做BatchNorm"></a>三、 训练阶段如何做BatchNorm</h2><p>假设对于一个深层神经网络来说，其中两层结构如下：</p><p><img src="/img/bn-1.png" alt="png"></p><p>要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p><p><img src="/img/bn-2.png" alt="png"></p><p>对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：</p><p><img src="/img/bn-3.png" alt="png"></p><p>要注意，这里t层某个神经元的x(k)不是指原始输入，就是说不是t-1层每个神经元的输出，而是t层这个神经元的线性激活x=WU+B，这里的U才是t-1层神经元的输出。<br>变换的意思是：某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p><p>经过这个变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，<br>加快训练收敛速度。但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，<br>使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作：</p><p><img src="/img/bn-4.png" alt="png"></p><p>BN其具体操作流程，如论文中描述的一样：</p><p><img src="/img/bn-5.png" alt="png"></p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><ul><li>《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》</li><li><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">深入理解Batch Normalization批标准化</a></li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> BN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关联规则分析</title>
      <link href="/2018/08/14/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%88%86%E6%9E%90/"/>
      <url>/2018/08/14/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>这两天做了两道笔试题，第一道题是常规的结构化数据分类问题，用一个觉得效果最好的分类算法来算AUC，<br>并给出参数优化的过程。<br>第二题我之前还没有碰到过类似的问题，是对结构化的数据进行挖掘，找出其中的association rules，并且Rule<br>的左侧少于等于两项，Support在0.1以上，Confidence在0.7以上。<br><a id="more"></a></p><h1 id="关联规则分析"><a href="#关联规则分析" class="headerlink" title="关联规则分析"></a>关联规则分析</h1><p>整个过程是：</p><ol><li>先对原始数据进行处理，将值和特征名称组合起来构成新的词项</li><li>将原始的DataFrame转换成为提取频繁项集时所需要的列表</li><li>采用Apriori算法或者FP-growth算法生成频繁项集</li><li>对频繁项集进行规则查找</li></ol><h1 id="Apriori算法"><a href="#Apriori算法" class="headerlink" title="Apriori算法"></a>Apriori算法</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>如果某个项集是频繁的，那么它的所有子集势必也是频繁的，这个原理从表面上看没什么大用，但是反过来，如果一个项集是非频繁项集，<br>那么它所对应的超集就全都是非频繁项集。这样在确定了一个项集是非频繁项集了之后，它所对应的超集的支持度我们就可以不去计算了，<br>这在很大程度上避免了项集数目的指数增长，可以更加合理的计算频繁项集。</p><h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p>(1) 发现频繁项集</p><p>Apriori算法的两个输入参数分别是最小支持度和数据集。该算法首先生成所有单个物品的项集列表，遍历之后去掉不满足最小支持度要求<br>的项集；接下来对剩下的集合进行组合生成包含两个元素的项集，去掉不满足最小支持度的项集；重复该过程直到去掉所有不满足最小支<br>持度的项集。</p><p>(2) 从频繁项集中挖掘关联规则</p><p>假设有一个频繁项集，它们之间就有可能有一条关联规则，即可以表示为：”…—&gt;…”，但反过来并不一定成立（其中箭头左边对应的集合为前件，箭头右边对应的集合为后件）。<br>在上一节，我们使用最小支持度来量化频繁项集，对应的，采用可信度来量化关联规则。其中一条规则p—&gt;H的可信度定义为：support(P|H)/support(P)，为找到其中的关联规则，<br>我们可以先生成一个可能的规则列表，然后测试每条规则的可信度，结合可信度的最小要求，得到关联规则。同寻找频繁项集类似，我们可以为每个频繁项集产生许多关联规则，<br>这样就会有很多的关联规则产生。结合Apriori原理，如果某条规则不满足最小可信度要求，那么该规则的所有子集也就不满足最小可信度要求，据此我们可以减少需要测试的规则数目，简化问题。<br>寻找关联规则的思想是：从一个频繁项集开始，创建一个规则列表，首先将规则的右边限定为一个元素，对这些规则进行测试，接下来合并剩下的规则来创建一个新的规则列表，<br>规则的右边限定为两个元素，就这样一步一步实现。</p><h1 id="FP-growth算法"><a href="#FP-growth算法" class="headerlink" title="FP-growth算法"></a>FP-growth算法</h1><p>FP-growth(Frequent Pattern Growth，频繁模式增长)，它比Apriori算法效率更高，在整个算法执行过程中，只需要遍历数据集2次，就<br>可完成频繁模式的发现。</p><h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>(1) 创建FP-tree</p><p>对于输入的dataset，统计所有事项中各元素的出现频次，即各个1项集的频数，并将各元素按照频数降序排序，删除那些出现频数少于设定<br>支持度sup的元素，形成列表L，留下来的元素就构成了频繁1项集。(这是对数据集的第一遍扫描)</p><p>对数据集中每个事物的元素按照列表L排序(按支持度降序排列)，开始构造FP-tree。树的根节点为空，每个事务中的所有元素形成一条<br>从根节点到叶子节点的路径。若几个事务的元素按列表L排序后，具有相同的前m个元素，则它们在FP-tree中共享前m个元素代表的节点。<br>树中每个节点的计数为路径经过该节点的事务集的个数。(这是对数据集的第二遍扫描)</p><p>在创建FP-tree的同时，headTable也就创建好了，headTable可以理解为一个具有三列的表。第一列为元素(项ID)，第二列为支持度计数，<br>第三列为节点链。如下所示</p><table><thead><tr><th>项ID</th><th style="text-align:center">支持度计数</th><th style="text-align:right">节点链 </th></tr></thead><tbody><tr><td>啤酒</td><td style="text-align:center">4</td><td style="text-align:right">nodelink1</td></tr><tr><td>尿不湿</td><td style="text-align:center">3</td><td style="text-align:right">nodelink2</td></tr><tr><td>…</td><td style="text-align:center">…</td><td style="text-align:right">…</td></tr><tr><td>牛奶</td><td style="text-align:center">2</td><td style="text-align:right">nodelinkn</td></tr></tbody></table><p>headTable中的项也是按照支持度计数从大到小排列的。节点链则链接到FP-tree中这一项所在的各个叶子结点上，后面频繁项集的发现就是靠的这些节点链。</p><p>(2) 寻找FP</p><p>从headTable中的最后一行开始，依次往上取项，比如最开始我们取‘牛奶’。寻找‘牛奶’的所有前缀路径，这些前缀路径形成‘牛奶’的CPB(Condition Pattern Base，条件模式基)，<br>而这些CPB又形成新的事务数据库。将‘牛奶’这一项添加到我们的集合中，此时，‘牛奶’这个频繁1-项集的支持度就是headTable中的支持度计数。然后用‘牛奶’的CPB形成的事务数据构造FP-tree，<br>构造的过程中将不满足支持度的项删除，而满足支持度的项又会构成另外一个FP-tree和headTable，我们记为FP-tree1和headTable1。同样的从headTable1的最后一行开始，<br>比如是可乐，那么把‘可乐’和之前的‘牛奶’就形成了频繁2-项集，此时，{‘牛奶’，‘可乐’}这个频繁2-项集的支持度就是headTable1中‘可乐’的支持度。<br>同时，我们也要寻找‘可乐’的前缀路径形成的CPB构成的又一个事务数据集，仍旧是构造FP-tree，从headTable取元素项，将元素项加集合</p><p>所以，FP的发现过程就是一个循环里不断递归的操作。循环，循环的是headTable中的各个元素项；递归，递归的是元素项的CPB构成的事务数据集形成的FP-tree中发现FP。</p><p>原理就介绍到这，下面是具体实践。</p><p>经过一番搜索，我查到两种算法对应的实现过程如下：</p><ul><li>pymining：根据Apriori算法进行关联规则挖掘</li><li>Orange3：根据FP-growth算法进行关联规则挖掘<br>但是按照CSDN上的教程实现下来出现了一些问题，在提取频繁项集的时候报错提示输入应该是int型的列表，但是教程中输入的是<br>str型的列表，花了很久也没有解决，随放弃。</li></ul><p>后来采用apyori.apriori来进行处理，得到了结果，但是在运行过程中炒鸡慢 = = 我一度怀疑是我的代码写的有问题，最终只跑了<br>两侧各为1的Rule，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Mon Aug 13 21:47:59 2018</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: ims</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> apyori <span class="keyword">import</span> apriori</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">df = pd.read_csv(<span class="string">r'Test2_data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始数据处理组合出新词项后生成列表</span></span><br><span class="line">transactions = []</span><br><span class="line">listToStore = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(df.iloc[:,<span class="number">0</span>].size):             <span class="comment">#df.iloc[:,0].size</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        s = df.iloc[i][col]</span><br><span class="line">        s = str(s)</span><br><span class="line">        s = col + <span class="string">'_'</span> + s</span><br><span class="line">        listToStore.append(s)</span><br><span class="line">    transactions.append(listToStore)</span><br><span class="line">    print(i)</span><br><span class="line">    listToStore = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用apriori算法生成规则</span></span><br><span class="line">rules = apriori(transactions,min_support=<span class="number">0.1</span>,min_confidence=<span class="number">0.7</span>,min_lift=<span class="number">1</span>,max_length=<span class="number">2</span>)</span><br><span class="line">results = list(rules)</span><br><span class="line">final_results = pd.DataFrame(np.random.randint(low=<span class="number">0</span>,high=<span class="number">1</span>,size=(len(results),<span class="number">6</span>)),columns=[<span class="string">'GeneralRules'</span>,\</span><br><span class="line">                             <span class="string">'LeftRules'</span>,<span class="string">'RightRules'</span>,<span class="string">'Support'</span>,<span class="string">'Confidence'</span>,<span class="string">'Lift'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将规则提取成dataframe形式</span></span><br><span class="line">index = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> g, s, i <span class="keyword">in</span> results:</span><br><span class="line">    final_results.iloc[index] = [<span class="string">' _&amp;&amp;_ '</span>.join(list(g)), <span class="string">' _&amp;&amp;_ '</span>.join(list(i[<span class="number">0</span>][<span class="number">0</span>])), <span class="string">' _&amp;&amp;_ '</span>.join(list(i[<span class="number">0</span>][<span class="number">1</span>])), s, i[<span class="number">0</span>][<span class="number">2</span>], i[<span class="number">0</span>][<span class="number">3</span>]]</span><br><span class="line">    index = index + <span class="number">1</span></span><br><span class="line">final_results = final_results.sort_values(<span class="string">'Lift'</span>,ascending=<span class="number">0</span>)</span><br><span class="line">final_rules = final_results[final_results[<span class="string">'RightRules'</span>]==<span class="string">'Label'</span>]</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">'The total time is:&#123;&#125;'</span>.format(end_time-start_time))</span><br></pre></td></tr></table></figure></p><p>提取出的规则如图：</p><h3 id="正向规则"><a href="#正向规则" class="headerlink" title="正向规则"></a>正向规则</h3><p><img src="/img/rule1.png" alt="png"></p><h3 id="反向规则"><a href="#反向规则" class="headerlink" title="反向规则"></a>反向规则</h3><p><img src="/img/rule2.png" alt="png"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://blog.csdn.net/qq_19528953/article/details/79412245" target="_blank" rel="noopener">使用python进行数据关联分析</a></li><li><a href="https://www.kaggle.com/asamir/online-retail-analyze-with-association-rules/notebook" target="_blank" rel="noopener">Online Retail Analyze With Association Rules</a></li><li>机器学习实战，Peter Harrington</li></ul>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> association analysis </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>改善深层神经网络-Optimization+methods</title>
      <link href="/2018/08/12/Optimization-methods/"/>
      <url>/2018/08/12/Optimization-methods/</url>
      <content type="html"><![CDATA[<p>Until now, you’ve always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you will learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result. </p><p>Gradient descent goes “downhill” on a cost function $J$. Think of it as trying to do this:<br><img src="img/cost.jpg" style="width:650px;height:300px;"></p><caption><center> <u> <strong>Figure 1</strong> </u>: <strong>Minimizing the cost is like finding the lowest point in a hilly landscape</strong><br> At each step of the training, you update your parameters following a certain direction to try to get to the lowest possible point. </center></caption><br><a id="more"></a><br><strong>Notations</strong>: As usual, $\frac{\partial J}{\partial a } = $ <code>da</code> for any variable <code>a</code>.<br><br>To get started, run the following code to import the libraries you will need.<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure><br><br>## 1 - Gradient Descent<br><br>A simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent.<br><br><strong>Warm-up exercise</strong>: Implement the gradient descent update rule. The  gradient descent rule is, for $l = 1, …, L$:<br>$$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{1}$$<br>$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{2}$$<br><br>where L is the number of layers and $\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary. Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift <code>l</code> to <code>l+1</code> when coding.<br><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_gd</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_gd</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using one step of gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters to be updated:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients to update each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, learning_rate = update_parameters_with_gd_test_case()</span><br><span class="line"></span><br><span class="line">parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><br><br>    W1 = [[ 1.63535156 -0.62320365 -0.53718766]<br>     [-1.07799357  0.85639907 -2.29470142]]<br>    b1 = [[ 1.74604067]<br>     [-0.75184921]]<br>    W2 = [[ 0.32171798 -0.25467393  1.46902454]<br>     [-2.05617317 -0.31554548 -0.3756023 ]<br>     [ 1.1404819  -1.09976462 -0.1612551 ]]<br>    b2 = [[-0.88020257]<br>     [ 0.02561572]<br>     [ 0.57539477]]<br><br><br><strong>Expected Output</strong>:<br><br><table><br>    <tr><br>    <td> <strong>W1</strong> </td><br>           <td> [[ 1.63535156 -0.62320365 -0.53718766]<br> [-1.07799357  0.85639907 -2.29470142]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>b1</strong> </td><br>           <td> [[ 1.74604067]<br> [-0.75184921]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>W2</strong> </td><br>           <td> [[ 0.32171798 -0.25467393  1.46902454]<br> [-2.05617317 -0.31554548 -0.3756023 ]<br> [ 1.1404819  -1.09976462 -0.1612551 ]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>b2</strong> </td><br>           <td> [[-0.88020257]<br> [ 0.02561572]<br> [ 0.57539477]] </td><br>    </tr><br></table><br><br><br>A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent.<br><br>- <strong>(Batch) Gradient Descent</strong>:<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure><br><br>- <strong>Stochastic Gradient Descent</strong>:<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure><br><br>In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. Here is an illustration of this:<br><br><img src="img/kiank_sgd.png" style="width:750px;height:250px;"><br><caption><center> <u> <font color="purple"> <strong>Figure 1</strong> </font></u><font color="purple">  : <strong>SGD vs GD</strong><br> “+” denotes a minimum of the cost. SGD leads to many oscillations to reach convergence. But each step is a lot faster to compute for SGD than for GD, as it uses only one training example (vs. the whole batch for GD). </font></center></caption><p><strong>Note</strong> also that implementing SGD requires 3 for-loops in total:</p><ol><li>Over the number of iterations</li><li>Over the $m$ training examples</li><li>Over the layers (to update all parameters, from $(W^{[1]},b^{[1]})$ to $(W^{[L]},b^{[L]})$)</li></ol><p>In practice, you’ll often get faster results if you do not use neither the whole training set, nor only one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples.</p><p><img src="img/kiank_minibatch.png" style="width:750px;height:250px;"></p><caption><center> <u> <font color="purple"> <strong>Figure 2</strong> </font></u>: <font color="purple">  <strong>SGD vs Mini-Batch GD</strong><br> “+” denotes a minimum of the cost. Using mini-batches in your optimization algorithm often leads to faster optimization. </font></center></caption><p><font color="blue"><br><strong>What you should remember</strong>:</font></p><ul><li>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.</li><li>You have to tune a learning rate hyperparameter $\alpha$.</li><li>With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</li></ul><h2 id="2-Mini-Batch-Gradient-descent"><a href="#2-Mini-Batch-Gradient-descent" class="headerlink" title="2 - Mini-Batch Gradient descent"></a>2 - Mini-Batch Gradient descent</h2><p>Let’s learn how to build mini-batches from the training set (X, Y).</p><p>There are two steps:</p><ul><li><strong>Shuffle</strong>: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches. </li></ul><p><img src="img/kiank_shuffle.png" style="width:550px;height:300px;"></p><ul><li><strong>Partition</strong>: Partition the shuffled (X, Y) into mini-batches of size <code>mini_batch_size</code> (here 64). Note that the number of training examples is not always divisible by <code>mini_batch_size</code>. The last mini batch might be smaller, but you don’t need to worry about this. When the final mini-batch is smaller than the full <code>mini_batch_size</code>, it will look like this: </li></ul><p><img src="img/kiank_partition.png" style="width:550px;height:300px;"></p><p><strong>Exercise</strong>: Implement <code>random_mini_batches</code>. We coded the shuffling part for you. To help you with the partitioning step, we give you the following code that selects the indexes for the $1^{st}$ and $2^{nd}$ mini-batches:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">first_mini_batch_X = shuffled_X[:, <span class="number">0</span> : mini_batch_size]</span><br><span class="line">second_mini_batch_X = shuffled_X[:, mini_batch_size : <span class="number">2</span> * mini_batch_size]</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>Note that the last mini-batch might end up smaller than <code>mini_batch_size=64</code>. Let $\lfloor s \rfloor$ represents $s$ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be $\lfloor \frac{m}{mini_batch_size}\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini__batch__size \times \lfloor \frac{m}{mini_batch_size}\rfloor$). </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: random_mini_batches</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a list of random minibatches from (X, Y)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(seed)            <span class="comment"># To make your "random" minibatches the same as ours</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                  <span class="comment"># number of training examples</span></span><br><span class="line">    mini_batches = []</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 1: Shuffle (X, Y)</span></span><br><span class="line">    permutation = list(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:,k*mini_batch_size : (k+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,k*mini_batch_size : (k+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:,<span class="number">0</span>:(m - mini_batch_size * (m//mini_batch_size))]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,<span class="number">0</span>:(m - mini_batch_size * (m//mini_batch_size))]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()</span><br><span class="line">mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 1st mini_batch_X: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 2nd mini_batch_X: "</span> + str(mini_batches[<span class="number">1</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 3rd mini_batch_X: "</span> + str(mini_batches[<span class="number">2</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 1st mini_batch_Y: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">1</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 2nd mini_batch_Y: "</span> + str(mini_batches[<span class="number">1</span>][<span class="number">1</span>].shape)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 3rd mini_batch_Y: "</span> + str(mini_batches[<span class="number">2</span>][<span class="number">1</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"mini batch sanity check: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>:<span class="number">3</span>]))</span><br></pre></td></tr></table></figure><pre><code>shape of the 1st mini_batch_X: (12288, 64)shape of the 2nd mini_batch_X: (12288, 64)shape of the 3rd mini_batch_X: (12288, 20)shape of the 1st mini_batch_Y: (1, 64)shape of the 2nd mini_batch_Y: (1, 64)shape of the 3rd mini_batch_Y: (1, 20)mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:50%"><br>    <tr><br>    <td> <strong>shape of the 1st mini_batch_X</strong> </td><br>           <td> (12288, 64) </td><br>    </tr><br><br>    <tr><br>    <td> <strong>shape of the 2nd mini_batch_X</strong> </td><br>           <td> (12288, 64) </td><br>    </tr><br><br>    <tr><br>    <td> <strong>shape of the 3rd mini_batch_X</strong> </td><br>           <td> (12288, 20) </td><br>    </tr><br>    <tr><br>    <td> <strong>shape of the 1st mini_batch_Y</strong> </td><br>           <td> (1, 64) </td><br>    </tr><br>    <tr><br>    <td> <strong>shape of the 2nd mini_batch_Y</strong> </td><br>           <td> (1, 64) </td><br>    </tr><br>    <tr><br>    <td> <strong>shape of the 3rd mini_batch_Y</strong> </td><br>           <td> (1, 20) </td><br>    </tr><br>    <tr><br>    <td> <strong>mini batch sanity check</strong> </td><br>           <td> [ 0.90085595 -0.7612069   0.2344157 ] </td><br>    </tr><br><br></table><p><font color="blue"><br><strong>What you should remember</strong>:</font></p><ul><li>Shuffling and Partitioning are the two steps required to build mini-batches</li><li>Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.</li></ul><h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3 - Momentum"></a>3 - Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations. </p><p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. </p><p><img src="img/opt_momentum.png" style="width:400px;height:250px;"></p><p><caption><center> <u><font color="purple"><strong>Figure 3</strong></font></u><font color="purple">: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color="black"> </font></font></center></caption></p><p><strong>Exercise</strong>: Initialize the velocity. The velocity, $v$, is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the <code>grads</code> dictionary, that is:<br>for $l =1,…,L$:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br></pre></td></tr></table></figure></p><p><strong>Note</strong> that the iterator l starts at 0 in the for loop while the first parameters are v[“dW1”] and v[“db1”] (that’s a “one” on the superscript). This is why we are shifting l to l+1 in the <code>for</code> loop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_velocity</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_velocity</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes the velocity as a python dictionary with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity.</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize velocity</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_velocity_test_case()</span><br><span class="line"></span><br><span class="line">v = initialize_velocity(parameters)</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>v[&quot;dW1&quot;] = [[0. 0. 0.] [0. 0. 0.]]v[&quot;db1&quot;] = [[0.] [0.]]v[&quot;dW2&quot;] = [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]]v[&quot;db2&quot;] = [[0.] [0.] [0.]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:40%"><br>    <tr><br>    <td> <strong>v[“dW1”]</strong> </td><br>           <td> [[ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“db1”]</strong> </td><br>           <td> [[ 0.]<br> [ 0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“dW2”]</strong> </td><br>           <td> [[ 0.  0.  0.]<br> [ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“db2”]</strong> </td><br>           <td> [[ 0.]<br> [ 0.]<br> [ 0.]] </td><br>    </tr><br></table><p><strong>Exercise</strong>:  Now, implement the parameters update with momentum. The momentum update rule is, for $l = 1, …, L$: </p><p>$$ \begin{cases}<br>v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \<br>W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}<br>\end{cases}\tag{3}$$</p><p>$$\begin{cases}<br>v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \<br>b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}}<br>\end{cases}\tag{4}$$</p><p>where L is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary.  Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift <code>l</code> to <code>l+1</code> when coding.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_momentum</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Momentum</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = ...</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Momentum update for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">        <span class="comment"># compute velocities</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.dot(beta,v[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)]) + np.dot(<span class="number">1</span> - beta, grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)])</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.dot(beta,v[<span class="string">'db'</span> + str(l+<span class="number">1</span>)]) + np.dot(<span class="number">1</span> - beta, grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)])</span><br><span class="line">        <span class="comment"># update parameters</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, v = update_parameters_with_momentum_test_case()</span><br><span class="line"></span><br><span class="line">parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = <span class="number">0.9</span>, learning_rate = <span class="number">0.01</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[ 1.62544598 -0.61290114 -0.52907334] [-1.07347112  0.86450677 -2.30085497]]b1 = [[ 1.74493465] [-0.76027113]]W2 = [[ 0.31930698 -0.24990073  1.4627996 ] [-2.05974396 -0.32173003 -0.38320915] [ 1.13444069 -1.0998786  -0.1713109 ]]b2 = [[-0.87809283] [ 0.04055394] [ 0.58207317]]v[&quot;dW1&quot;] = [[-0.11006192  0.11447237  0.09015907] [ 0.05024943  0.09008559 -0.06837279]]v[&quot;db1&quot;] = [[-0.01228902] [-0.09357694]]v[&quot;dW2&quot;] = [[-0.02678881  0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]]v[&quot;db2&quot;] = [[0.02344157] [0.16598022] [0.07420442]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:90%"><br>    <tr><br>    <td> <strong>W1</strong> </td><br>           <td> [[ 1.62544598 -0.61290114 -0.52907334]<br> [-1.07347112  0.86450677 -2.30085497]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>b1</strong> </td><br>           <td> [[ 1.74493465]<br> [-0.76027113]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>W2</strong> </td><br>           <td> [[ 0.31930698 -0.24990073  1.4627996 ]<br> [-2.05974396 -0.32173003 -0.38320915]<br> [ 1.13444069 -1.0998786  -0.1713109 ]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>b2</strong> </td><br>           <td> [[-0.87809283]<br> [ 0.04055394]<br> [ 0.58207317]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“dW1”]</strong> </td><br>           <td> [[-0.11006192  0.11447237  0.09015907]<br> [ 0.05024943  0.09008559 -0.06837279]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“db1”]</strong> </td><br>           <td> [[-0.01228902]<br> [-0.09357694]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“dW2”]</strong> </td><br>           <td> [[-0.02678881  0.05303555 -0.06916608]<br> [-0.03967535 -0.06871727 -0.08452056]<br> [-0.06712461 -0.00126646 -0.11173103]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“db2”]</strong> </td><br>           <td> [[ 0.02344157]<br> [ 0.16598022]<br> [ 0.07420442]]</td><br>    </tr><br></table><p><strong>Note</strong> that:</p><ul><li>The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps.</li><li>If $\beta = 0$, then this just becomes standard gradient descent without momentum. </li></ul><p><strong>How do you choose $\beta$?</strong></p><ul><li>The larger the momentum $\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\beta$ is too big, it could also smooth out the updates too much. </li><li>Common values for $\beta$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $\beta = 0.9$ is often a reasonable default. </li><li>Tuning the optimal $\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. </li></ul><p><font color="blue"><br><strong>What you should remember</strong>:</font></p><ul><li>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.</li><li>You have to tune a momentum hyperparameter $\beta$ and a learning rate $\alpha$.</li></ul><h2 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4 - Adam"></a>4 - Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. </p><p><strong>How does Adam work?</strong></p><ol><li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li><li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li><li>It updates parameters in a direction based on combining information from “1” and “2”.</li></ol><p>The update rule is, for $l = 1, …, L$: </p><p>$$\begin{cases}<br>v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \<br>v^{corrected}<em>{dW^{[l]}} = \frac{v</em>{dW^{[l]}}}{1 - (\beta_1)^t} \<br>s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \<br>s^{corrected}<em>{dW^{[l]}} = \frac{s</em>{dW^{[l]}}}{1 - (\beta_1)^t} \<br>W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}<em>{dW^{[l]}}}{\sqrt{s^{corrected}</em>{dW^{[l]}}} + \varepsilon}<br>\end{cases}$$<br>where:</p><ul><li>t counts the number of steps taken of Adam </li><li>L is the number of layers</li><li>$\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages. </li><li>$\alpha$ is the learning rate</li><li>$\varepsilon$ is a very small number to avoid dividing by zero</li></ul><p>As usual, we will store all parameters in the <code>parameters</code> dictionary  </p><p><strong>Exercise</strong>: Initialize the Adam variables $v, s$ which keep track of the past information.</p><p><strong>Instruction</strong>: The variables $v, s$ are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for <code>grads</code>, that is:<br>for $l = 1, …, L$:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br><span class="line">s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_adam</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span> :</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes v and s as two python dictionaries with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters["W" + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters["b" + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class="line"><span class="string">                    v["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class="line"><span class="string">                    s["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    s["db" + str(l)] = ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize v, s. Input: "parameters". Outputs: "v, s".</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)].shape)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_adam_test_case()</span><br><span class="line"></span><br><span class="line">v, s = initialize_adam(parameters)</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW1\"] = "</span> + str(s[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db1\"] = "</span> + str(s[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW2\"] = "</span> + str(s[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db2\"] = "</span> + str(s[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>v[&quot;dW1&quot;] = [[0. 0. 0.] [0. 0. 0.]]v[&quot;db1&quot;] = [[0.] [0.]]v[&quot;dW2&quot;] = [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]]v[&quot;db2&quot;] = [[0.] [0.] [0.]]s[&quot;dW1&quot;] = [[0. 0. 0.] [0. 0. 0.]]s[&quot;db1&quot;] = [[0.] [0.]]s[&quot;dW2&quot;] = [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]]s[&quot;db2&quot;] = [[0.] [0.] [0.]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:40%"><br>    <tr><br>    <td> <strong>v[“dW1”]</strong> </td><br>           <td> [[ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“db1”]</strong> </td><br>           <td> [[ 0.]<br> [ 0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“dW2”]</strong> </td><br>           <td> [[ 0.  0.  0.]<br> [ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“db2”]</strong> </td><br>           <td> [[ 0.]<br> [ 0.]<br> [ 0.]] </td><br>    </tr><br>    <tr><br>    <td> <strong>s[“dW1”]</strong> </td><br>           <td> [[ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>s[“db1”]</strong> </td><br>           <td> [[ 0.]<br> [ 0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>s[“dW2”]</strong> </td><br>           <td> [[ 0.  0.  0.]<br> [ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>s[“db2”]</strong> </td><br>           <td> [[ 0.]<br> [ 0.]<br> [ 0.]] </td><br>    </tr><br><br></table><p><strong>Exercise</strong>:  Now, implement the parameters update with Adam. Recall the general update rule is, for $l = 1, …, L$: </p><p>$$\begin{cases}<br>v_{W^{[l]}} = \beta_1 v_{W^{[l]}} + (1 - \beta_1) \frac{\partial J }{ \partial W^{[l]} } \<br>v^{corrected}<em>{W^{[l]}} = \frac{v</em>{W^{[l]}}}{1 - (\beta_1)^t} \<br>s_{W^{[l]}} = \beta_2 s_{W^{[l]}} + (1 - \beta_2) (\frac{\partial J }{\partial W^{[l]} })^2 \<br>s^{corrected}<em>{W^{[l]}} = \frac{s</em>{W^{[l]}}}{1 - (\beta_2)^t} \<br>W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}<em>{W^{[l]}}}{\sqrt{s^{corrected}</em>{W^{[l]}}}+\varepsilon}<br>\end{cases}$$</p><p><strong>Note</strong> that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift <code>l</code> to <code>l+1</code> when coding.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_adam</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate = <span class="number">0.01</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Adam</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                 <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v_corrected = &#123;&#125;                         <span class="comment"># Initializing first moment estimate, python dictionary</span></span><br><span class="line">    s_corrected = &#123;&#125;                         <span class="comment"># Initializing second moment estimate, python dictionary</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Perform Adam update on all parameters</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment"># Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v_corrected[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = v[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)]/(<span class="number">1</span>-(beta1)**t)</span><br><span class="line">        v_corrected[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = v[<span class="string">"db"</span> + str(l + <span class="number">1</span>)]/(<span class="number">1</span>-(beta1)**t)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta2) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]**<span class="number">2</span></span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta2) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]**<span class="number">2</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        s_corrected[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] =s[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)]/(<span class="number">1</span>-(beta2)**t)</span><br><span class="line">        s_corrected[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = s[<span class="string">"db"</span> + str(l + <span class="number">1</span>)]/(<span class="number">1</span>-(beta2)**t)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)]-learning_rate*(v_corrected[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)]/np.sqrt( s_corrected[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)]+epsilon))</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)]-learning_rate*(v_corrected[<span class="string">"db"</span> + str(l + <span class="number">1</span>)]/np.sqrt( s_corrected[<span class="string">"db"</span> + str(l + <span class="number">1</span>)]+epsilon))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, v, s = update_parameters_with_adam_test_case()</span><br><span class="line">parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW1\"] = "</span> + str(s[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db1\"] = "</span> + str(s[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW2\"] = "</span> + str(s[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db2\"] = "</span> + str(s[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[ 1.63178673 -0.61919778 -0.53561312] [-1.08040999  0.85796626 -2.29409733]]b1 = [[ 1.75225313] [-0.75376553]]W2 = [[ 0.32648046 -0.25681174  1.46954931] [-2.05269934 -0.31497584 -0.37661299] [ 1.14121081 -1.09245036 -0.16498684]]b2 = [[-0.88529978] [ 0.03477238] [ 0.57537385]]v[&quot;dW1&quot;] = [[-0.11006192  0.11447237  0.09015907] [ 0.05024943  0.09008559 -0.06837279]]v[&quot;db1&quot;] = [[-0.01228902] [-0.09357694]]v[&quot;dW2&quot;] = [[-0.02678881  0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]]v[&quot;db2&quot;] = [[0.02344157] [0.16598022] [0.07420442]]s[&quot;dW1&quot;] = [[0.00121136 0.00131039 0.00081287] [0.0002525  0.00081154 0.00046748]]s[&quot;db1&quot;] = [[1.51020075e-05] [8.75664434e-04]]s[&quot;dW2&quot;] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04] [1.57413361e-04 4.72206320e-04 7.14372576e-04] [4.50571368e-04 1.60392066e-07 1.24838242e-03]]s[&quot;db2&quot;] = [[5.49507194e-05] [2.75494327e-03] [5.50629536e-04]]</code></pre><p><strong>Expected Output</strong>:</p><table><br>    <tr><br>    <td> <strong>W1</strong> </td><br>           <td> [[ 1.63178673 -0.61919778 -0.53561312]<br> [-1.08040999  0.85796626 -2.29409733]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>b1</strong> </td><br>           <td> [[ 1.75225313]<br> [-0.75376553]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>W2</strong> </td><br>           <td> [[ 0.32648046 -0.25681174  1.46954931]<br> [-2.05269934 -0.31497584 -0.37661299]<br> [ 1.14121081 -1.09245036 -0.16498684]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>b2</strong> </td><br>           <td> [[-0.88529978]<br> [ 0.03477238]<br> [ 0.57537385]] </td><br>    </tr><br>    <tr><br>    <td> <strong>v[“dW1”]</strong> </td><br>           <td> [[-0.11006192  0.11447237  0.09015907]<br> [ 0.05024943  0.09008559 -0.06837279]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“db1”]</strong> </td><br>           <td> [[-0.01228902]<br> [-0.09357694]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“dW2”]</strong> </td><br>           <td> [[-0.02678881  0.05303555 -0.06916608]<br> [-0.03967535 -0.06871727 -0.08452056]<br> [-0.06712461 -0.00126646 -0.11173103]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>v[“db2”]</strong> </td><br>           <td> [[ 0.02344157]<br> [ 0.16598022]<br> [ 0.07420442]] </td><br>    </tr><br>    <tr><br>    <td> <strong>s[“dW1”]</strong> </td><br>           <td> [[ 0.00121136  0.00131039  0.00081287]<br> [ 0.0002525   0.00081154  0.00046748]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>s[“db1”]</strong> </td><br>           <td> [[  1.51020075e-05]<br> [  8.75664434e-04]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>s[“dW2”]</strong> </td><br>           <td> [[  7.17640232e-05   2.81276921e-04   4.78394595e-04]<br> [  1.57413361e-04   4.72206320e-04   7.14372576e-04]<br> [  4.50571368e-04   1.60392066e-07   1.24838242e-03]] </td><br>    </tr><br><br>    <tr><br>    <td> <strong>s[“db2”]</strong> </td><br>           <td> [[  5.49507194e-05]<br> [  2.75494327e-03]<br> [  5.50629536e-04]] </td><br>    </tr><br></table><p>You now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let’s implement a model with each of these optimizers and observe the difference.</p><h2 id="5-Model-with-different-optimization-algorithms"><a href="#5-Model-with-different-optimization-algorithms" class="headerlink" title="5 - Model with different optimization algorithms"></a>5 - Model with different optimization algorithms</h2><p>Lets use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.) </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y = load_dataset()</span><br></pre></td></tr></table></figure><p><img src="output_34_0.png" alt="png"></p><p>We have already implemented a 3-layer neural network. You will train it with: </p><ul><li>Mini-batch <strong>Gradient Descent</strong>: it will call your function:<ul><li><code>update_parameters_with_gd()</code></li></ul></li><li>Mini-batch <strong>Momentum</strong>: it will call your functions:<ul><li><code>initialize_velocity()</code> and <code>update_parameters_with_momentum()</code></li></ul></li><li>Mini-batch <strong>Adam</strong>: it will call your functions:<ul><li><code>initialize_adam()</code> and <code>update_parameters_with_adam()</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, layers_dims, optimizer, learning_rate = <span class="number">0.0007</span>, mini_batch_size = <span class="number">64</span>, beta = <span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>, num_epochs = <span class="number">10000</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- python list, containing the size of each layer</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    mini_batch_size -- the size of a mini batch</span></span><br><span class="line"><span class="string">    beta -- Momentum hyperparameter</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(layers_dims)             <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    costs = []                       <span class="comment"># to keep track of the cost</span></span><br><span class="line">    t = <span class="number">0</span>                            <span class="comment"># initializing the counter required for Adam update</span></span><br><span class="line">    seed = <span class="number">10</span>                        <span class="comment"># For grading purposes, so that your "random" minibatches are the same as ours</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the optimizer</span></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># no initialization required for gradient descent</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Select a minibatch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagation</span></span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost</span></span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backward propagation</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># Adam counter</span></span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,</span><br><span class="line">                                                               t, learning_rate, beta1, beta2,  epsilon)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 epoch</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'epochs (per 100)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>You will now run this 3 layer neural network with each of the 3 optimization methods.</p><h3 id="5-1-Mini-batch-Gradient-descent"><a href="#5-1-Mini-batch-Gradient-descent" class="headerlink" title="5.1 - Mini-batch Gradient descent"></a>5.1 - Mini-batch Gradient descent</h3><p>Run the following code to see how the model does with mini-batch gradient descent.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"gd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Gradient Descent optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><pre><code>Cost after epoch 0: 0.705585Cost after epoch 1000: 0.657627Cost after epoch 2000: 0.619240Cost after epoch 3000: 0.668277Cost after epoch 4000: 0.552847Cost after epoch 5000: 0.590733Cost after epoch 6000: 0.580247Cost after epoch 7000: 0.439928Cost after epoch 8000: 0.575281Cost after epoch 9000: 0.397644</code></pre><p><img src="output_38_1.png" alt="png"></p><pre><code>Accuracy: 0.7966666666666666</code></pre><p><img src="output_38_3.png" alt="png"></p><h3 id="5-2-Mini-batch-gradient-descent-with-momentum"><a href="#5-2-Mini-batch-gradient-descent-with-momentum" class="headerlink" title="5.2 - Mini-batch gradient descent with momentum"></a>5.2 - Mini-batch gradient descent with momentum</h3><p>Run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">"momentum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Momentum optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><pre><code>Cost after epoch 0: 0.705618Cost after epoch 1000: 0.657684Cost after epoch 2000: 0.619335Cost after epoch 3000: 0.668387Cost after epoch 4000: 0.552911Cost after epoch 5000: 0.590782Cost after epoch 6000: 0.580360Cost after epoch 7000: 0.440007Cost after epoch 8000: 0.575160Cost after epoch 9000: 0.397872</code></pre><p><img src="output_40_1.png" alt="png"></p><pre><code>Accuracy: 0.7966666666666666</code></pre><p><img src="output_40_3.png" alt="png"></p><h3 id="5-3-Mini-batch-with-Adam-mode"><a href="#5-3-Mini-batch-with-Adam-mode" class="headerlink" title="5.3 - Mini-batch with Adam mode"></a>5.3 - Mini-batch with Adam mode</h3><p>Run the following code to see how the model does with Adam.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Adam optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><pre><code>Cost after epoch 0: 0.704265Cost after epoch 1000: 0.155592Cost after epoch 2000: 0.174820Cost after epoch 3000: 0.110171Cost after epoch 4000: 0.052977Cost after epoch 5000: 0.086523Cost after epoch 6000: 0.127797Cost after epoch 7000: 0.173547Cost after epoch 8000: 0.181141Cost after epoch 9000: 0.091550</code></pre><p><img src="output_42_1.png" alt="png"></p><pre><code>Accuracy: 0.94</code></pre><p><img src="output_42_3.png" alt="png"></p><h3 id="5-4-Summary"><a href="#5-4-Summary" class="headerlink" title="5.4 - Summary"></a>5.4 - Summary</h3><table><br>    <tr><br>        <td><br>        <strong>optimization method</strong><br>        </td><br>        <td><br>        <strong>accuracy</strong><br>        </td><br>        <td><br>        <strong>cost shape</strong><br>        </td><br><br>    </tr><br>        <td><br>        Gradient descent<br>        </td><br>        <td><br>        79.7%<br>        </td><br>        <td><br>        oscillations<br>        </td><br>    <tr><br>        <td><br>        Momentum<br>        </td><br>        <td><br>        79.7%<br>        </td><br>        <td><br>        oscillations<br>        </td><br>    </tr><br>    <tr><br>        <td><br>        Adam<br>        </td><br>        <td><br>        94%<br>        </td><br>        <td><br>        smoother<br>        </td><br>    </tr><br></table> <p>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.</p><p>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</p><p>Some advantages of Adam include:</p><ul><li>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) </li><li>Usually works well even with little tuning of hyperparameters (except $\alpha$)</li></ul><p><strong>References</strong>:</p><ul><li>Adam paper: <a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1412.6980.pdf</a></li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 损失优化方法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>新学期  新征程</title>
      <link href="/2018/08/08/%E6%96%B0%E5%AD%A6%E6%9C%9F-%E6%96%B0%E5%BE%81%E7%A8%8B/"/>
      <url>/2018/08/08/%E6%96%B0%E5%AD%A6%E6%9C%9F-%E6%96%B0%E5%BE%81%E7%A8%8B/</url>
      <content type="html"><![CDATA[<p>整整一个月没有更新了，新学期开始啦，不能偷懒啦，加油加油加油！</p>]]></content>
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Random forest</title>
      <link href="/2018/07/15/Random-forest/"/>
      <url>/2018/07/15/Random-forest/</url>
      <content type="html"><![CDATA[<p>随机森林是用随机的方式建立一个森林，其中，森林的基本单元是一棵棵决策树，并且每一棵决策树之间不存在关联，随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。</p><p>集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。</p><p>坦白来讲，在森林中，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出。</p><p>从一棵树到一片森林，其生成规则如下：</p><ol><li>如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（bootstrap抽样方法），作为该树的训练集；每棵树的训练集都是不同的，但里面包含重复的训练样本。</li><li>如果每个样本的特征维度为M，指定一个常数m，且m&lt;&lt;M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；</li><li>每棵树都尽最大程度的生长，并且没有剪枝过程。<br>在森林中，每棵树都是独立的，99.9%不相关的树做出的预测结果涵盖了所有的情况，这些预测结果将会彼此抵消。少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想。<br>不过我们需要认识到：bagging不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性。</li></ol><p>在生成随机森林时，为什么要有放回的抽样？<br>如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，都是绝对”片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是”盲人摸象”。</p><p>随机森林中的“随机”主要包含两个方面：随机抽取样本，随机抽取特征。这两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过度拟合，并具有很好的抗噪能力。</p><p>参数说明：<br>    • max_features：<br>    随机森林允许单个决策树使用特征的最大数量。Python为最大特征数提供了多个可选项。下面是其中的几个：<br>    Auto/None：简单地选取所有特征，每棵树都可以利用他们。这种情况下，每棵树都没有任何的限制。<br>    sqrt：此选项是每颗子树可以利用总特征数的平方根个。log2是另一种相似类型的选项。<br>    0.2：此选项允许每个随机森林的子树可以利用变量（特征）数的20%。如果想考察特征数的x%的作用，我们可以使用”0.x”的格式。</p><pre><code>增加max_features一般能提高模型的性能，因为在每个节点上，有更多的选择可以考虑。然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。但是，可以肯定，通过增加max_features会降低算法的速度。因此，需要适当的平衡和选择最佳max_features• n_estimators：在利用最大投票数或平均值来预测之前，你想要建立子树的数量。较多的子树可以让模型有更好的性能，但同时让你的代码变慢。应该选择尽可能高的值，只要你的cpu可以刚得住，因为这使你的预测更好更稳定。• min_sample_leaf：最小样本叶片大小阈值。叶是决策树的末端节点。较小的叶子使模型更容易捕捉训练数据中的噪声。• max_depth：决策树最大深度。默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的取值在10-100之间。• min_samples_split：内部节点再划分所需最小样本数。这个值限制了子树继续划分的条件，如果某节点的样本数小于阈值，则不会继续再尝试选择最优特征来进行划分。默认是2，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值• min_weight_fraction_leaf：叶子节点最小的样本权重和。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时就要注意这个值。• max_leaf_nodes：最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是&quot;None&quot;，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。• min_impurity_split：节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度（基于基尼系数，均方差）小于这个阈值，则该节点不再生成子节点。即为叶子节点。</code></pre>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 随机森林 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>分类问题中的样本不均衡算法</title>
      <link href="/2018/07/10/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E7%9A%84%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E7%AE%97%E6%B3%95/"/>
      <url>/2018/07/10/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E7%9A%84%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>最近在做一个房态预测的比赛，比赛具体是对具体客户实现预定时有无房间的预测，官方给定的数据集是标准的非均衡样本。</p><p>所以学习了一些分类问题中的样本不均衡算法。<br><a id="more"></a></p><ol><li>根据”The strength of weak learnablility” 方法，该方法是一种boosting算法，它递归地训练三个弱学习器，然后将这三个弱学习器结合起形成一个强的学习器。算法流程如下：</li></ol><ul><li>首先使用原始数据集训练第一个学习器L1。</li><li>然后使用50%在L1中学习正确和50%学习错误的样本训练得到学习器L2，即从L1中学习错误的样本集和学习正确的样本集中，循环采样，各占一半。</li><li>接着使用L1和L2不一致的那些样本去训练得到学习器L3。</li><li>最后，使用投票方式来预测测试集。<br>应用到具体问题中，假设是一个二分类问题，大类为true类，流程如下：</li><li>使用60%原始样本来训练L1</li><li>用L1对剩下带标签的原始样本进行预测，取分类正确和错误的样本各50%，生成平衡的L2的样本集</li><li>用其中60%的样本来训练L2，对剩下的样本进行预测</li><li>对L1与L2分类不一样的样本进行训练来得到L3</li><li><p>结合L1，L2，L3，采用投票的方式来决定分类结果<br>代码实现如下：<br>‘’’ python<br>def model_L1(data):<br>  label = ‘LABEL’<br>  IDcol = ‘PERSONID’<br>  predictors = [x for x in data.columns if x not in [label,IDcol]]<br>  X_train, X_test, y_train, y_test = train_test_split(data[predictors],data[label],test_size=0.4,random_state=2018)</p><p>  model = lgb.LGBMClassifier(objective=’binary’,metric=’AUC’,num_leaves=90,depth=8,learning_rate=0.01,</p><pre><code>colsample_bytree=0.8,n_estimators=2000,seed=2018,subsample=0.9,boosting=&apos;rf&apos;,boosting_type=&apos;gbdt&apos;,reg_alpha=0.0,reg_lambda=0.7,bagging_fraction=0.7,bagging_freq=1)</code></pre><p>  model.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric=’AUC’,early_stopping_rounds=0,verbose=False)<br>  prediction_L1 = model.predict(X_test)<br>  prediction1 = model.predict_proba(X_test)<br>  area1 = auc_score(y_test,prediction1[:,1])<br>  print(‘the test score is:{}’.format(area1))<br>  print(classification_report(y_test,prediction_L1,target_names=[“noroom”,”haveroom”]))</p><h1 id="print-‘the-y-test-is-shape-is-’-format-y-test-y-test-shape"><a href="#print-‘the-y-test-is-shape-is-’-format-y-test-y-test-shape" class="headerlink" title="print(‘the y_test is:{},shape is:{}’.format(y_test,y_test.shape))"></a>print(‘the y_test is:{},shape is:{}’.format(y_test,y_test.shape))</h1><p>  prediction2 = model.predict_proba(X_train)<br>  area2 = auc_score(y_train,prediction2[:,1])<br>  print(‘the train score is:{}’.format(area2))</p><p>  i = 0<br>  wrong_index = [] # 得到L1学习器预测错误的所有样本index<br>  for j in y_test.index:</p><pre><code>if (prediction_L1[i] != y_test[j]):    wrong_index.append(j)    i += 1</code></pre><p>  print(“the num of wrong predictions is:{}”.format(i))</p><p>  right_index = [] # 取与预测错误的index数目相同的预测正确的样本index<br>  num = 0<br>  for index in y_test.index:</p><pre><code>if index not in wrong_index:    right_index.append(index)    num += 1     if num == i: # 82915需要根据新数据集进行更改        break</code></pre><p>  print(“the num of right index is:{}”.format(len(right_index)))</p><h1 id="for-num-in-range-82915-82915需要根据新数据集进行更改"><a href="#for-num-in-range-82915-82915需要根据新数据集进行更改" class="headerlink" title="for num in range(82915): # 82915需要根据新数据集进行更改"></a>for num in range(82915): # 82915需要根据新数据集进行更改</h1><h1 id="right-index-append-right-indexs-num"><a href="#right-index-append-right-indexs-num" class="headerlink" title="right_index.append(right_indexs[num])"></a>right_index.append(right_indexs[num])</h1><p>  total_index = wrong_index + right_index<br>  return model, predictors,total_index,prediction_L1</p></li></ul><p>def model_L2(data):<br>    label = ‘LABEL’<br>    IDcol = ‘PERSONID’<br>    predictors = [x for x in data.columns if x not in [label,IDcol]]<br>    X_train, X_test, y_train, y_test = train_test_split(data[predictors],data[label],test_size=0.4,random_state=2018)<br>    model = lgb.LGBMClassifier(objective=’binary’,metric=’AUC’,num_leaves=90,depth=8,learning_rate=0.01,<br>                               colsample_bytree=0.8,n_estimators=2000,seed=2018,subsample=0.9,boosting=’rf’,<br>                               boosting_type=’gbdt’,reg_alpha=0.0,reg_lambda=0.0,bagging_fraction=0.7,<br>                               bagging_freq=1)<br>    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric=’AUC’,early_stopping_rounds=0,verbose=False)<br>    prediction_L2 = model.predict(X_test)<br>    L3_index = []<br>    i = 0<br>    for index in y_test.index:<br>        if(prediction_L2[i] != y_test[index]):<br>            L3_index.append(index)<br>            i += 1<br>    return model,L3_index,prediction_L2</p><p>def model_L3(data):<br>    label = ‘LABEL’<br>    IDcol = ‘PERSONID’<br>    predictors = [x for x in data.columns if x not in [label,IDcol]]<br>    X_train,X_test,y_train,y_test = train_test_split(data[predictors],data[label],test_size=0.1,random_state=2018)<br>    model = lgb.LGBMClassifier(objective=’binary’,metric=’AUC’,num_leaves=90,depth=8,learning_rate=0.01,<br>                               colsample_bytree=0.8,n_estimators=2000,seed=2018,subsample=0.9,boosting=’rf’,<br>                               boosting_type=’gbdt’,reg_alpha=0.0,reg_lambda=0.0,bagging_fraction=0.7,<br>                               bagging_freq=1)<br>    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric=’AUC’,early_stopping_rounds=3000,verbose=False)<br>    return model    </p><p>model_L1,predictors,L2_index,prediction_L1 = model_L1(train)</p><p>#train =train.reset_index()<br>data_L2 = train[train.index.isin(L2_index)]</p><p>#data_L2 = data_L2.set_index(‘orderid’)<br>model_L2,L3_index,prediction_L2 = model_L2(data_L2)<br>data_L3 = train[train.index.isin(L3_index)]</p><p>#data_L3 = data_L3.set_index(‘orderid’)<br>model_L3 = model_L3(data_L3)</p><p>X_train, X_test, y_train, y_test = train_test_split(train[predictors],train[‘LABEL’],test_size=0.4,random_state=2018)</p><p>label = []<br>L1_result = model_L1.predict_proba(X_test)[:,1]<br>L2_result = model_L2.predict_proba(X_test)[:,1]<br>L3_result = model_L3.predict_proba(X_test)[:,1]<br>for i in range(len(X_test)):<br>    if((L1_result[i] &gt;= 0.5) and (L2_result[i] &gt;= 0.5) and (L3_result[i] &gt;= 0.5)) or ((L1_result[i] &lt; 0.5) and (L2_result[i] &lt; 0.5) and (L3_result[i] &lt; 0.5)):<br>        label.append((L1_result[i]+L2_result[i]+L3_result[i])/3.0)<br>    elif ((L1_result[i] &gt;= 0.5) and (L2_result[i] &gt;= 0.5) and (L3_result[i] &lt; 0.5)) or ((L1_result[i] &lt; 0.5) and (L2_result[i] &lt; 0.5) and (L3_result[i] &gt;= 0.5)):<br>        label.append((L1_result[i]+L2_result[i])/2.0)<br>    elif ((L1_result[i] &gt;= 0.5) and (L2_result[i] &lt; 0.5) and (L3_result[i] &lt; 0.5)) or ((L1_result[i] &lt; 0.5) and (L2_result[i] &gt;= 0.5) and (L3_result[i] &gt;= 0.5)):<br>        label.append((L2_result[i]+L3_result[i])/2.0)<br>    elif ((L1_result[i] &gt;= 0.5) and (L2_result[i] &lt; 0.5) and (L3_result[i] &gt;= 0.5)) or ((L1_result[i] &lt; 0.5) and (L2_result[i] &gt;= 0.5) and (L3_result[i] &lt; 0.5)):<br>        label.append((L1_result[i]+L3_result[i])/2.0)</p><h1 id="if-L1-result-i-lt-0-5-and-L2-result-i-gt-0-5-and-L3-result-i-gt-0-5"><a href="#if-L1-result-i-lt-0-5-and-L2-result-i-gt-0-5-and-L3-result-i-gt-0-5" class="headerlink" title="if((L1_result[i] &lt; 0.5) and (L2_result[i] &gt;= 0.5) and (L3_result[i] &gt;= 0.5)):"></a>if((L1_result[i] &lt; 0.5) and (L2_result[i] &gt;= 0.5) and (L3_result[i] &gt;= 0.5)):</h1><h1 id="noroom-append-L2-result-i-L3-result-i-2-0"><a href="#noroom-append-L2-result-i-L3-result-i-2-0" class="headerlink" title="noroom.append((L2_result[i]+L3_result[i])/2.0)"></a>noroom.append((L2_result[i]+L3_result[i])/2.0)</h1><h1 id="else"><a href="#else" class="headerlink" title="else:"></a>else:</h1><h1 id="noroom-append-L1-result-i-L2-result-i-L3-result-i-3-0"><a href="#noroom-append-L1-result-i-L2-result-i-L3-result-i-3-0" class="headerlink" title="noroom.append((L1_result[i]+L2_result[i]+L3_result[i])/3.0)"></a>noroom.append((L1_result[i]+L2_result[i]+L3_result[i])/3.0)</h1><p>‘’’</p><h2 id="以下方法会破坏某些类的样本分布"><a href="#以下方法会破坏某些类的样本分布" class="headerlink" title="以下方法会破坏某些类的样本分布"></a>以下方法会破坏某些类的样本分布</h2><ol start="2"><li><p>在训练模型时，可以增加小类样本的权重，降低大类样本的权重，从而使得分类器将重点集中在小类样本上。（这种方法其实改变了原始数据集的样本分布，得到的模型性能甚至会变差）<br>开始时可以设置每个类别的权重为样本个数比例的倒数，然后使用过采样方法进行调优。</p></li><li><p>设大类样本的个数是小类样本的L倍，那么在随机梯度下降算法中，每次遇到一个小类样本进行训练时，训练L次。</p></li><li><p>将大类样本划分到L个聚类，然后训练L个分类器，每个分类器使用大类中的一个簇和所有的小类样本进行训练。最后采用L个分类器投票的方法进行分类，如果是回归问题，则采用平均值。</p></li><li><p>设小类中有N个样本，将大类聚类成N个簇，然后用每个簇的中心组成N个样本，和小类中的所有样本一起训练。</p></li></ol>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 非均衡样本 </tag>
            
            <tag> 模型训练 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>湖人总冠军!</title>
      <link href="/2018/07/02/%E6%B9%96%E4%BA%BA%E6%80%BB%E5%86%A0%E5%86%9B/"/>
      <url>/2018/07/02/%E6%B9%96%E4%BA%BA%E6%80%BB%E5%86%A0%E5%86%9B/</url>
      <content type="html"><![CDATA[<p>故事的最后老詹竟然来了湖人，希望我湖能够抓住机会，重振威风，不再让湖人总冠军这句话成为别人的笑谈。</p>]]></content>
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>改善深层神经网络-Gradient+Checking</title>
      <link href="/2018/06/26/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Gradient-Checking/"/>
      <url>/2018/06/26/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Gradient-Checking/</url>
      <content type="html"><![CDATA[<h1 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h1><p>Welcome to the final assignment for this week! In this assignment you will learn to implement and use gradient checking. </p><p>You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud–whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user’s account has been taken over by a hacker. </p><p>But backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company’s CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, “Give me a proof that your backpropagation is actually working!” To give this reassurance, you are going to use “gradient checking”.</p><p>Let’s do it!<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Packages</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> gc_utils <span class="keyword">import</span> sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector</span><br></pre></td></tr></table></figure><h2 id="1-How-does-gradient-checking-work"><a href="#1-How-does-gradient-checking-work" class="headerlink" title="1) How does gradient checking work?"></a>1) How does gradient checking work?</h2><p>Backpropagation computes the gradients $\frac{\partial J}{\partial \theta}$, where $\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and your loss function.</p><p>Because forward propagation is relatively easy to implement, you’re confident you got that right, and so you’re almost  100% sure that you’re computing the cost $J$ correctly. Thus, you can use your code for computing $J$ to verify the code for computing $\frac{\partial J}{\partial \theta}$. </p><p>Let’s look back at the definition of a derivative (or gradient):<br>$$ \frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} \tag{1}$$</p><p>If you’re not familiar with the “$\displaystyle \lim_{\varepsilon \to 0}$” notation, it’s just a way of saying “when $\varepsilon$ is really really small.”</p><p>We know the following:</p><ul><li>$\frac{\partial J}{\partial \theta}$ is what you want to make sure you’re computing correctly. </li><li>You can compute $J(\theta + \varepsilon)$ and $J(\theta - \varepsilon)$ (in the case that $\theta$ is a real number), since you’re confident your implementation for $J$ is correct. </li></ul><p>Lets use equation (1) and a small value for $\varepsilon$ to convince your CEO that your code for computing  $\frac{\partial J}{\partial \theta}$ is correct!</p><h2 id="2-1-dimensional-gradient-checking"><a href="#2-1-dimensional-gradient-checking" class="headerlink" title="2) 1-dimensional gradient checking"></a>2) 1-dimensional gradient checking</h2><p>Consider a 1D linear function $J(\theta) = \theta x$. The model contains only a single real-valued parameter $\theta$, and takes $x$ as input.</p><p>You will implement code to compute $J(.)$ and its derivative $\frac{\partial J}{\partial \theta}$. You will then use gradient checking to make sure your derivative computation for $J$ is correct. </p><p><img src="/img/1Dgrad_kiank.png" style="width:600px;height:250px;"></p><caption><center> <u> <strong>Figure 1</strong> </u>: <strong>1D linear model</strong><br> </center></caption><p>The diagram above shows the key computation steps: First start with $x$, then evaluate the function $J(x)$ (“forward propagation”). Then compute the derivative $\frac{\partial J}{\partial \theta}$ (“backward propagation”). </p><p><strong>Exercise</strong>: implement “forward propagation” and “backward propagation” for this simple function. I.e., compute both $J(.)$ (“forward propagation”) and its derivative with respect to $\theta$ (“backward propagation”), in two separate functions. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- the value of function J, computed using the formula J(theta) = theta * x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    J = theta * x</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x, theta = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">J = forward_propagation(x, theta)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"J = "</span> + str(J))</span><br></pre></td></tr></table></figure><pre><code>J = 8</code></pre><p><strong>Expected Output</strong>:</p><table style=""><br>    <tr><br>        <td>  <strong> J </strong>  </td><br>        <td> 8</td><br>    </tr><br></table><p><strong>Exercise</strong>: Now, implement the backward propagation step (derivative computation) of Figure 1. That is, compute the derivative of $J(\theta) = \theta x$ with respect to $\theta$. To save you from doing the calculus, you should get $dtheta = \frac { \partial J }{ \partial \theta} = x$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the derivative of J with respect to theta (see Figure 1).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dtheta -- the gradient of the cost with respect to theta</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dtheta = x</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dtheta</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x, theta = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">dtheta = backward_propagation(x, theta)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dtheta = "</span> + str(dtheta))</span><br></pre></td></tr></table></figure><pre><code>dtheta = 2</code></pre><p><strong>Expected Output</strong>:</p><table><br>    <tr><br>        <td>  <strong> dtheta </strong>  </td><br>        <td> 2 </td><br>    </tr><br></table><p><strong>Exercise</strong>: To show that the <code>backward_propagation()</code> function is correctly computing the gradient $\frac{\partial J}{\partial \theta}$, let’s implement gradient checking.</p><p><strong>Instructions</strong>:</p><ul><li>First compute “gradapprox” using the formula above (1) and a small value of $\varepsilon$. Here are the Steps to follow:<ol><li>$\theta^{+} = \theta + \varepsilon$</li><li>$\theta^{-} = \theta - \varepsilon$</li><li>$J^{+} = J(\theta^{+})$</li><li>$J^{-} = J(\theta^{-})$</li><li>$gradapprox = \frac{J^{+} - J^{-}}{2  \varepsilon}$</li></ol></li><li>Then compute the gradient using backward propagation, and store the result in a variable “grad”</li><li>Finally, compute the relative difference between “gradapprox” and the “grad” using the following formula:<br>$$ difference = \frac {\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2} \tag{2}$$<br>You will need 3 Steps to compute this formula:<ul><li>1’. compute the numerator using np.linalg.norm(…)</li><li>2’. compute the denominator. You will need to call np.linalg.norm(…) twice.</li><li>3’. divide them.</li></ul></li><li>If this difference is small (say less than $10^{-7}$), you can be quite confident that you have computed your gradient correctly. Otherwise, there may be a mistake in the gradient computation. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: gradient_check</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(x, theta, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></span><br><span class="line">    thetaplus = theta + epsilon                               <span class="comment"># Step 1</span></span><br><span class="line">    thetaminus = theta - epsilon                              <span class="comment"># Step 2</span></span><br><span class="line">    J_plus = forward_propagation(x, thetaplus)                                  <span class="comment"># Step 3</span></span><br><span class="line">    J_minus = forward_propagation(x, thetaminus)                                 <span class="comment"># Step 4</span></span><br><span class="line">    gradapprox = (J_plus - J_minus) / <span class="number">2</span> / epsilon                              <span class="comment"># Step 5</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check if gradapprox is close enough to the output of backward_propagation()</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    grad = backward_propagation(x, theta)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    numerator = np.linalg.norm(grad-gradapprox)                               <span class="comment"># Step 1'</span></span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                             <span class="comment"># Step 2'</span></span><br><span class="line">    difference = numerator / denominator                              <span class="comment"># Step 3'</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> difference &lt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is correct!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is wrong!"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x, theta = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">difference = gradient_check(x, theta)</span><br><span class="line">print(<span class="string">"difference = "</span> + str(difference))</span><br></pre></td></tr></table></figure><pre><code>The gradient is correct!difference = 2.919335883291695e-10</code></pre><p><strong>Expected Output</strong>:<br>The gradient is correct!</p><table><br>    <tr><br>        <td>  <strong> difference </strong>  </td><br>        <td> 2.9193358103083e-10 </td><br>    </tr><br></table><p>Congrats, the difference is smaller than the $10^{-7}$ threshold. So you can have high confidence that you’ve correctly computed the gradient in <code>backward_propagation()</code>. </p><p>Now, in the more general case, your cost function $J$ has more than a single 1D input. When you are training a neural network, $\theta$ actually consists of multiple matrices $W^{[l]}$ and biases $b^{[l]}$! It is important to know how to do a gradient check with higher-dimensional inputs. Let’s do it!</p><h2 id="3-N-dimensional-gradient-checking"><a href="#3-N-dimensional-gradient-checking" class="headerlink" title="3) N-dimensional gradient checking"></a>3) N-dimensional gradient checking</h2><p>The following figure describes the forward and backward propagation of your fraud detection model.</p><p><img src="/img/NDgrad_kiank.png" style="width:600px;height:400px;"></p><caption><center> <u> <strong>Figure 2</strong> </u>: <strong>deep neural network</strong><br><em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</em></center></caption><p>Let’s look at your implementations for forward propagation and backward propagation. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_n</span><span class="params">(X, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation (and computes the cost) presented in Figure 3.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- training set for m examples</span></span><br><span class="line"><span class="string">    Y -- labels for m examples </span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (5, 4)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (5, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 5)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- the cost function (logistic cost for one example)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cost</span></span><br><span class="line">    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(<span class="number">1</span> - A3), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">1.</span>/m * np.sum(logprobs)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, cache</span><br></pre></td></tr></table></figure><p>Now, run backward propagation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_n</span><span class="params">(X, Y, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in figure 2.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    Y -- true "label"</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_n()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) </span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,</span><br><span class="line">                 <span class="string">"dA2"</span>: dA2, <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2,</span><br><span class="line">                 <span class="string">"dA1"</span>: dA1, <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>You obtained some results on the fraud detection test set but you are not 100% sure of your model. Nobody’s perfect! Let’s implement gradient checking to verify if your gradients are correct.</p><p><strong>How does gradient checking work?</strong>.</p><p>As in 1) and 2), you want to compare “gradapprox” to the gradient computed by backpropagation. The formula is still:</p><p>$$ \frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} \tag{1}$$</p><p>However, $\theta$ is not a scalar anymore. It is a dictionary called “parameters”. We implemented a function “<code>dictionary_to_vector()</code>“ for you. It converts the “parameters” dictionary into a vector called “values”, obtained by reshaping all parameters (W1, b1, W2, b2, W3, b3) into vectors and concatenating them.</p><p>The inverse function is “<code>vector_to_dictionary</code>“ which outputs back the “parameters” dictionary.</p><p><img src="/img/dictionary_to_vector.png" style="width:600px;height:400px;"></p><caption><center> <u> <strong>Figure 2</strong> </u>: <strong>dictionary_to_vector() and vector_to_dictionary()</strong><br> You will need these functions in gradient_check_n()</center></caption><p>We have also converted the “gradients” dictionary into a vector “grad” using gradients_to_vector(). You don’t need to worry about that.</p><p><strong>Exercise</strong>: Implement gradient_check_n().</p><p><strong>Instructions</strong>: Here is pseudo-code that will help you implement the gradient check.</p><p>For each i in num_parameters:</p><ul><li>To compute <code>J_plus[i]</code>:<ol><li>Set $\theta^{+}$ to <code>np.copy(parameters_values)</code></li><li>Set $\theta^{+}_i$ to $\theta^{+}_i + \varepsilon$</li><li>Calculate $J^{+}_i$ using to <code>forward_propagation_n(x, y, vector_to_dictionary(</code>$\theta^{+}$ <code>))</code>.     </li></ol></li><li>To compute <code>J_minus[i]</code>: do the same thing with $\theta^{-}$</li><li>Compute $gradapprox[i] = \frac{J^{+}_i - J^{-}_i}{2 \varepsilon}$</li></ul><p>Thus, you get a vector gradapprox, where gradapprox[i] is an approximation of the gradient with respect to <code>parameter_values[i]</code>. You can now compare this gradapprox vector to the gradients vector from backpropagation. Just like for the 1D case (Steps 1’, 2’, 3’), compute:<br>$$ difference = \frac {| grad - gradapprox |_2}{| grad |_2 + | gradapprox |_2 } \tag{3}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: gradient_check_n</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check_n</span><span class="params">(parameters, gradients, X, Y, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. </span></span><br><span class="line"><span class="string">    x -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    y -- true "label"</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set-up variables</span></span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[<span class="number">0</span>]</span><br><span class="line">    J_plus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    J_minus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gradapprox</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".</span></span><br><span class="line">        <span class="comment"># "_" is used because the function you have to outputs two parameters but we only care about the first one</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 3 lines)</span></span><br><span class="line">        thetaplus = np.copy(parameters_values)                                     <span class="comment"># Step 1</span></span><br><span class="line">        thetaplus[i][<span class="number">0</span>] = thetaplus[i][<span class="number">0</span>] + epsilon                                <span class="comment"># Step 2</span></span><br><span class="line">        J_plus[i], _ = forward_propagation_n(X,Y,vector_to_dictionary(thetaplus))                                   <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 3 lines)</span></span><br><span class="line">        thetaminus = np.copy(parameters_values)                                     <span class="comment"># Step 1</span></span><br><span class="line">        thetaminus[i][<span class="number">0</span>] = thetaminus[i][<span class="number">0</span>] - epsilon                               <span class="comment"># Step 2        </span></span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X,Y,vector_to_dictionary(thetaminus))                                  <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute gradapprox[i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">        gradapprox[i] = (J_plus[i]-J_minus[i]) / (<span class="number">2</span> * epsilon)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compare gradapprox to backward propagation gradients by computing difference.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    numerator = np.linalg.norm(grad-gradapprox,ord=<span class="number">2</span>)                                          <span class="comment"># Step 1'</span></span><br><span class="line">    denominator = np.linalg.norm(grad,ord=<span class="number">2</span>) + np.linalg.norm(gradapprox,ord=<span class="number">2</span>)                                         <span class="comment"># Step 2'</span></span><br><span class="line">    difference = numerator / denominator                                          <span class="comment"># Step 3'</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> difference &gt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[93m"</span> + <span class="string">"There is a mistake in the backward propagation! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[92m"</span> + <span class="string">"Your backward propagation works perfectly fine! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, Y, parameters = gradient_check_n_test_case()</span><br><span class="line"></span><br><span class="line">cost, cache = forward_propagation_n(X, Y, parameters)</span><br><span class="line">gradients = backward_propagation_n(X, Y, cache)</span><br><span class="line">difference = gradient_check_n(parameters, gradients, X, Y)</span><br></pre></td></tr></table></figure><pre><code>[93mThere is a mistake in the backward propagation! difference = 1.1885552035482149e-07[0m</code></pre><p><strong>Expected output</strong>:</p><table><br>    <tr><br>        <td>  <strong> There is a mistake in the backward propagation!</strong>  </td><br>        <td> difference = 0.285093156781 </td><br>    </tr><br></table><p>It seems that there were errors in the <code>backward_propagation_n</code> code we gave you! Good that you’ve implemented the gradient check. Go back to <code>backward_propagation</code> and try to find/correct the errors <em>(Hint: check dW2 and db1)</em>. Rerun the gradient check when you think you’ve fixed it. Remember you’ll need to re-execute the cell defining <code>backward_propagation_n()</code> if you modify the code. </p><p>Can you get gradient check to declare your derivative computation correct? Even though this part of the assignment isn’t graded, we strongly urge you to try to find the bug and re-run gradient check until you’re convinced backprop is now correctly implemented. </p><p><strong>Note</strong> </p><ul><li>Gradient Checking is slow! Approximating the gradient with $\frac{\partial J}{\partial \theta} \approx  \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}$ is computationally costly. For this reason, we don’t run gradient checking at every iteration during training. Just a few times to check if the gradient is correct. </li><li>Gradient Checking, at least as we’ve presented it, doesn’t work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout. </li></ul><p>Congrats, you can be confident that your deep learning model for fraud detection is working correctly! You can even use this to convince your CEO. :) </p><p><font color="blue"><br><strong>What you should remember from this notebook</strong>:</font></p><ul><li>Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).</li><li>Gradient checking is slow, so we don’t run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process. </li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>改善深层神经网络-Regularization</title>
      <link href="/2018/06/26/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Regularization/"/>
      <url>/2018/06/26/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Regularization/</url>
      <content type="html"><![CDATA[<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><p>Welcome to the second assignment of this week. Deep Learning models have so much flexibility and capacity that <strong>overfitting can be a serious problem</strong>, if the training dataset is not big enough. Sure it does well on the training set, but the learned network <strong>doesn’t generalize to new examples</strong> that it has never seen!</p><p><strong>You will learn to:</strong> Use regularization in your deep learning models.</p><p>Let’s first import the packages you are going to use.<br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import packages</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> compute_cost, predict, forward_propagation, backward_propagation, update_parameters</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure><pre><code>E:\DeepLearningAI作业\02-第二课 改善深层神经网络\第二课第一周编程作业\assignment1\reg_utils.py:85: SyntaxWarning: assertion is always true, perhaps remove parentheses?  assert(parameters[&apos;W&apos; + str(l)].shape == layer_dims[l], layer_dims[l-1])E:\DeepLearningAI作业\02-第二课 改善深层神经网络\第二课第一周编程作业\assignment1\reg_utils.py:86: SyntaxWarning: assertion is always true, perhaps remove parentheses?  assert(parameters[&apos;W&apos; + str(l)].shape == layer_dims[l], 1)E:\anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.  from ._conv import register_converters as _register_converters</code></pre><p><strong>Problem Statement</strong>: You have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France’s goal keeper should kick the ball so that the French team’s players can then hit it with their head. </p><p><img src="/img/field_kiank.png" style="width:600px;height:350px;"></p><caption><center> <u> <strong>Figure 1</strong> </u>: <strong>Football field</strong><br> The goal keeper kicks the ball in the air, the players of each team are fighting to hit the ball with their head </center></caption><p>They give you the following 2D dataset from France’s past 10 games.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y, test_X, test_Y = load_2D_dataset()</span><br></pre></td></tr></table></figure><p>Each dot corresponds to a position on the football field where a football player has hit the ball with his/her head after the French goal keeper has shot the ball from the left side of the football field.</p><ul><li>If the dot is blue, it means the French player managed to hit the ball with his/her head</li><li>If the dot is red, it means the other team’s player hit the ball with their head</li></ul><p><strong>Your goal</strong>: Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.</p><p><strong>Analysis of the dataset</strong>: This dataset is a little noisy, but it looks like a diagonal line separating the upper left half (blue) from the lower right half (red) would work well. </p><p>You will first try a non-regularized model. Then you’ll learn how to regularize it and decide which model you will choose to solve the French Football Corporation’s problem. </p><h2 id="1-Non-regularized-model"><a href="#1-Non-regularized-model" class="headerlink" title="1 - Non-regularized model"></a>1 - Non-regularized model</h2><p>You will use the following neural network (already implemented for you below). This model can be used:</p><ul><li>in <em>regularization mode</em> – by setting the <code>lambd</code> input to a non-zero value. We use “<code>lambd</code>“ instead of “<code>lambda</code>“ because “<code>lambda</code>“ is a reserved keyword in Python. </li><li>in <em>dropout mode</em> – by setting the <code>keep_prob</code> to a value less than one</li></ul><p>You will first try the model without any regularization. Then, you will implement:</p><ul><li><em>L2 regularization</em> – functions: “<code>compute_cost_with_regularization()</code>“ and “<code>backward_propagation_with_regularization()</code>“</li><li><em>Dropout</em> – functions: “<code>forward_propagation_with_dropout()</code>“ and “<code>backward_propagation_with_dropout()</code>“</li></ul><p>In each part, you will run this model with the correct inputs so that it calls the functions you’ve implemented. Take a look at the code below to familiarize yourself with the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, print_cost = True, lambd = <span class="number">0</span>, keep_prob = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- If True, print the cost every 10000 iterations</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learned by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                        <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="keyword">if</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span>:</span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="keyword">assert</span>(lambd==<span class="number">0</span> <span class="keyword">or</span> keep_prob==<span class="number">1</span>)    <span class="comment"># it is possible to use both L2 regularization and dropout, </span></span><br><span class="line">                                            <span class="comment"># but this assignment will only explore one at a time</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span> <span class="keyword">and</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        <span class="keyword">elif</span> lambd != <span class="number">0</span>:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 10000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (x1,000)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Let’s train the model without any regularization, and observe the accuracy on the train/test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the training set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6557412523481002Cost after iteration 10000: 0.1632998752572419Cost after iteration 20000: 0.13851642423239133</code></pre><p><img src="/img/output_9_1.png" alt="png"></p><pre><code>On the training set:Accuracy: 0.9478672985781991On the test set:Accuracy: 0.915</code></pre><p>The train accuracy is 94.8% while the test accuracy is 91.5%. This is the <strong>baseline model</strong> (you will observe the impact of regularization on this model). Run the following code to plot the decision boundary of your model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model without regularization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="/img/output_11_0.png" alt="png"></p><p>The non-regularized model is obviously overfitting the training set. It is fitting the noisy points! Lets now look at two techniques to reduce overfitting.</p><h2 id="2-L2-Regularization"><a href="#2-L2-Regularization" class="headerlink" title="2 - L2 Regularization"></a>2 - L2 Regularization</h2><p>The standard way to avoid overfitting is called <strong>L2 regularization</strong>. It consists of appropriately modifying your cost function, from:<br>$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{<a href="i">L</a>}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right) \large{)} \tag{1}$$<br>To:<br>$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{<a href="i">L</a>}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}$$</p><p>Let’s modify your cost and observe the consequences.</p><p><strong>Exercise</strong>: Implement <code>compute_cost_with_regularization()</code> which computes the cost given by formula (2). To calculate $\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}$  , use :<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(np.square(Wl))</span><br></pre></td></tr></table></figure></p><p>Note that you have to do this for $W^{[1]}$, $W^{[2]}$ and $W^{[3]}$, then sum the three terms and multiply by $ \frac{1}{m} \frac{\lambda}{2} $.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost_with_regularization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function with L2 regularization. See formula (2) above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing parameters of the model</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - value of the regularized loss function (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) <span class="comment"># This gives you the cross-entropy part of the cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    L2_regularization_cost = (np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))*lambd/<span class="number">2</span>/m</span><br><span class="line">    <span class="comment"># L2_regularization_cost = np.sum(np.sum(np.square(W1)),np.sum(np.square(W2)),np.sum(np.square(W3)))*lambd/2/m</span></span><br><span class="line">    <span class="comment">### END CODER HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A3, Y_assess, parameters = compute_cost_with_regularization_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = <span class="number">0.1</span>)))</span><br></pre></td></tr></table></figure><pre><code>cost = 1.7864859451590758</code></pre><p><strong>Expected Output</strong>: </p><table><br>    <tr><br>    <td><br>    <strong>cost</strong><br>    </td><br>        <td><br>    1.78648594516<br>    </td><br><br>    </tr><br><br></table> <p>Of course, because you changed the cost, you have to change backward propagation as well! All the gradients have to be computed with respect to this new cost. </p><p><strong>Exercise</strong>: Implement the changes needed in backward propagation to take into account regularization. The changes only concern dW1, dW2 and dW3. For each, you have to add the regularization term’s gradient ($\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W$).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation_with_regularization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation()</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + lambd*W3/m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) + lambd*W2/m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) + lambd*W1/m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()</span><br><span class="line"></span><br><span class="line">grads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW2 = "</span>+ str(grads[<span class="string">"dW2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW3 = "</span>+ str(grads[<span class="string">"dW3"</span>]))</span><br></pre></td></tr></table></figure><pre><code>dW1 = [[-0.25604646  0.12298827 -0.28297129] [-0.17706303  0.34536094 -0.4410571 ]]dW2 = [[ 0.79276486  0.85133918] [-0.0957219  -0.01720463] [-0.13100772 -0.03750433]]dW3 = [[-1.77691347 -0.11832879 -0.09397446]]</code></pre><p><strong>Expected Output</strong>:</p><table><br>    <tr><br>    <td><br>    <strong>dW1</strong><br>    </td><br>        <td><br>    [[-0.25604646  0.12298827 -0.28297129]<br> [-0.17706303  0.34536094 -0.4410571 ]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>dW2</strong><br>    </td><br>        <td><br>    [[ 0.79276486  0.85133918]<br> [-0.0957219  -0.01720463]<br> [-0.13100772 -0.03750433]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>dW3</strong><br>    </td><br>        <td><br>    [[-1.77691347 -0.11832879 -0.09397446]]<br>    </td><br>    </tr><br></table> <p>Let’s now run the model with L2 regularization $(\lambda = 0.7)$. The <code>model()</code> function will call: </p><ul><li><code>compute_cost_with_regularization</code> instead of <code>compute_cost</code></li><li><code>backward_propagation_with_regularization</code> instead of <code>backward_propagation</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6974484493131264Cost after iteration 10000: 0.2684918873282239Cost after iteration 20000: 0.2680916337127301</code></pre><p><img src="/img/output_22_1.png" alt="png"></p><pre><code>On the train set:Accuracy: 0.9383886255924171On the test set:Accuracy: 0.93</code></pre><p>Congrats, the test set accuracy increased to 93%. You have saved the French football team!</p><p>You are not overfitting the training data anymore. Let’s plot the decision boundary.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with L2-regularization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="/img/output_24_0.png" alt="png"></p><p><strong>Observations</strong>:</p><ul><li>The value of $\lambda$ is a hyperparameter that you can tune using a dev set.</li><li>L2 regularization makes your decision boundary smoother. If $\lambda$ is too large, it is also possible to “oversmooth”, resulting in a model with high bias.</li></ul><p><strong>What is L2-regularization actually doing?</strong>:</p><p>L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. </p><p><font color="blue"><br><strong>What you should remember</strong> – the implications of L2-regularization on:</font></p><ul><li>The cost computation:<ul><li>A regularization term is added to the cost</li></ul></li><li>The backpropagation function:<ul><li>There are extra terms in the gradients with respect to weight matrices</li></ul></li><li>Weights end up smaller (“weight decay”): <ul><li>Weights are pushed to smaller values.</li></ul></li></ul><h2 id="3-Dropout"><a href="#3-Dropout" class="headerlink" title="3 - Dropout"></a>3 - Dropout</h2><p>Finally, <strong>dropout</strong> is a widely used regularization technique that is specific to deep learning.<br><strong>It randomly shuts down some neurons in each iteration.</strong> Watch these two videos to see what this means!</p><!--To understand drop-out, consider this conversation with a friend:- Friend: "Why do you need all these neurons to train your network and classify images?". - You: "Because each neuron contains a weight and can learn specific features/details/shape of an image. The more neurons I have, the more featurse my model learns!"- Friend: "I see, but are you sure that your neurons are learning different features and not all the same features?"- You: "Good point... Neurons in the same layer actually don't talk to each other. It should be definitly possible that they learn the same image features/shapes/forms/details... which would be redundant. There should be a solution."!--> <center><br><video width="620" height="440" src="images/dropout1_kiank.mp4" type="video/mp4" controls><br></video><br></center><br><br><br><caption><center> <u> Figure 2 </u>: Drop-out on the second hidden layer. <br> At each iteration, you shut down (= set to zero) each neuron of a layer with probability $1 - keep_prob$ or keep it with probability $keep_prob$ (50% here). The dropped neurons don’t contribute to the training in both the forward and backward propagations of the iteration. </center></caption><br><br><center><br><video width="620" height="440" src="images/dropout2_kiank.mp4" type="video/mp4" controls><br></video><br></center><caption><center> <u> Figure 3 </u>: Drop-out on the first and third hidden layers. <br> $1^{st}$ layer: we shut down on average 40% of the neurons.  $3^{rd}$ layer: we shut down on average 20% of the neurons. </center></caption><p>When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. </p><h3 id="3-1-Forward-propagation-with-dropout"><a href="#3-1-Forward-propagation-with-dropout" class="headerlink" title="3.1 - Forward propagation with dropout"></a>3.1 - Forward propagation with dropout</h3><p><strong>Exercise</strong>: Implement the forward propagation with dropout. You are using a 3 layer neural network, and will add dropout to the first and second hidden layers. We will not apply dropout to the input layer or output layer. </p><p><strong>Instructions</strong>:<br>You would like to shut down some neurons in the first and second layers. To do that, you are going to carry out 4 Steps:</p><ol><li>In lecture, we dicussed creating a variable $d^{[1]}$ with the same shape as $a^{[1]}$ using <code>np.random.rand()</code> to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix $D^{[1]} = [d^{<a href="1">1</a>} d^{<a href="2">1</a>} … d^{<a href="m">1</a>}] $ of the same dimension as $A^{[1]}$.</li><li>Set each entry of $D^{[1]}$ to be 0 with probability (<code>1-keep_prob</code>) or 1 with probability (<code>keep_prob</code>), by thresholding values in $D^{[1]}$ appropriately. Hint: to set all the entries of a matrix X to 0 (if entry is less than 0.5) or 1 (if entry is more than 0.5) you would do: <code>X = (X &lt; 0.5)</code>. Note that 0 and 1 are respectively equivalent to False and True.</li><li>Set $A^{[1]}$ to $A^{[1]} * D^{[1]}$. (You are shutting down some neurons). You can think of $D^{[1]}$ as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.</li><li>Divide $A^{[1]}$ by <code>keep_prob</code>. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation_with_dropout</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (20, 2)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (20, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 20)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span></span><br><span class="line"><span class="string">    cache -- tuple, information stored for computing the backward propagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])                                        <span class="comment"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D1 = np.where(D1 &lt;= keep_prob, <span class="number">1</span>, <span class="number">0</span>)                                        <span class="comment"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A1 = A1 * D1                                       <span class="comment"># Step 3: shut down some neurons of A1</span></span><br><span class="line">    A1 = A1 / keep_prob                                         <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])                                        <span class="comment"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span></span><br><span class="line">    D2 = np.where(D2 &lt;= keep_prob, <span class="number">1</span>, <span class="number">0</span>)                                         <span class="comment"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A2 = A2 * D2                                         <span class="comment"># Step 3: shut down some neurons of A2</span></span><br><span class="line">    A2 = A2 / keep_prob                                         <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_assess, parameters = forward_propagation_with_dropout_test_case()</span><br><span class="line"></span><br><span class="line">A3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"A3 = "</span> + str(A3))</span><br></pre></td></tr></table></figure><pre><code>A3 = [[0.36974721 0.00305176 0.04565099 0.49683389 0.36974721]]</code></pre><p><strong>Expected Output</strong>: </p><table><br>    <tr><br>    <td><br>    <strong>A3</strong><br>    </td><br>        <td><br>    [[ 0.36974721  0.00305176  0.04565099  0.49683389  0.36974721]]<br>    </td><br><br>    </tr><br><br></table> <h3 id="3-2-Backward-propagation-with-dropout"><a href="#3-2-Backward-propagation-with-dropout" class="headerlink" title="3.2 - Backward propagation with dropout"></a>3.2 - Backward propagation with dropout</h3><p><strong>Exercise</strong>: Implement the backward propagation with dropout. As before, you are training a 3 layer network. Add dropout to the first and second hidden layers, using the masks $D^{[1]}$ and $D^{[2]}$ stored in the cache. </p><p><strong>Instruction</strong>:<br>Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps:</p><ol><li>You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to <code>A1</code>. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to <code>dA1</code>. </li><li>During forward propagation, you had divided <code>A1</code> by <code>keep_prob</code>. In backpropagation, you’ll therefore have to divide <code>dA1</code> by <code>keep_prob</code> again (the calculus interpretation is that if $A^{[1]}$ is scaled by <code>keep_prob</code>, then its derivative $dA^{[1]}$ is also scaled by the same <code>keep_prob</code>).</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation_with_dropout</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added dropout.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_with_dropout()</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA2 = dA2 * D2              <span class="comment"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA2 = dA2 / keep_prob              <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA1 = dA1 * D1              <span class="comment"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA1 = dA1 / keep_prob              <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess, cache = backward_propagation_with_dropout_test_case()</span><br><span class="line"></span><br><span class="line">gradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob = <span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA1 = "</span> + str(gradients[<span class="string">"dA1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA2 = "</span> + str(gradients[<span class="string">"dA2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>dA1 = [[ 0.36544439  0.         -0.00188233  0.         -0.17408748] [ 0.65515713  0.         -0.00337459  0.         -0.        ]]dA2 = [[ 0.58180856  0.         -0.00299679  0.         -0.27715731] [ 0.          0.53159854 -0.          0.53159854 -0.34089673] [ 0.          0.         -0.00292733  0.         -0.        ]]</code></pre><p><strong>Expected Output</strong>: </p><table><br>    <tr><br>    <td><br>    <strong>dA1</strong><br>    </td><br>        <td><br>    [[ 0.36544439  0.         -0.00188233  0.         -0.17408748]<br> [ 0.65515713  0.         -0.00337459  0.         -0.        ]]<br>    </td><br><br>    </tr><br>    <tr><br>    <td><br>    <strong>dA2</strong><br>    </td><br>        <td><br>    [[ 0.58180856  0.         -0.00299679  0.         -0.27715731]<br> [ 0.          0.53159854 -0.          0.53159854 -0.34089673]<br> [ 0.          0.         -0.00292733  0.         -0.        ]]<br>    </td><br><br>    </tr><br></table> <p>Let’s now run the model with dropout (<code>keep_prob = 0.86</code>). It means at every iteration you shut down each neurons of layer 1 and 2 with 24% probability. The function <code>model()</code> will now call:</p><ul><li><code>forward_propagation_with_dropout</code> instead of <code>forward_propagation</code>.</li><li><code>backward_propagation_with_dropout</code> instead of <code>backward_propagation</code>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, keep_prob = <span class="number">0.86</span>, learning_rate = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6543912405149825E:\DeepLearningAI作业\02-第二课 改善深层神经网络\第二课第一周编程作业\assignment1\reg_utils.py:236: RuntimeWarning: divide by zero encountered in log  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)E:\DeepLearningAI作业\02-第二课 改善深层神经网络\第二课第一周编程作业\assignment1\reg_utils.py:236: RuntimeWarning: invalid value encountered in multiply  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)Cost after iteration 10000: 0.061016986574905605Cost after iteration 20000: 0.060582435798513114</code></pre><p><img src="/img/output_35_3.png" alt="png"></p><pre><code>On the train set:Accuracy: 0.9289099526066351On the test set:Accuracy: 0.95</code></pre><p>Dropout works great! The test accuracy has increased again (to 95%)! Your model is not overfitting the training set and does a great job on the test set. The French football team will be forever grateful to you! </p><p>Run the code below to plot the decision boundary.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with dropout"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="/img/output_37_0.png" alt="png"></p><p><strong>Note</strong>:</p><ul><li>A <strong>common mistake</strong> when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. </li><li>Deep learning frameworks like <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout" target="_blank" rel="noopener">tensorflow</a>, <a href="http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html" target="_blank" rel="noopener">PaddlePaddle</a>, <a href="https://keras.io/layers/core/#dropout" target="_blank" rel="noopener">keras</a> or <a href="http://caffe.berkeleyvision.org/tutorial/layers/dropout.html" target="_blank" rel="noopener">caffe</a> come with a dropout layer implementation. Don’t stress - you will soon learn some of these frameworks.</li></ul><p><font color="blue"><br><strong>What you should remember about dropout:</strong></font></p><ul><li>Dropout is a regularization technique.</li><li>You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.</li><li>Apply dropout both during forward and backward propagation.</li><li>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.  </li></ul><h2 id="4-Conclusions"><a href="#4-Conclusions" class="headerlink" title="4 - Conclusions"></a>4 - Conclusions</h2><p><strong>Here are the results of our three models</strong>: </p><table><br>    <tr><br>        <td><br>        <strong>model</strong><br>        </td><br>        <td><br>        <strong>train accuracy</strong><br>        </td><br>        <td><br>        <strong>test accuracy</strong><br>        </td><br><br>    </tr><br>        <td><br>        3-layer NN without regularization<br>        </td><br>        <td><br>        95%<br>        </td><br>        <td><br>        91.5%<br>        </td><br>    <tr><br>        <td><br>        3-layer NN with L2-regularization<br>        </td><br>        <td><br>        94%<br>        </td><br>        <td><br>        93%<br>        </td><br>    </tr><br>    <tr><br>        <td><br>        3-layer NN with dropout<br>        </td><br>        <td><br>        93%<br>        </td><br>        <td><br>        95%<br>        </td><br>    </tr><br></table> <p>Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system. </p><p>Congratulations for finishing this assignment! And also for revolutionizing French football. :-) </p><p><font color="blue"><br><strong>What we want you to remember from this notebook</strong>:</font></p><ul><li>Regularization will help you reduce overfitting.</li><li>Regularization will drive your weights to lower values.</li><li>L2 regularization and Dropout are two very effective regularization techniques.</li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> L2正则化 </tag>
            
            <tag> He正则化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>改善深层神经网络-Initialization</title>
      <link href="/2018/06/26/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Initialization/"/>
      <url>/2018/06/26/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Initialization/</url>
      <content type="html"><![CDATA[<h1 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h1><p>Welcome to the first assignment of “Improving Deep Neural Networks”. </p><p>Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  </p><p>If you completed the previous course of this specialization, you probably followed our instructions for weight initialization, and it has worked out so far. But how do you choose the initialization for a new neural network? In this notebook, you will see how different initializations lead to different results. </p><p>A well chosen initialization can:</p><ul><li>Speed up the convergence of gradient descent</li><li>Increase the odds of gradient descent converging to a lower training (and generalization) error </li></ul><a id="more"></a><p>To get started, run the following cell to load the packages and the planar dataset you will try to classify.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> sigmoid, relu, compute_loss, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load image dataset: blue/red dots in circles</span></span><br><span class="line">train_X, train_Y, test_X, test_Y = load_dataset()</span><br></pre></td></tr></table></figure><pre><code>E:\anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.  from ._conv import register_converters as _register_converters</code></pre><p>You would like a classifier to separate the blue dots from the red dots.</p><h2 id="1-Neural-Network-model"><a href="#1-Neural-Network-model" class="headerlink" title="1 - Neural Network model"></a>1 - Neural Network model</h2><p>You will use a 3-layer neural network (already implemented for you). Here are the initialization methods you will experiment with:  </p><ul><li><em>Zeros initialization</em> –  setting <code>initialization = &quot;zeros&quot;</code> in the input argument.</li><li><em>Random initialization</em> – setting <code>initialization = &quot;random&quot;</code> in the input argument. This initializes the weights to large random values.  </li><li><em>He initialization</em> – setting <code>initialization = &quot;he&quot;</code> in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015. </li></ul><p><strong>Instructions</strong>: Please quickly read over the code below, and run it. In the next part you will implement the three initialization methods that this <code>model()</code> calls.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">15000</span>, print_cost = True, initialization = <span class="string">"he"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for gradient descent </span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    initialization -- flag to choose which initialization to use ("zeros","random" or "he")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = [] <span class="comment"># to keep track of the loss</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    <span class="keyword">if</span> initialization == <span class="string">"zeros"</span>:</span><br><span class="line">        parameters = initialize_parameters_zeros(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"random"</span>:</span><br><span class="line">        parameters = initialize_parameters_random(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"he"</span>:</span><br><span class="line">        parameters = initialize_parameters_he(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        cost = compute_loss(a3, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        grads = backward_propagation(X, Y, cache)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the loss</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="2-Zero-initialization"><a href="#2-Zero-initialization" class="headerlink" title="2 - Zero initialization"></a>2 - Zero initialization</h2><p>There are two types of parameters to initialize in a neural network:</p><ul><li>the weight matrices $(W^{[1]}, W^{[2]}, W^{[3]}, …, W^{[L-1]}, W^{[L]})$</li><li>the bias vectors $(b^{[1]}, b^{[2]}, b^{[3]}, …, b^{[L-1]}, b^{[L]})$</li></ul><p><strong>Exercise</strong>: Implement the following function to initialize all parameters to zeros. You’ll see later that this does not work well since it fails to “break symmetry”, but lets try it anyway and see what happens. Use np.zeros((..,..)) with the correct shapes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_zeros </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l],layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_zeros([<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[0. 0. 0.] [0. 0. 0.]]b1 = [[0.] [0.]]W2 = [[0. 0.]]b2 = [[0.]]</code></pre><p><strong>Expected Output</strong>:</p><table><br>    <tr><br>    <td><br>    <strong>W1</strong><br>    </td><br>        <td><br>    [[ 0.  0.  0.]<br> [ 0.  0.  0.]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>b1</strong><br>    </td><br>        <td><br>    [[ 0.]<br> [ 0.]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>W2</strong><br>    </td><br>        <td><br>    [[ 0.  0.]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>b2</strong><br>    </td><br>        <td><br>    [[ 0.]]<br>    </td><br>    </tr><br><br></table> <p>Run the following code to train your model on 15,000 iterations using zeros initialization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"zeros"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6931471805599453Cost after iteration 1000: 0.6931471805599453Cost after iteration 2000: 0.6931471805599453Cost after iteration 3000: 0.6931471805599453Cost after iteration 4000: 0.6931471805599453Cost after iteration 5000: 0.6931471805599453Cost after iteration 6000: 0.6931471805599453Cost after iteration 7000: 0.6931471805599453Cost after iteration 8000: 0.6931471805599453Cost after iteration 9000: 0.6931471805599453Cost after iteration 10000: 0.6931471805599455Cost after iteration 11000: 0.6931471805599453Cost after iteration 12000: 0.6931471805599453Cost after iteration 13000: 0.6931471805599453Cost after iteration 14000: 0.6931471805599453</code></pre><p><img src="/img/output_11_1.png" alt="png"></p><pre><code>On the train set:Accuracy: 0.5On the test set:Accuracy: 0.5</code></pre><p>The performance is really bad, and the cost does not really decrease, and the algorithm performs no better than random guessing. Why? Lets look at the details of the predictions and the decision boundary:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"predictions_train = "</span> + str(predictions_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"predictions_test = "</span> + str(predictions_test))</span><br></pre></td></tr></table></figure><pre><code>predictions_train = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0]]predictions_test = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with Zeros initialization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="/img/output_14_0.png" alt="png"></p><p>The model is predicting 0 for every example. </p><p>In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with $n^{[l]}=1$ for every layer, and the network is no more powerful than a linear classifier such as logistic regression. </p><p><font color="blue"><br><strong>What you should remember</strong>:</font></p><ul><li>The weights $W^{[l]}$ should be initialized randomly to break symmetry. </li><li>It is however okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly. </li></ul><h2 id="3-Random-initialization"><a href="#3-Random-initialization" class="headerlink" title="3 - Random initialization"></a>3 - Random initialization</h2><p>To break symmetry, lets intialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you will see what happens if the weights are intialized randomly, but to very large values. </p><p><strong>Exercise</strong>: Implement the following function to initialize your weights to large random values (scaled by *10) and your biases to zeros. Use <code>np.random.randn(..,..) * 10</code> for weights and <code>np.zeros((.., ..))</code> for biases. We are using a fixed <code>np.random.seed(..)</code> to make sure your “random” weights  match ours, so don’t worry if running several times your code gives you always the same initial values for the parameters. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_random</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)               <span class="comment"># This seed makes sure your "random" numbers will be the as ours</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># integer representing the number of layers</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>]) * <span class="number">10</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_random([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[ 17.88628473   4.36509851   0.96497468] [-18.63492703  -2.77388203  -3.54758979]]b1 = [[0.] [0.]]W2 = [[-0.82741481 -6.27000677]]b2 = [[0.]]</code></pre><p><strong>Expected Output</strong>:</p><table><br>    <tr><br>    <td><br>    <strong>W1</strong><br>    </td><br>        <td><br>    [[ 17.88628473   4.36509851   0.96497468]<br> [-18.63492703  -2.77388203  -3.54758979]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>b1</strong><br>    </td><br>        <td><br>    [[ 0.]<br> [ 0.]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>W2</strong><br>    </td><br>        <td><br>    [[-0.82741481 -6.27000677]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>b2</strong><br>    </td><br>        <td><br>    [[ 0.]]<br>    </td><br>    </tr><br><br></table> <p>Run the following code to train your model on 15,000 iterations using random initialization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"random"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>E:\DeepLearningAI作业\02-第二课 改善深层神经网络\第二课第一周编程作业\assignment1\init_utils.py:145: RuntimeWarning: divide by zero encountered in log  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)E:\DeepLearningAI作业\02-第二课 改善深层神经网络\第二课第一周编程作业\assignment1\init_utils.py:145: RuntimeWarning: invalid value encountered in multiply  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)Cost after iteration 0: infCost after iteration 1000: 0.6250982793959966Cost after iteration 2000: 0.5981216596703697Cost after iteration 3000: 0.5638417572298645Cost after iteration 4000: 0.5501703049199763Cost after iteration 5000: 0.5444632909664456Cost after iteration 6000: 0.5374513807000807Cost after iteration 7000: 0.4764042074074983Cost after iteration 8000: 0.39781492295092263Cost after iteration 9000: 0.3934764028765484Cost after iteration 10000: 0.3920295461882659Cost after iteration 11000: 0.38924598135108Cost after iteration 12000: 0.3861547485712325Cost after iteration 13000: 0.384984728909703Cost after iteration 14000: 0.3827828308349524</code></pre><p><img src="/img/output_22_2.png" alt="png"></p><pre><code>On the train set:Accuracy: 0.83On the test set:Accuracy: 0.86</code></pre><p>If you see “inf” as the cost after the iteration 0, this is because of numerical roundoff; a more numerically sophisticated implementation would fix this. But this isn’t worth worrying about for our purposes. </p><p>Anyway, it looks like you have broken symmetry, and this gives better results. than before. The model is no longer outputting all 0s. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (predictions_train)</span><br><span class="line"><span class="keyword">print</span> (predictions_test)</span><br></pre></td></tr></table></figure><pre><code>[[1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1  1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0  0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0  1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0  0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1  1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1  0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1  1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1  1 1 1 1 0 0 0 1 1 1 1 0]][[1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1  0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0  1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with large random initialization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="/img/output_25_0.png" alt="png"></p><p><strong>Observations</strong>:</p><ul><li>The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when $\log(a^{[3]}) = \log(0)$, the loss goes to infinity.</li><li>Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. </li><li>If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.</li></ul><p><font color="blue"><br><strong>In summary</strong>:</font></p><ul><li>Initializing weights to very large random values does not work well. </li><li>Hopefully intializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part! </li></ul><h2 id="4-He-initialization"><a href="#4-He-initialization" class="headerlink" title="4 - He initialization"></a>4 - He initialization</h2><p>Finally, try “He Initialization”; this is named for the first author of He et al., 2015. (If you have heard of “Xavier initialization”, this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of <code>sqrt(1./layers_dims[l-1])</code> where He initialization would use <code>sqrt(2./layers_dims[l-1])</code>.)</p><p><strong>Exercise</strong>: Implement the following function to initialize your parameters with He initialization.</p><p><strong>Hint</strong>: This function is similar to the previous <code>initialize_parameters_random(...)</code>. The only difference is that instead of multiplying <code>np.random.randn(..,..)</code> by 10, you will multiply it by $\sqrt{\frac{2}{\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_he</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span> <span class="comment"># integer representing the number of layers</span></span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>]) * np.sqrt(<span class="number">1.</span>/layers_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_he([<span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[ 1.26475132  0.30865908] [ 0.06823401 -1.31768833] [-0.19614308 -0.25085248] [-0.05850706 -0.44335643]]b1 = [[0.] [0.] [0.] [0.]]W2 = [[-0.02190908 -0.23860902 -0.65693238  0.44231119]]b2 = [[0.]]</code></pre><p><strong>Expected Output</strong>:</p><table><br>    <tr><br>    <td><br>    <strong>W1</strong><br>    </td><br>        <td><br>    [[ 1.78862847  0.43650985]<br> [ 0.09649747 -1.8634927 ]<br> [-0.2773882  -0.35475898]<br> [-0.08274148 -0.62700068]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>b1</strong><br>    </td><br>        <td><br>    [[ 0.]<br> [ 0.]<br> [ 0.]<br> [ 0.]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>W2</strong><br>    </td><br>        <td><br>    [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]<br>    </td><br>    </tr><br>    <tr><br>    <td><br>    <strong>b2</strong><br>    </td><br>        <td><br>    [[ 0.]]<br>    </td><br>    </tr><br><br></table> <p>Run the following code to train your model on 15,000 iterations using He initialization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"he"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.729451703805014Cost after iteration 1000: 0.6939441869897982Cost after iteration 2000: 0.686361215705118Cost after iteration 3000: 0.6783983102177117Cost after iteration 4000: 0.6642724691723297Cost after iteration 5000: 0.6361798618357726Cost after iteration 6000: 0.5845947883351541Cost after iteration 7000: 0.4968293476934554Cost after iteration 8000: 0.3892480115718082Cost after iteration 9000: 0.29444331521215017Cost after iteration 10000: 0.2286133640197962Cost after iteration 11000: 0.18314787866387175Cost after iteration 12000: 0.15008009718377693Cost after iteration 13000: 0.1267612749373751Cost after iteration 14000: 0.10850689140892145</code></pre><p><img src="/img/output_32_1.png" alt="png"></p><pre><code>On the train set:Accuracy: 0.99On the test set:Accuracy: 0.93</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with He initialization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="/img/output_33_0.png" alt="png"></p><p><strong>Observations</strong>:</p><ul><li>The model with He initialization separates the blue and the red dots very well in a small number of iterations.</li></ul><h2 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5 - Conclusions"></a>5 - Conclusions</h2><p>You have seen three different types of initializations. For the same number of iterations and same hyperparameters the comparison is:</p><table><br>    <tr><br>        <td><br>        <strong>Model</strong><br>        </td><br>        <td><br>        <strong>Train accuracy</strong><br>        </td><br>        <td><br>        <strong>Problem/Comment</strong><br>        </td><br><br>    </tr><br>        <td><br>        3-layer NN with zeros initialization<br>        </td><br>        <td><br>        50%<br>        </td><br>        <td><br>        fails to break symmetry<br>        </td><br>    <tr><br>        <td><br>        3-layer NN with large random initialization<br>        </td><br>        <td><br>        83%<br>        </td><br>        <td><br>        too large weights<br>        </td><br>    </tr><br>    <tr><br>        <td><br>        3-layer NN with He initialization<br>        </td><br>        <td><br>        99%<br>        </td><br>        <td><br>        recommended method<br>        </td><br>    </tr><br></table> <p><font color="blue"><br><strong>What you should remember from this notebook</strong>:</font></p><ul><li>Different initializations lead to different results</li><li>Random initialization is used to break symmetry and make sure different hidden units can learn different things</li><li>Don’t intialize to values that are too large</li><li>He initialization works well for networks with ReLU activations. </li></ul>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 初始化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow笔记</title>
      <link href="/2018/06/13/TensorFlow%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/06/13/TensorFlow%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="神经网络的基本概念"><a href="#神经网络的基本概念" class="headerlink" title="神经网络的基本概念"></a>神经网络的基本概念</h2><ol><li>张量是多维数组（列表），用‘阶’表示张量的维度</li><li>TensorFlow的数据类型有tf.float32、tf.int32等</li><li>计算图：是承载一个或多个计算节点的一张图，只搭建网络，不运算</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>],[<span class="number">4.0</span>]])</span><br><span class="line">y = tf.matmul(x,w)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><pre><code>Tensor(&quot;MatMul_3:0&quot;, shape=(1, 1), dtype=float32)</code></pre><p>可以看到，print的结构显示y是一个张量，只搭建承载计算过程的计算图，并没有运算。<br><a id="more"></a></p><ol start="4"><li>会话：执行计算图中的节点运算<br>用with结构实现，语法如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure><pre><code>[[11.]]</code></pre><h2 id="神经网络的搭建"><a href="#神经网络的搭建" class="headerlink" title="神经网络的搭建"></a>神经网络的搭建</h2><ol><li>准备数据集，提取特征，作为输入喂给NN</li><li>搭建NN结构，从输入到输出（先搭建计算图，再用会话执行）<br> （NN前向传播算法–&gt;计算输出）</li><li>大量特征数据喂给NN，迭代优化NN参数<br> （NN反向传播算法–&gt;优化参数训练模型）</li><li>使用训练好的模型预测和分类</li></ol><h3 id="前向传播的tensorflow描述"><a href="#前向传播的tensorflow描述" class="headerlink" title="前向传播的tensorflow描述"></a>前向传播的tensorflow描述</h3><p>变量初始化、计算图节点运算都要用会话（with结构）实现</p><pre><code>with tf.Session() as sess:    sess.run()</code></pre><p>变量初始化：在sess.run函数中用tf.global_variables_initializer()汇总所有待优化变量</p><pre><code>init_op = tf.global_variables_initializer()sess.run(init_op)</code></pre><p>计算图节点运算：在sess.run函数中写入待运算的节点</p><pre><code>sess.run(y)</code></pre><p>用tf.placeholder占位，在sess.run函数中用feed_dict喂数据<br>喂一组数据：</p><pre><code>x = tf.placeholder(tf.float32, shape=(1,2))sess.run(y,feed_dict={x: [[0.5,0.6]]})</code></pre><p>喂多组数据：</p><pre><code>x = tf.placeholder(tf.float32, shape=(None,2))sess.run(y,feed_dict={x: [[0.5,0.6]],[[0.1,0.5]],[[0.4,0.2]]，[[0.8,0.7]]})</code></pre><h3 id="反向传播的tensorflow描述"><a href="#反向传播的tensorflow描述" class="headerlink" title="反向传播的tensorflow描述"></a>反向传播的tensorflow描述</h3><ul><li><p>反向传播：训练模型参数，在所有参数上用梯度下降，使NN模型在训练数据上的损失函数最小</p></li><li><p>损失函数（loss）：计算得到的预测值y与已知y_的差距</p></li><li><p>均方误差MSE：常用的损失函数计算方法，是求前向传播计算结果与已知答案之差的平方再求平均。</p><p>  loss_mse = tf.reduce_mean(tf.square(y_ - y)) </p></li><li><p>反向传播训练方法：以减小 loss 值为优化目标，有梯度下降、momentum 优化器、adam 优化器等优化方法。</p><p>  train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</p><p>  train_step=tf.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)</p><p>  train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)</p></li><li><p>学习率：决定每次参数更新的幅度。</p></li></ul><h2 id="神经网络优化"><a href="#神经网络优化" class="headerlink" title="神经网络优化"></a>神经网络优化</h2><ul><li>交叉熵(Cross Entropy)：表示两个概率分布之间的距离。交叉熵越大，两个概率分布距离越远，两个概率分布越相异；交叉熵越小，两个概率分布距离越近，两个概率分布越相似。交叉熵计算公式：𝐇(𝐲_ , 𝐲) = −∑𝐲_ ∗ 𝒍𝒐𝒈 𝒚</li></ul><p>用 Tensorflow 函数表示为</p><pre><code>ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0))) </code></pre><ul><li>softmax函数：将 n 分类的 n 个输出（y1,y2…yn）变为满足以下概率分布要求的函数。</li></ul><p>在 Tensorflow 中，一般让模型的输出经过 sofemax 函数，以获得输出分类的概率分布，再与标准<br>答案对比，求出交叉熵，得到损失函数，用如下函数实现：</p><pre><code>ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))cem = tf.reduce_mean(ce)</code></pre><ul><li>学习率 learning_rate：表示了每次参数更新的幅度大小。学习率过大，会导致待优化的参数在最小值附近波动，不收敛；学习率过小，会导致待优化的参数收敛缓慢。在训练过程中，参数的更新向着损失函数梯度下降的方向。</li></ul><p>参数的更新公式为：</p><pre><code>𝒘𝒏+𝟏 = 𝒘𝒏 − 𝒍𝒆𝒂𝒓𝒏𝒊𝒏𝒈_𝒓𝒂𝒕𝒆delta</code></pre><ul><li>指数衰减学习率：学习率随着训练轮数变化而动态更新</li></ul><p>用 Tensorflow 的函数表示为：</p><pre><code>global_step = tf.Variable(0, trainable=False)learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP, LEARNING_RATE_DECAY,staircase=True/False)</code></pre><p>   注：staircase为True时，表示global_step/learning rate step取整数，学习率阶梯型衰减</p><pre><code>staircase为False时，学习率是一条平滑下降的曲线</code></pre><ul><li>滑动平均：记录了一段时间内模型中所有参数 w 和 b 各自的平均值。利用滑动平均值可以增强模型的泛化能力。</li></ul><p>滑动平均值（影子）计算公式：</p><p>影子 = 衰减率 <em> 影子 +（1 - 衰减率）</em> 参数</p><p>其中，衰减率 = 𝐦𝐢𝐧 {𝑴𝑶𝑽𝑰𝑵𝑮𝑨𝑽𝑬𝑹𝑨𝑮𝑬𝑫𝑬𝑪𝑨𝒀,(𝟏+轮数)/(𝟏𝟎+轮数)}，影子初值=参数初值   </p><p>用 Tensorflow 函数表示为：</p><pre><code>ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY，global_step)</code></pre><p>其中，MOVING_AVERAGE_DECAY 表示滑动平均衰减率，一般会赋接近 1 的值，global_step 表示当前训练了多少轮。</p><pre><code>ema_op = ema.apply(tf.trainable_variables())</code></pre><p>其中，ema.apply()函数实现对括号内参数求滑动平均，tf.trainable_variables()函数实现把所有待训练参数汇总为列表。</p><pre><code>with tf.control_dependencies([train_step, ema_op]):     train_op = tf.no_op(name=&apos;train&apos;) </code></pre><ul><li><p>过拟合：神经网络模型在训练数据集上的准确率较高，在新的数据进行预测或分类时准确率较低，说明模型的泛化能力差。 </p></li><li><p>正则化：在损失函数中给每个参数 w 加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。 </p></li></ul><p>正则化计算方法：<br>① L1 正则化： 𝒍𝒐𝒔𝒔𝑳𝟏 = ∑𝒊|𝒘𝒊|</p><p>用 Tensorflow 函数表示:</p><pre><code>loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w)</code></pre><p>② L2 正则化： 𝒍𝒐𝒔𝒔𝑳𝟐 = ∑𝒊|𝒘𝒊|^𝟐</p><p>用 Tensorflow 函数表示:</p><pre><code>loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w)</code></pre><p>用 Tensorflow 函数实现正则化：</p><pre><code>tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w)loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;)) </code></pre><p>利用L1经过训练后，会让权重得到稀疏解，即权重中的一部分项为0，这种作用相当于对原始数据进行了特征选择；利用L2进行训练后，会让权重更趋于0，但不会得到稀疏结，这样做可以避免某些权重过大；两种正则做法都可以减轻过拟合，使训练结果更加具有鲁棒性。</p>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>梯度提升树(GBDT)概述及sklearn调参</title>
      <link href="/2018/06/11/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-GBDT-%E6%A6%82%E8%BF%B0%E5%8F%8Asklearn%E8%B0%83%E5%8F%82/"/>
      <url>/2018/06/11/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-GBDT-%E6%A6%82%E8%BF%B0%E5%8F%8Asklearn%E8%B0%83%E5%8F%82/</url>
      <content type="html"><![CDATA[<h1 id="GBDT概述"><a href="#GBDT概述" class="headerlink" title="GBDT概述"></a>GBDT概述</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>   GBDT隶属于Boosting，但不同于Adaboost，GBDT限定了弱学习器只能使用CART回归树模型。</p><h1 id="1-GBDT原理概述"><a href="#1-GBDT原理概述" class="headerlink" title="1.GBDT原理概述"></a>1.GBDT原理概述</h1><p>　　　　在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$, 损失函数是$L(y,f_{t-1}(x))$, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y,f_{t}(x) =L(y,f_{t-1}(x)+ h_t(x))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p><br><p>　　　　GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。</p><br><p>　　　　从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？</p><br><a id="more"></a><br><br><br># 2. GBDT的负梯度拟合<br><p>　　　　在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]<em>{f(x) = f</em>{t-1}\;\; (x)}$$</p><br><p>　　　　利用$(x_i,r_{ti})\;\; (i=1,2,..m)$,我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域$R_{tj}, j =1,2,…, J$。其中J为叶子节点的个数。</p><br><p>　　　　针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值$c_{tj}$如下：$$c_{tj} = \underbrace{arg\; min}<em>{c}\sum\limits</em>{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$$</p><br><p>　　　　这样我们就得到了本轮的决策树拟合函数如下：$$h_t(x) = \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p><br><p>　　　　从而本轮最终得到的强学习器的表达式如下：$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p><br><p>　　　　通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p><h1 id="3-GBDT回归算法"><a href="#3-GBDT回归算法" class="headerlink" title="3.GBDT回归算法"></a>3.GBDT回归算法</h1><p>　　　　好了，有了上面的思路，下面我们总结下GBDT的回归算法。为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们在下一节讲。</p><br><p>　　　　输入是训练集样本$T={(x_,y_1),(x_2,y_2), …(x_m,y_m)}$， 最大迭代次数T, 损失函数L。</p><br><p>　　　　输出是强学习器f(x)</p><br><p>　　　　1) 初始化弱学习器$$f_0(x) = \underbrace{arg\; min}<em>{c}\sum\limits</em>{i=1}^{m}L(y_i, c)$$</p><br><p>　　　　2) 对迭代轮数t=1,2,…T有：</p><br><p>　　　　　　a)对样本i=1,2，…m，计算负梯度$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]<em>{f(x) = f</em>{t-1}\;\; (x)}$$</p><br><p>　　　　　　b)利用$(x_i,r_{ti})\;\; (i=1,2,..m)$, 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为$R_{tj}, j =1,2,…, J$。其中J为回归树t的叶子节点的个数。</p><br><p>　　　　　　c) 对叶子区域j =1,2,..J,计算最佳拟合值$$c_{tj} = \underbrace{arg\; min}<em>{c}\sum\limits</em>{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$$</p><br><p>　　　　　　d) 更新强学习器$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p><br><p>　　　　3) 得到强学习器f(x)的表达式$$f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p><h1 id="4-GBDT分类算法"><a href="#4-GBDT分类算法" class="headerlink" title="4. GBDT分类算法"></a>4. GBDT分类算法</h1><p>　　　　这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。</p><br><p>　　　　为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。</p><h2 id="4-1-二元GBDT分类算法"><a href="#4-1-二元GBDT分类算法" class="headerlink" title="4.1 二元GBDT分类算法"></a>4.1 二元GBDT分类算法</h2><p>　　　　对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：$$L(y, f(x)) = log(1+ exp(-yf(x)))$$</p><br><p>　　　　其中$y \in{-1, +1}$。则此时的负梯度误差为$$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]<em>{f(x) = f</em>{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$$</p><br><p>　　　　对于生成的决策树，我们各个叶子节点的最佳残差拟合值为$$c_{tj} = \underbrace{arg\; min}<em>{c}\sum\limits</em>{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$$</p><br><p>　　　　由于上式比较难优化，我们一般使用近似值代替$$c_{tj} =\sum\limits_{x_i \in R_{tj}}r_{ti}\bigg /\sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$$</p><br><p>　　　　除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。</p><h2 id="4-2-多元GBDT分类算法"><a href="#4-2-多元GBDT分类算法" class="headerlink" title="4.2 多元GBDT分类算法"></a>4.2 多元GBDT分类算法</h2><p>　　　　多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：$$L(y, f(x)) = - \sum\limits_{k=1}^{K}y_klog\;p_k(x)$$</p><br><p>　　　　其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为：$$p_k(x) = exp(f_k(x)) \bigg / \sum\limits_{l=1}^{K}exp(f_l(x))$$</p><br><p>　　　　集合上两式，我们可以计算出第$t$轮的第$i$个样本对应类别$l$的负梯度误差为$$r_{til} =-\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1}\;\; (x)} = y_{il} - p_{l, t-1}(x_i)$$</p><br><p>　　　　观察上式可以看出，其实这里的误差就是样本$i$对应类别$l$的真实概率和$t-1$轮预测概率的差值。</p><br><p>　　　　对于生成的决策树，我们各个叶子节点的最佳残差拟合值为$$c_{tjl} = \underbrace{arg\; min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K}L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$$</p><br><p>　　　　由于上式比较难优化，我们一般使用近似值代替$$c_{tjl} = \frac{K-1}{K} \; \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$$</p><br><p>　　　　除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p><h1 id="5-GBDT常用损失函数"><a href="#5-GBDT常用损失函数" class="headerlink" title="5. GBDT常用损失函数"></a>5. GBDT常用损失函数</h1><p>　　　　这里我们再对常用的GBDT损失函数做一个总结。</p><br><p>　　　　对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种:</p><br><p>　　　　a) 如果是指数损失函数，则损失函数表达式为$$L(y, f(x)) = exp(-yf(x))$$</p><br><p>　　　　其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理篇。</p><br><p>　　　　b)如果是对数损失函数，分为二元分类和多元分类两种，参见4.1节和4.2节。</p><br><p>　　　　</p><br><p>　　　　对于回归算法，常用损失函数有如下4种:</p><br><p>　　　　a)均方差，这个是最常见的回归损失函数了$$L(y, f(x)) =(y-f(x))^2$$</p><br><p>　　　　b)绝对损失，这个损失函数也很常见$$L(y, f(x)) =|y-f(x)|$$</p><br><p>　　　　　　对应负梯度误差为：$$sign(y_i-f(x_i))$$</p><br><p>　　　　c)Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：</p><br><p>$$L(y, f(x))=\begin{cases}\frac{1}{2}(y-f(x))^2 \qquad {|y-f(x)| \leq \delta}\ \delta(|y-f(x)| - \frac{\delta}{2}) \qquad {|y-f(x)| \geq \delta} \end{cases}$$</p><br><p>　　　　对应的负梯度误差为：</p><br><p>$$r(y_i, f(x_i))= \begin{cases} y_i-f(x_i) \qquad {|y_i-f(x_i)| \leq \delta}\ \delta sign(y_i-f(x_i)) \qquad {|y_i-f(x_i)| \geq\ \delta} \end{cases}$$</p><br><p>　　　　d) 分位数损失。它对应的是分位数回归的损失函数，表达式为$$L(y, f(x)) =\sum\limits_{y \geq f(x)}\theta|y - f(x)| + \sum\limits_{y   \leq f(x)}(1-\theta)|y - f(x)|$$</p><br><p>　　　　　　其中$\theta$为分位数，需要我们在回归前指定。对应的负梯度误差为：</p><br><p>$$r(y_i, f(x_i))= \begin{cases} \theta \qquad { y_i \geq f(x_i)}\ \theta - 1 \qquad {y_i \leq f(x_i) } \end{cases}$$</p><br><p>　　　　对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p><h1 id="6-GBDT的正则化"><a href="#6-GBDT的正则化" class="headerlink" title="6. GBDT的正则化"></a>6. GBDT的正则化</h1><p>　　　　和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。</p><br><p>　　　　第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为$\nu$,对于前面的弱学习器的迭代$$f_{k}(x) = f_{k-1}(x) + h_k(x) $$</p><br><p>　　　　如果我们加上了正则化项，则有$$f_{k}(x) = f_{k-1}(x) + \nu h_k(x) $$</p><br><p>　　　　$\nu$的取值范围为$0 \leq \nu \leq 1 $。对于同样的训练集学习效果，较小的$\nu$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p><br><p></p><br><p>　　　　第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。</p><br><p>　　　　使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</p><br><p></p><br><p>　　　　第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了。</p><h1 id="7-GBDT小结"><a href="#7-GBDT小结" class="headerlink" title="7. GBDT小结"></a>7. GBDT小结</h1><p>　　　　由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。</p><br><p>　　　　最后总结下GBDT的优缺点。</p><br><p>　　　　GBDT主要的优点有：</p><br><p>　　　　1) 可以灵活处理各种类型的数据，包括连续值和离散值。</p><br><p>　　　　2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。</p><br><p>　　　　3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p><br><p>　　　　GBDT的主要缺点有：</p><br><p>　　　　1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p><br><p></p><h1 id="8-GBDT-sklearn调参"><a href="#8-GBDT-sklearn调参" class="headerlink" title="8.GBDT sklearn调参"></a>8.GBDT sklearn调参</h1><h2 id="8-1-GBDT类Boosting框架参数"><a href="#8-1-GBDT类Boosting框架参数" class="headerlink" title="8.1 GBDT类Boosting框架参数"></a>8.1 GBDT类Boosting框架参数</h2><p>　　　　1)<strong>n_estimators</strong>: 弱学习器的最大迭代次数，或者说最大的弱学习器的个数。太小，容易欠拟合，太大，容易过拟合。默认是100。实际调参过程中常常将n_estimators和learning_rate一起考虑</p><br><p>　　　　2)<strong>learning_rate</strong>: 即每个弱学习器的权重缩减系数$\nu$，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为$f_{k}(x) = f_{k-1}(x) + \nu h_k(x)$。$\nu$的取值范围为$0 \leq \nu \leq 1 $。对于同样的训练集拟合效果，较小的$\nu$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的$\nu$开始调参，默认是1。</p><br><p>　　　　3)<strong>subsample</strong>: 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。</p><br><p>　　　　4)<strong>init</strong>: 即我们的初始化的时候的弱学习器，拟合对应原理篇里面的$f_{0}(x)$，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。</p><br><p>　　　　5)<strong>loss: </strong>即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。</p><br><p>　　　　　　对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。</p><br><p>　　　　　　对于回归模型，有均方差”ls”,绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。</p><br><p>　　　　6)<strong>alpha：</strong>这个参数只有GradientBoostingRegressor有，当我们使用Huber损失”huber”和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。</p><h2 id="8-2-GBDT类弱学习器参数"><a href="#8-2-GBDT类弱学习器参数" class="headerlink" title="8.2 GBDT类弱学习器参数"></a>8.2 GBDT类弱学习器参数</h2><p>　　　　这里我们再对GBDT的类库弱学习器的重要参数做一个总结。由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似。如果你已经很熟悉决策树算法的调参，那么这一节基本可以跳过。不熟悉的朋友可以继续看下去。</p><br><p>　　　　1)划分时考虑的最大特征数<strong>max_features</strong>:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数；如果是”log2”意味着划分时最多考虑$log_2N$个特征；如果是”sqrt”或者”auto”意味着划分时最多考虑$\sqrt{N}$个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p><br><p>　　　　2)决策树最大深度<strong>max_depth</strong>:默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</p><br><p>　　　　3)内部节点再划分所需最小样本数<strong>min_samples_split</strong>:这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p><br><p>　　　　4)叶子节点最少样本数<strong>min_samples_leaf</strong>:这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p><br><p>　　　　5）叶子节点最小的样本权重和<strong>min_weight_fraction_leaf</strong>：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p><br><p>　　　　6)最大叶子节点数<strong>max_leaf_nodes</strong>:通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</p><br><p>　　　　7)节点划分最小不纯度<strong>min_impurity_split:</strong>这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点。一般不推荐改动默认值1e-7。</p>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>天池美年大健康比赛总结-30/3183</title>
      <link href="/2018/06/07/%E5%A4%A9%E6%B1%A0%E7%BE%8E%E5%B9%B4%E5%A4%A7%E5%81%A5%E5%BA%B7%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93-30-3183/"/>
      <url>/2018/06/07/%E5%A4%A9%E6%B1%A0%E7%BE%8E%E5%B9%B4%E5%A4%A7%E5%81%A5%E5%BA%B7%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93-30-3183/</url>
      <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>​    严格意义上来说，这是我第一次参加机器学习方面的比赛，先不管结果如何，把自己平时所学的算法、tricks应用在实际问题中并且得到一定产出是一件非常有快感的事情。<br>本次比赛历时2个月（2018.4.10-2018.6.7），是我打过的周期最长的一次比赛了，最终拿到了第一赛季82/3151，第二赛季30/3183的成绩。关于成绩，我想说的是，如果看百分比，<br>第二赛季的成绩其实是个还不错的成绩，但是看到有很多第一次比赛的小伙伴拿了20多名，作为一个研一的“老年人”心里多少有点不甘吧。但是这个结果我应该很快就可以接受，<br>重要的是在比赛的这两个月所学到的东西，向前看。岁月不饶人，我又何曾饶过岁月。</p><a id="more"></a><h1 id="初赛"><a href="#初赛" class="headerlink" title="初赛"></a>初赛</h1><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>​    根据赛题所给的5万名患者的体检数据，预测患者的舒张压、收缩压、血清甘油三酯、血清低密度脂蛋白、血清高密度脂蛋白 共5项指标。具体的题目和数据集见：<a href="https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11409106.5678.1.4678560cho1fKX&amp;raceId=231654" target="_blank" rel="noopener">https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11409106.5678.1.4678560cho1fKX&amp;raceId=231654</a></p><p>开源方案：</p><p>初赛12名：<a href="https://github.com/yzbdt/tianchi_meinian_rank12_1st_season" target="_blank" rel="noopener">初赛12名方案</a></p><p>初赛第1名：<a href="https://github.com/RobinSeaside/Solution-meinian-disease-risk-prediction-1st-in-round1-14th-in-round2" target="_blank" rel="noopener">初赛第1名方案</a></p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>数据集描述：</p><p>数据集大小：300+MB, 800万条*3列 样本数据</p><p>数据内容：vid（字符串）、table_id（数字）、field_results（中文、数字混合）</p><h3 id="step1-数据集转换-8-000-000-3-gt-50000-600"><a href="#step1-数据集转换-8-000-000-3-gt-50000-600" class="headerlink" title="step1 数据集转换 8,000,000  3 -&gt; 50000  600"></a>step1 数据集转换 8,000,000 <em> 3 -&gt; 50000 </em> 600</h3><p>​    这一步主要是用了pandas 的pivot()函数，把长数据转换成宽数据。这里会涉及到一个问题，当数据的vid、table_id 相同而field_result 不同（也就是某人同一个项目检查了多次）时，<br>是取field_result 的均值还是取某一个值？我们选择的做法是只取第一次出现的值（值得探讨，这个方案有问题，但是如何优化呢？）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreProc</span><span class="params">(data)</span>:</span>   <span class="comment"># only for digit , do not used in word</span></span><br><span class="line">    data = data.drop_duplicates(subset=[<span class="string">'vid'</span>,<span class="string">'table_id'</span>])</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'drop_duplicates data shape : &#123;&#125;'</span>.format(data.shape)</span><br><span class="line">    featMat = data.pivot(<span class="string">'vid'</span>,<span class="string">'table_id'</span>,<span class="string">'field_results'</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'drop_duplicates featMat shape : &#123;&#125;'</span>.format(featMat.shape)</span><br><span class="line">    <span class="keyword">return</span> featMat</span><br></pre></td></tr></table></figure><h3 id="step2-数字特征提取"><a href="#step2-数字特征提取" class="headerlink" title="step2 数字特征提取"></a>step2 数字特征提取</h3><p>​    数据集转换后发现特征可以大致分为4类：纯数字特征、短文本特征（正常/异常、未见异常/轻度异常）、长文本特征（一般是造影检查的诊断结果，语句格式比较固定，不同患者略有不同）、<br>数字文本混合特征。首先对数字特征进行提取。我们的做法是把特征中所有的数字都提取出来，如果遇到数字文本混合特征，需要该特征的数字样本数大于100。看了初赛第一名的开源代码，他们的<br>做法是数字样本数大于总样本数的50%时，认为是数字特征。这一步主要用了正则表达式和pandas的字符串处理str.extract()方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DigitDetect</span><span class="params">(featMat)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> featMat.columns:</span><br><span class="line">        featMat[i] = featMat[i].str.extract(<span class="string">'(^(\+|\-)?\d+\.?\d*)'</span>,expand=<span class="keyword">False</span>)</span><br><span class="line">    digitOnly = featMat</span><br><span class="line">    digitOnly.dropna(axis=<span class="number">1</span>,how=<span class="string">'all'</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'digitOnly shape : &#123;&#125;'</span>.format(digitOnly.shape)</span><br><span class="line">    table_id_digitFiltered = digitOnly.count()[digitOnly.count()&gt;<span class="number">100</span>].index</span><br><span class="line">    digitFiltered = digitOnly[table_id_digitFiltered]</span><br><span class="line">    digitFiltered = digitFiltered.fillna(<span class="string">'-1'</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'digitFiltered shape : &#123;&#125;'</span>.format(digitFiltered.shape)</span><br><span class="line">    <span class="keyword">return</span> digitFiltered</span><br></pre></td></tr></table></figure><h3 id="step3-短文本特征处理"><a href="#step3-短文本特征处理" class="headerlink" title="step3 短文本特征处理"></a>step3 短文本特征处理</h3><p>短文本特征是比较好处理的一类特征，可以直接用labelencoder的方法将文本编码加入模型。这里主要用到的是正则表达式和sklearn里面的labelencoder()思想。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Feat0421</span><span class="params">(dataFeatureMat)</span>:</span></span><br><span class="line">    dataFeatureMat[<span class="string">'0421'</span>] = dataFeatureMat[<span class="string">'0421'</span>].str.replace(<span class="string">'(不齐)'</span>,<span class="string">'(杂乱)'</span>)</span><br><span class="line">    dataFeatureMat[<span class="string">'0421_HR'</span>]=<span class="string">'999'</span></span><br><span class="line">    regular = np.where(dataFeatureMat[<span class="string">'0421'</span>].str.contains(<span class="string">'(整齐|齐)'</span>))[<span class="number">0</span>]</span><br><span class="line">    dataFeatureMat[<span class="string">'0421_HR'</span>].iloc[regular] = <span class="string">'0'</span></span><br><span class="line">    irregular = np.where(dataFeatureMat[<span class="string">'0421'</span>].str.contains(<span class="string">'(杂乱|过|早搏)'</span>))[<span class="number">0</span>]</span><br><span class="line">    dataFeatureMat[<span class="string">'0421_HR'</span>].iloc[irregular] = <span class="string">'1'</span></span><br><span class="line">    <span class="keyword">del</span> dataFeatureMat[<span class="string">'0421'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataFeatureMat</span><br></pre></td></tr></table></figure><h3 id="step4-长文本特征处理"><a href="#step4-长文本特征处理" class="headerlink" title="step4 长文本特征处理"></a>step4 长文本特征处理</h3><p>​    长文本特征包含了丰富的信息，但是也较难处理，由于团队没有NLP方面的积累，这部分工作很难开展。一般来说这部分信息的处理有2种方案，第一，暴力正则，优点是可以准确提取出<br>人类正确理解的信息，缺点是极其浪费体力。第二，word2vector（词向量）、tf-idf（词频统计）等nlp算法，优点是可以拓展nlp方面的知识，写起来不像正则那么麻烦，缺点是这些算法大<br>多关注的是词语之间的关联度，并不能真正理解词语的含义，比如对于某项检查而言，人们最关心的就是结果正常还是不正常，这个信息我们可以轻而易举的从[正常，异常，不正常，轻度异<br>常，未见异常，…]这些词语中区分出来，但是nlp算法找到的却是这些词语之间的关联度，“正常”和“异常”的关联度在算法的视角里可能很高，“正常”和“未见异常”的关联度在算法的视角里<br>可能很低（个人对nlp算法的理解不深，仅对大致看过的tf-idf、word2vec方法而言）。基于这些，我们选择了暴力正则的方法处理长文本特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal0114</span><span class="params">(data)</span>:</span></span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^\s.*'</span>,<span class="string">'999'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*强回声.*'</span>,<span class="string">'11'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*无回声.*'</span>,<span class="string">'10'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*高回声.*'</span>,<span class="string">'9'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*弱回声.*'</span>,<span class="string">'8'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*低回声.*'</span>,<span class="string">'7'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*絮状回声.*'</span>,<span class="string">'6'</span>)                </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*光团.*'</span>,<span class="string">'5'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*光斑.*'</span>,<span class="string">'4'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*声影.*'</span>,<span class="string">'3'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*可见.*'</span>,<span class="string">'2'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*附着.*'</span>,<span class="string">'2'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*探及.*'</span>,<span class="string">'2'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*建议.*'</span>,<span class="string">'2'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*厚约.*'</span>,<span class="string">'2'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*mm.*'</span>,<span class="string">'2'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*cm.*'</span>,<span class="string">'2'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*毛糙.*'</span>,<span class="string">'1'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*欠光滑.*'</span>,<span class="string">'1'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*不光整.*'</span>,<span class="string">'1'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*欠光整.*'</span>,<span class="string">'1'</span>)        </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*胆囊大小、形态正常，囊壁光整，囊腔内透声好，胆总管无扩张.*'</span>,<span class="string">'0'</span>) </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*胆.*'</span>,<span class="string">'0'</span>) </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'.*未显示.*'</span>,<span class="string">'0'</span>) </span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^0\\s*'</span>,<span class="string">'0'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^2\\s*'</span>,<span class="string">'2'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^0.*'</span>,<span class="string">'0'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^2.*'</span>,<span class="string">'2'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^3.*'</span>,<span class="string">'3'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^0\n.*'</span>,<span class="string">'0'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^2\n.*'</span>,<span class="string">'2'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^2\n.*'</span>,<span class="string">'2'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^1\n.*'</span>,<span class="string">'1'</span>)</span><br><span class="line">    data[<span class="string">'0114'</span>] = data[<span class="string">'0114'</span>].str.replace(<span class="string">'^999\n.*'</span>,<span class="string">'999'</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>​    可以看到，仅对一个长文本特征的处理代码就这么多，而且基本都是是重复的操作。看了初赛第一名的代码发现他们也采取了类似的方案，不同的是他们做了一些特征交叉，而我们没有找到合适的<br>特征交叉的方案（脱敏的特征，不知道如何交叉，看过一个遍历特征做交叉的方案，类似于特征筛选中的过滤式特征筛选，但是我们的特征太多了，遍历代价无法承受，最终放弃）。我们唯一做交叉的<br>是对几个第二性征检查的特征做了交叉，得到性别特征，后面会提到。</p><h3 id="step-5-缺失值处理"><a href="#step-5-缺失值处理" class="headerlink" title="step 5 缺失值处理"></a>step 5 缺失值处理</h3><p>​    这是一个比较重要的环节，因为数据的缺失值非常多，有些特征的缺失值达到70%以上。常见的缺失值处理方案有3种：1.根据已知数据补全缺失值，一般按众数或者平均值补全，具体用什么指标需要<br>结合特征的分布进行判断，例如，如果特征分布是一个标准正态分布，那么用均值补全即可，如果是一个长尾分布（不均匀分布），用众数补全较合理。2.若有缺失值的特征数量较少，可以将缺失值<br>当做label，做一个模型来对缺失值进行预测补全。3.用-1或者999对缺失值进行补全。我们采用的是第3个方案，因为特征多，缺失的特征也多，逐一查看特征的分布不是很现实，时间不够（当然也是自己没<br>安排好）。一开始处理的时候还给每列特征加了一个dummy variance，表征是否缺失，后来发现至少对于树模型而言，这种做法完全没有必要，因为树模型受稀疏特征的影响会比较大，添加很<br>多dummy variance后，特征矩阵会变得很稀疏，事实也证明没有dummy variance的效果要更好。所以最终，我们直接用-1 对缺失值进行补全。</p><h3 id="step-6-特征工程"><a href="#step-6-特征工程" class="headerlink" title="step 6 特征工程"></a>step 6 特征工程</h3><p>​    到了这一步，我们终于得到了一个适用于机器学习的数据集了，接下来就是特征工程了。特征工程的方法有3种，过滤式特征选择、包裹式特征选择、嵌入式特征选择。这三种方案我们其实都尝试过，<br>过滤式特征选择主要是看特征与label之间的相关关系，有很多可以衡量的标准，例如Pearson相关系数、最大信息系数（MIC）等等。但是由于数据集比较稀疏，线性相关性都很低，MIC计算速度又非常慢<br>（其实回想起来MIC应该坚持算完的），所以没有继续采用过滤式特征选择。包裹式特征选择就是根据模型的表现来决定使用哪些特征，我们当时采用的办法是用全特征跑一个模型，选出特征重要性<br>（importance）排名靠前的一些特征入模型再跑，很遗憾，我们发现这样的做法始终无法超越全特征的得分，于是放弃。<strong>当然，回想一下，这个步骤当时做的很不科学，应该做一个特征子集，然后加减<br>特征跑迭代的，然后选出得分高于当前最高值的特征子集作为当前的特征。参见周志华《机器学习》第11章。</strong> 嵌入式特征选择，不用说，模型中的L1正则已经帮我们做了这个事情了（稀疏超参数）。<br>所以，因为不科学的特征选择做法，我们最后相当于没有做特征筛选。</p><p>​    特征工程的第二部分就是特征组合，我们对一些第二性征的检查例如：生殖器、乳腺等，做了交叉，得到了性别特征。由于其他数据脱敏以及缺乏医学专业知识，我们没能找到其他的组合方式。贪婪<br>交叉特征的方法时间代价极大（详见：<a href="https://github.com/luoda888/tianchi-diabetes-top12），我们也没有挑战人品随机组合。" target="_blank" rel="noopener">https://github.com/luoda888/tianchi-diabetes-top12），我们也没有挑战人品随机组合。</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_division_feature</span><span class="params">(data,feature_name)</span>:</span></span><br><span class="line">    new_feature = []</span><br><span class="line">    new_feature_name = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data[feature_name].columns)<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,len(data[feature_name].columns)):</span><br><span class="line">            new_feature_name.append(data[feature_name].columns[i] + <span class="string">'/'</span> + data[feature_name].columns[j])</span><br><span class="line">            new_feature_name.append(data[feature_name].columns[i] + <span class="string">'*'</span> + data[feature_name].columns[j])</span><br><span class="line">            new_feature_name.append(data[feature_name].columns[i] + <span class="string">'+'</span> + data[feature_name].columns[j])</span><br><span class="line">            new_feature_name.append(data[feature_name].columns[i] + <span class="string">'-'</span> + data[feature_name].columns[j])</span><br><span class="line">            new_feature.append(data[data[feature_name].columns[i]]/data[data[feature_name].columns[j]])</span><br><span class="line">            new_feature.append(data[data[feature_name].columns[i]]*data[data[feature_name].columns[j]])</span><br><span class="line">            new_feature.append(data[data[feature_name].columns[i]]+data[data[feature_name].columns[j]])</span><br><span class="line">            new_feature.append(data[data[feature_name].columns[i]]-data[data[feature_name].columns[j]])</span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line">    temp_data = DF(pd.concat(new_feature,axis=<span class="number">1</span>))</span><br><span class="line">    temp_data.columns = new_feature_name</span><br><span class="line">    data = pd.concat([data,temp_data],axis=<span class="number">1</span>).reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    print(data.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data.reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_square_feature</span><span class="params">(data,feature_name)</span>:</span></span><br><span class="line">    new_feature = []</span><br><span class="line">    new_feature_name = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data[feature_name].columns)):</span><br><span class="line">        new_feature_name.append(data[feature_name].columns[i] + <span class="string">'**2'</span>)</span><br><span class="line">        new_feature_name.append(data[feature_name].columns[i] + <span class="string">'**1/2'</span>)</span><br><span class="line">        new_feature.append(data[data[feature_name].columns[i]]**<span class="number">2</span>)</span><br><span class="line">        new_feature.append(data[data[feature_name].columns[i]]**(<span class="number">1</span>/<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">    temp_data = DF(pd.concat(new_feature,axis=<span class="number">1</span>))</span><br><span class="line">    temp_data.columns = new_feature_name</span><br><span class="line">    data = pd.concat([data,temp_data],axis=<span class="number">1</span>).reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    print(data.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data.reset_index(drop=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>​    问题是一个回归问题，一般常用的方法有线性模型、树模型、深度学习。线性模型包括SVM、LR、BP网络等等，这些模型对输入数据非常敏感，前提是我们必须非常了解每个特征的分布（因为要做数据<br>归一化），否则归一化后的特征可能反而会使得这类线性模型的表现更差，这也是为什么天池、kaggle这些比赛中获奖方案较少采用线性模型的原因。树模型一直都是天池、kaggle比赛的宠儿，说白了因为<br>它调参简单，可以在完全不了解特征背后的分布和含义的情况下，直接丢进模型就能得到一个不错的结果。深度学习模型一般需要在较大的数据集上才能得到不错的结果，小数据集效果跟线性模型和树模型<br>相差无几。</p><p>​    我们还是比较保守地选择了树模型，第一是我们作为新手，没有丰富的特征工程和调参经验，第二，相对而言我们比较熟悉树模型，前段时间刚看过一些树模型的算法原理，可以巩固平日所学。开始<br>用的sklearn里面的GBDT Regression，但是sklearn的GBDT实现是串行执行，速度很慢，而且没有正则，也就无法做嵌入式特征筛选，而且目标函数只能选择rmse。当然用sklearn的GBDT效果也一般。之后<br>尝试了陈天奇的神作xgboost，特征层面并行，给目标函数加入了l1、l2正则，近似分裂点寻找，可以自己修改目标函数（<strong>我们把目标函数改成gamma regression后，线下得分有了很高的提升，所以后来<br>就一直使用gamma regression作为目标函数。在看了初赛第一、第十二的开源方案后，发现他们的objective function的选择都是rmse，难道是我们的线下得分欺骗了我们？我现在不太记得有没有做过线上<br>的比较了</strong>），xgboost确实比sklearn快很多，但是这两个也都有个共同问题是无法自动处理类别特征（one-hot encoding）。所以稀疏特征会给这两种模型带来一些负面影响（1.花费精力重做类别特<br>征。or 2.损失精度）。而lightgbm和catboost可以自动处理类别特征，而且lightgbm在并行计算上做的比xgboost更好，而且引入了leaf-wise的树生成策略提升精度，所以接下来我们尝试了lightgbm，也是<br>我们初赛最高得分的单模型。</p><h4 id="sklearn-GBDT-regression"><a href="#sklearn-GBDT-regression" class="headerlink" title="sklearn GBDT regression"></a>sklearn GBDT regression</h4><h4 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h4><p>​    对比了一下我们的xgboost参数和第一名的xgboost参数，他们的xgboost的迭代次数远高于我们，学习率远低于我们，但是这里是有个权衡的，低学习率容易陷入局部最优，但若没有陷入局部最优，找到<br>的最优点会比高学习率找到的最优点更好，很遗憾，我没有在他们的代码里看到调参策略，因此目前还不知道他们的参数是如何确定的。另外我们的objective function如上文所说，选择了gamma 而他们仅仅<br>选择了linear（rmse）</p><p>我们的xgb参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">other_params = &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'n_estimators'</span>: i, <span class="string">'max_depth'</span>: <span class="number">6</span>, <span class="string">'min_child_weight'</span>: <span class="number">1</span>, <span class="string">'seed'</span>: <span class="number">0</span>,<span class="string">'subsample'</span>: <span class="number">0.8</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>, <span class="string">'gamma'</span>: <span class="number">1</span>, \</span><br><span class="line"><span class="string">'reg_alpha'</span>: <span class="number">0.1</span>, <span class="string">'reg_lambda'</span>: <span class="number">1</span>,<span class="string">'objective'</span>:<span class="string">'reg:gamma'</span>&#125;</span><br></pre></td></tr></table></figure><p>第一名的xgb参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">PARAMS = &#123;</span><br><span class="line">    <span class="string">'xgb'</span>: &#123;</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'reg:linear'</span>,</span><br><span class="line">        <span class="string">'seed'</span>: RANDOM_SEED,</span><br><span class="line">        <span class="string">'nthread'</span>: <span class="number">56</span>,</span><br><span class="line">        <span class="string">'eta'</span>: <span class="number">0.01</span>,</span><br><span class="line">        <span class="string">'min_child_weight'</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">'subsample'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'silent'</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">'max_depth'</span>: <span class="number">7</span>,</span><br><span class="line">        <span class="string">'num_rounds'</span>: <span class="number">10000</span>,</span><br><span class="line">        <span class="string">'num_early_stop'</span>: <span class="number">50</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="lightgbm"><a href="#lightgbm" class="headerlink" title="lightgbm"></a>lightgbm</h4><p>同样，我对比了一下我们的lgb参数和第一名、第十二名的参数</p><p>这是我们的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">model = lgb.LGBMRegressor(</span><br><span class="line">    boosting_type=<span class="string">'gbdt'</span>, </span><br><span class="line">    num_leaves=<span class="number">63</span>, </span><br><span class="line">    max_depth=<span class="number">-1</span>, </span><br><span class="line">    learning_rate=<span class="number">0.01</span>, </span><br><span class="line">    n_estimators=<span class="number">1200</span>, </span><br><span class="line">    subsample_for_bin=<span class="number">200000</span>,</span><br><span class="line">    objective=<span class="string">'gamma'</span>, </span><br><span class="line">    class_weight=<span class="keyword">None</span>,</span><br><span class="line">    min_split_gain=<span class="number">0.01</span>, </span><br><span class="line">    min_child_weight=<span class="number">0.001</span>, </span><br><span class="line">    min_child_samples=<span class="number">20</span>, </span><br><span class="line">    subsample=<span class="number">0.8</span>, </span><br><span class="line">    subsample_freq=<span class="number">1</span>, </span><br><span class="line">    colsample_bytree=<span class="number">1.0</span>, </span><br><span class="line">    reg_alpha=<span class="number">1.0</span>, </span><br><span class="line">    reg_lambda=<span class="number">1.0</span>, </span><br><span class="line">    random_state=<span class="keyword">None</span>, </span><br><span class="line">    n_jobs=<span class="number">4</span>, </span><br><span class="line">    silent=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>这是第一名的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'lgb'</span>: &#123;</span><br><span class="line">        <span class="string">'task'</span>: <span class="string">'train'</span>,</span><br><span class="line">        <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'mse'</span>,</span><br><span class="line">        <span class="string">'metric'</span>: <span class="string">'l2'</span>,</span><br><span class="line">        <span class="string">'num_leaves'</span>: <span class="number">31</span>,</span><br><span class="line">        <span class="string">'learning_rate'</span>: <span class="number">0.01</span>,</span><br><span class="line">        <span class="string">'feature_fraction'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'bagging_fraction'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'bagging_freq'</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">'verbose'</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">'bagging_seed'</span>: RANDOM_SEED,</span><br><span class="line">        <span class="string">'num_rounds_lgb'</span>: <span class="number">10000</span>,</span><br><span class="line">        <span class="string">'num_early_stop'</span>: <span class="number">200</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这是第十二名的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.025</span>,</span><br><span class="line">    <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'regression'</span>,</span><br><span class="line">    <span class="string">'metric'</span>: <span class="string">'mse'</span>,  <span class="comment"># 使用均方误差</span></span><br><span class="line">    <span class="string">'num_leaves'</span>: <span class="number">60</span>,  <span class="comment"># 最大叶子数for base learner</span></span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">0.6</span>,  <span class="comment"># 选择部分的特征</span></span><br><span class="line">    <span class="string">'min_data'</span>: <span class="number">100</span>,  <span class="comment"># 一个叶子上的最少样本数</span></span><br><span class="line">    <span class="string">'min_hessian'</span>: <span class="number">1</span>,  <span class="comment"># 一个叶子上的最小 hessian 和，子叶权值需大于的最小和</span></span><br><span class="line">    <span class="string">'verbose'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'lambda_l1'</span>: <span class="number">0.3</span>,  <span class="comment"># L1正则项系数</span></span><br><span class="line">    <span class="string">'device'</span>: <span class="string">'cpu'</span>,</span><br><span class="line">    <span class="string">'num_threads'</span>: <span class="number">8</span>, <span class="comment">#最好设置为真实核心数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    可以看到同样的，我们的objective function选择不同，第一名甚至都没有做正则。。。不得不说特征为王！其他的参数大同小异，问题不是很大。</p><h5 id="note"><a href="#note" class="headerlink" title="note"></a>note</h5><p>​    这里有个非常重要的参数，是我们复赛的时候才发现的，那就是每个模型的<strong>basescore</strong>，这个参数是模型求解过程中的一个初始值，默认0.5，但是如果我们可以把他调整到跟目标label值相近的值，<br>模型可以更快收敛，相同的迭代次数下，模型能够找到更精确的最优解，在复赛的时候我们修改这个参数获得了二十名的提升。</p><h2 id="建模策略"><a href="#建模策略" class="headerlink" title="建模策略"></a>建模策略</h2><p>​    这一部分主要回顾总结一下我们在比赛过程中用到的一些建模策略，包括如何调参、如何做cv、如何做线下评价机制以及如何把我们的模型做成一个可持续提升分数的结构（最后这一点是我后来的一些<br>思考）。</p><p>​    在初赛中，我们的策略主要是想做一个效果最好的单模型，因为单模型做到效果特别好的时候，后期再做模型融合的上升空间更大。据我们了解，模型融合能够带来的提升是少量的（对于我们的比赛而<br>言，可能就是千分位或者万分位上的提高），而且无脑做模型融合而不去深挖特征是拣了芝麻丢了西瓜的行为，最终的结果也证明目前的机器学习比赛或者是项目都是特征为王的。</p><p>​    那么，单模型究竟怎么做呢？一开始，没有经验，为了评价模型的线下效果，我们直接把训练集拆成5份，用4份训练，1份线下评估，拆分的过程也没有设置随机数种子。这是一种非常不科学的做法，<br>因为没有做cv，没有固定随机数种子，每次得到的模型都是不稳定的，更重要的是，这种做法等于直接丢掉了1份宝贵的训练数据，而且线下评估的时候也仅仅评估了模型的偏差，无法考量方差，所以无法<br>评价模型的泛化能力。后来跟 怪物的姐姐 交流了一下，才得知正确的做法是做完整的5折cv然后把每一次的预测结果做一个平均融合，这样，线下评估就可以得到模型的方差和偏差也可以利用完整的数据<br>集进行训练。下面给出这种方案的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_ssy</span><span class="params">(train,test,kind = <span class="string">'SSY'</span>)</span>:</span></span><br><span class="line">    train.reset_index(drop = <span class="keyword">True</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line">    target = kind</span><br><span class="line">    IDcol = <span class="string">'vid'</span></span><br><span class="line">    predictors = [x <span class="keyword">for</span> x <span class="keyword">in</span> train.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [target,IDcol]]</span><br><span class="line">    cv_params = &#123;<span class="string">'n_estimators'</span>: [<span class="number">600</span>]&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> cv_params[<span class="string">'n_estimators'</span>]:</span><br><span class="line">        model = lgb.LGBMRegressor(boosting_type=<span class="string">'gbdt'</span>, num_leaves=<span class="number">63</span>, max_depth=<span class="number">-1</span>, </span><br><span class="line">                                  learning_rate=<span class="number">0.01</span>, n_estimators=<span class="number">1200</span>,                                                      subsample_for_bin=<span class="number">200000</span>, objective=<span class="string">'gamma'</span>,                                                     class_weight=<span class="keyword">None</span>, min_split_gain=<span class="number">0.01</span>, </span><br><span class="line">                                  min_child_weight=<span class="number">0.001</span>, min_child_samples=<span class="number">20</span>,                                                   subsample=<span class="number">0.8</span>, subsample_freq=<span class="number">1</span>,                                                                 colsample_bytree=<span class="number">1.0</span>, reg_alpha=<span class="number">1.0</span>, reg_lambda=<span class="number">1.0</span>,                                             random_state=<span class="keyword">None</span>, n_jobs=<span class="number">4</span>, silent=<span class="keyword">True</span>)</span><br><span class="line">        kf = KFold(n_splits=<span class="number">5</span>,shuffle=<span class="keyword">True</span>,seed=<span class="number">29</span>)<span class="comment">#随机5折，设置随机种子</span></span><br><span class="line">        X = train[predictors]</span><br><span class="line">        y = train[target]</span><br><span class="line">        score = []</span><br><span class="line">        score1 = []</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        final_result = pd.DataFrame()</span><br><span class="line">        final_result[<span class="string">'vid'</span>] = test[<span class="string">'vid'</span>]</span><br><span class="line">        <span class="keyword">for</span> train_index,test_index <span class="keyword">in</span> kf.split(X):<span class="comment">#做5折cv，加入了样本权重（lgb支持），每折后预测test集，结果保存在final_result[i]中，最后对每折的结果做平均，得到最终的结果。</span></span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">            X_train, X_test = X.ix[train_index], X.ix[test_index]</span><br><span class="line">            y_train, y_test = y.ix[train_index], y.ix[test_index]</span><br><span class="line">            model.fit(X_train.drop(<span class="string">'weight'</span>,axis=<span class="number">1</span>), </span><br><span class="line">                      y_train,eval_set=[(X_test.drop(<span class="string">'weight'</span>,axis=<span class="number">1</span>),y_test)],</span><br><span class="line">                      eval_metric=<span class="string">'rmse'</span>,</span><br><span class="line">                      early_stopping_rounds=<span class="number">2</span>,</span><br><span class="line">                      verbose=<span class="keyword">False</span>,sample_weight=X_train[<span class="string">'weight'</span>])</span><br><span class="line">            prediction = model.predict(X_test.drop(<span class="string">'weight'</span>,axis=<span class="number">1</span>))</span><br><span class="line">            score.append(evalerror1(preds=prediction,labels=y_test))</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'the &#123;0&#125; &#123;1&#125;round Jcv: &#123;2&#125;'</span>.format(kind,count,score[<span class="number">-1</span>])</span><br><span class="line">            prediction = model.predict(X_train.drop(<span class="string">'weight'</span>,axis=<span class="number">1</span>))</span><br><span class="line">            score1.append(evalerror1(preds=prediction,labels=y_train))</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'the &#123;0&#125; round Jtrain: &#123;1&#125;'</span>.format(kind,score1[<span class="number">-1</span>])</span><br><span class="line">            predictors1 = [x <span class="keyword">for</span> x <span class="keyword">in</span> test.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [IDcol]]</span><br><span class="line">            final_result[str(count)] = model.predict(test[predictors1])</span><br><span class="line">    <span class="keyword">return</span> final_result,score</span><br></pre></td></tr></table></figure><p>​    可以看到我们的做法是：做5折cv，加入了样本权重（lgb支持），每折后预测test集，结果保存在final_result[i]中，最后对每折的结果做平均，得到最终的结果。每折的线下评估会保存在一个list<br>中，这样最后可以得到5个线下得分，据此可以计算5次得分的均值得到模型的偏差，5次得分的方差得到模型的方差，据此评估模型的准确性和泛化能力。这就是我们的单模型建模思路以及线下评估机制。</p><p>​    接下来，我想讨论一下如何做一个可持续上分的建模策略，这一部分不涉及我们的具体工作（因为比赛中我们并没有做到这一点），只是自己的一些想法。</p><h4 id="上分过程"><a href="#上分过程" class="headerlink" title="上分过程"></a>上分过程</h4><p>GBDT数字+粗糙处理文本特征特征：线下：0.02000，线上：0.03712，线下成绩比第一名还高，线上一测40名。后来发现是线下测评函数写错了log（x+1）写成了log（x+2）。</p><p>xgboost（objective=rmse）数字+粗糙处理文本特征：线下：0.032，线上：0.03560，线上线下差别明显减少，xgb的效果也确实比gbdt好。</p><p>xgboost（objective=gamma）数字+粗糙处理文本特征： 线下： ，线上：0.03411，对于这个数据集而言gamma分布回归效果要好于linear（rmse）。</p><p>lightgbm（objective=gamma）数字+粗糙处理文本特征：线下：，线上：0.03383，lgb号称是相比于xgboost损失了精度提升了效率，但是我们实际测试发现lgb应该是全方位超过了xgboost，原因应该是<br>leaf-wise的树生成策略弥补了最优分裂点寻找时损失的精度。</p><p>lightgbm（objective=gamma）数字+精细处理文本特征：线下： ，线上：0.03130，这里主要是对文本特征进行了提取工作，提取了脂肪肝、胰腺炎、脾脏健康程度等相关的特征。</p><p>lightgbm（objective=gamma）数字+精细处理文本特征+模型参数调整（<strong>basescore非常重要</strong>）：线下：，线上：0.03054。</p><p>lightgbm（objective=gamma）数字+精细处理文本特征+模型参数调整+组合特征：线下： ，线上：0.02947，在之前的基础上加入了组合特征：性别。</p><p><strong>notice</strong>：这次的提交记录做的不是很好，以后需要对每次提交结果的特征、模型、参数、得分做更详细的记录。</p><p>​    第一次踩了很多坑，希望会对以后有所帮助。这些是总结出来的上分过程，实际的上分过程远远比这里叙述的曲折，譬如每次特征有优化，我们就要把之前的很多工作重复做一遍，浪费了很多时间和<br>精力在重复的劳动上。如果特征的结构出现了较大改动，那可能之前搭好的模型里面需要调整很多内容（5折cv，5个label相当于是25个模型）。造成这个原因更多因为我们代码架构问题，总是想着快点做<br>出结果所以没有很好地去思考如何做好整体的架构（当然这跟自己代码能力弱有很大关系）。下次做比赛需要了解一下pipeline这个东西，据说是专门用于模型之间的参数传递的，对于一些复杂模型，<br>这个pipeline会比较有用。</p><h4 id="数据预处理class"><a href="#数据预处理class" class="headerlink" title="数据预处理class"></a>数据预处理class</h4><p>​    另外，可以写一个数据预处理的class，参数传递采用整个数据集（前提是数据集不大，内存可以读的下），这样在整个class里面可以定义不同的处理方法（缺失值处理方法，labelencoder，<br>onehot encoder等），每个方法输入值为整个数据集，输出值是改变后的数据集，如果需要修改特征，只需要增加方法再调用即可。然后还需要有一个训练集、测试集切分的方法，可以灵活快速地得<br>到A榜、B榜的数据（这次比赛中我们这种机制其实做的已经不错了）。另外还要加入特征筛选的方法（过滤式、包裹式），以及对数据集的切分（为cross-validation服务）。</p><h4 id="可迭代的模型训练"><a href="#可迭代的模型训练" class="headerlink" title="可迭代的模型训练"></a>可迭代的模型训练</h4><p>​    这一块主要是对于模型的参数进行修改、模型融合，这次在比赛中发现了树模型中比较重要的参数有basescore、objective、eta、tree_num等，模型融合的方法主要有stacking、blending等。<br>需要注意的是，当融合了很多个模型的时候，修改参数会变成一个非常繁琐的任务，因为众多模型融合的时候，已经没有办法用控制变量的方法去查看那些参数会对结果又提升（因为参数实在太多了）<br>。所以我很好奇的一点是，有篇文章说kaggle比赛的第一名融合了上千个模型是如何做到的？我没有找到那个解决方案的开源代码，难道是不断融合结果.csv？那我是否可以做一个稳定模型后，把模型<br>保存起来（用pickle 这个package），随着比赛的进行我们会积累各种各样的模型，最后把这些模型载入，对test集进行预测做平均融合，这样我们每一次新产生的特征或者模型都会在之前的基础上<br>迭代，充分利用了之前的工作（这些工作的特征工程可能不尽相同，模型也不尽相同，所以融合可以得到不错的效果），可以形成一个可持续上分的训练机制。</p><p>​    最后说说模型融合：stacking、blending、平均融合。stacking容易出现数据泄露问题（其实就是过拟合，线下分数非常高，线上非常惨），但是优点是利用了所有训练数据集。blending的问题<br>是有一部分训练集没有利用到，但是好处是不存在数据泄露问题。平均融合就是直接平均结果.csv，是现在天池这类竞赛强强联合最重要的一个手段，第二名和第三名把结果融合一下，可能分数就<br>超过第一名了。stacking和blending的做法比平均融合要复杂的多，贴一下我们复赛做的stacking代码吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_ground</span><span class="params">(kind=<span class="string">'sys'</span>)</span>:</span></span><br><span class="line">    test_data_set = test_dataset_all_sys</span><br><span class="line">    result = pd.DataFrame()</span><br><span class="line">    new_feat = pd.DataFrame()</span><br><span class="line">    result[<span class="string">'vid'</span>] = o.get_table(test_data_set,project=<span class="string">'prj_tc_231654_144430_w9xa66'</span>).to_df().to_pandas()[<span class="string">'vid'</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,k_fold+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> (i==<span class="number">1</span>) | (i==<span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> kind == <span class="string">'sys'</span>:</span><br><span class="line">                baseScore = <span class="string">'100'</span></span><br><span class="line">            <span class="keyword">elif</span> kind == <span class="string">'dia'</span>:</span><br><span class="line">                baseScore = <span class="string">'70'</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                baseScore = <span class="string">'0.5'</span></span><br><span class="line">            model_name = <span class="string">'ps_smart_wxc_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            input_train_name = <span class="string">'train_all_data_cv_0606_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            input_test_name = <span class="string">'test_all_data_cv_0606_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            output_pred_name_cv = <span class="string">'pssmart_cv_'</span>+kind+str(i)</span><br><span class="line">            output_pred_name = <span class="string">'pssmart_pred_'</span>+kind+str(i)</span><br><span class="line">            o.execute_sql(<span class="string">'drop table if exists '</span>+output_pred_name+ <span class="string">';'</span>)  <span class="comment">#  同步的方式执行，会阻塞直到SQL执行完成</span></span><br><span class="line">            o.execute_sql(<span class="string">'drop table if exists '</span>+output_pred_name_cv+ <span class="string">';'</span>) </span><br><span class="line">            o.delete_offline_model(model_name,if_exists=<span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">                parameters_train = &#123;<span class="string">'objective'</span>:<span class="string">'reg:gamma'</span>,<span class="string">'featureColNames'</span>:predictors,<span class="string">'metric'</span>:<span class="string">'rmse'</span>,<span class="string">'treeCount'</span>:<span class="string">'1000'</span>,<span class="string">'shrinkage'</span>:<span class="string">'0.02'</span>,</span><br><span class="line">                                    <span class="string">'l1'</span>:<span class="string">'0'</span>,<span class="string">'l2'</span>:<span class="string">'0'</span>,<span class="string">'maxDepth'</span>:<span class="string">'5'</span>,<span class="string">'baseScore'</span>:baseScore,</span><br><span class="line">                                    <span class="string">'labelColName'</span>:kind,<span class="string">'inputTableName'</span>:<span class="string">'prj_tc_231654_144430_w9xa66.'</span>+input_train_name,</span><br><span class="line">                                    <span class="string">'modelName'</span>:<span class="string">r'algo_public/offlinemodels/'</span>+model_name&#125;</span><br><span class="line">            <span class="keyword">elif</span> i == <span class="number">2</span>:</span><br><span class="line">                parameters_train = &#123;<span class="string">'objective'</span>:<span class="string">'reg:gamma'</span>,<span class="string">'featureColNames'</span>:predictors,<span class="string">'metric'</span>:<span class="string">'rmse'</span>,<span class="string">'treeCount'</span>:<span class="string">'1200'</span>,<span class="string">'shrinkage'</span>:<span class="string">'0.02'</span>,</span><br><span class="line">                                    <span class="string">'l1'</span>:<span class="string">'0'</span>,<span class="string">'l2'</span>:<span class="string">'0.5'</span>,<span class="string">'maxDepth'</span>:<span class="string">'5'</span>,<span class="string">'baseScore'</span>:<span class="string">'110'</span>,</span><br><span class="line">                                    <span class="string">'labelColName'</span>:kind,<span class="string">'inputTableName'</span>:<span class="string">'prj_tc_231654_144430_w9xa66.'</span>+input_train_name,</span><br><span class="line">                                    <span class="string">'modelName'</span>:<span class="string">r'algo_public/offlinemodels/'</span>+model_name&#125;</span><br><span class="line">            parameters_test = &#123;<span class="string">'modelName'</span>:<span class="string">r'algo_public/offlinemodels/'</span>+model_name,<span class="string">'inputTableName'</span>:input_test_name,</span><br><span class="line">                               <span class="string">'outputTableName'</span>:output_pred_name_cv,<span class="string">'featureColNames'</span>:predictors,</span><br><span class="line">                               <span class="string">'appendColNames'</span>:<span class="string">'vid'</span>,<span class="string">'resultColName'</span>:kind+<span class="string">'_pred'</span>&#125;</span><br><span class="line">            parameters_pred = &#123;<span class="string">'modelName'</span>:<span class="string">r'algo_public/offlinemodels/'</span>+model_name,<span class="string">'inputTableName'</span>:test_data_set,</span><br><span class="line">                               <span class="string">'outputTableName'</span>:output_pred_name,<span class="string">'featureColNames'</span>:predictors,</span><br><span class="line">                               <span class="string">'appendColNames'</span>:<span class="string">'vid'</span>,<span class="string">'resultColName'</span>:kind+<span class="string">'_pred'</span>&#125;       </span><br><span class="line">            <span class="comment"># use 4 fold data train model                  </span></span><br><span class="line">            inst_train = o.execute_xflow(<span class="string">'ps_smart'</span>, <span class="string">'algo_public'</span>,parameters=parameters_train)</span><br><span class="line">            <span class="comment"># get 1 fold test data predictions</span></span><br><span class="line">            inst_predict = o.execute_xflow(<span class="string">'prediction'</span>,<span class="string">'algo_public'</span>,parameters=parameters_test)</span><br><span class="line">            predict = o.get_table(output_pred_name_cv,project=<span class="string">'prj_tc_231654_144430_w9xa66'</span>).to_df().to_pandas()</span><br><span class="line">            new_feat = new_feat.append(predict)</span><br><span class="line">            <span class="comment"># get test predictions</span></span><br><span class="line">            inst_predict_test = o.execute_xflow(<span class="string">'prediction'</span>,<span class="string">'algo_public'</span>,parameters=parameters_pred)</span><br><span class="line">            result[<span class="string">'result_pssmart_'</span>+str(i)] = o.get_table(output_pred_name,project=<span class="string">'prj_tc_231654_144430_w9xa66'</span>).to_df().to_pandas()[kind+<span class="string">'_pred'</span>]</span><br><span class="line">        <span class="keyword">if</span> (i==<span class="number">3</span>) | (i==<span class="number">4</span>):</span><br><span class="line">            model_name = <span class="string">'xgboost_wxc_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            input_train_name = <span class="string">'train_all_data_cv_0606_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            input_test_name = <span class="string">'test_all_data_cv_0606_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            output_pred_name_cv = <span class="string">'xgboost_cv_'</span>+kind+str(i)</span><br><span class="line">            output_pred_name = <span class="string">'xgboost_pred_'</span>+kind+str(i)</span><br><span class="line">            o.execute_sql(<span class="string">'drop table if exists '</span>+output_pred_name+ <span class="string">';'</span>)  <span class="comment">#  同步的方式执行，会阻塞直到SQL执行完成</span></span><br><span class="line">            o.execute_sql(<span class="string">'drop table if exists '</span>+output_pred_name_cv+ <span class="string">';'</span>) </span><br><span class="line">            o.delete_offline_model(model_name,if_exists=<span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">                parameters_train = &#123;<span class="string">'eta'</span>:<span class="string">'0.01'</span>,<span class="string">'num_round'</span>:<span class="string">'1200'</span>,<span class="string">'featureColNames'</span>:predictors,<span class="string">'labelColName'</span>:kind,<span class="string">'max_depth'</span>:<span class="string">'8'</span>,<span class="string">'colsample_bytree'</span>:<span class="string">'0.6'</span>,<span class="string">'seed'</span>:<span class="string">'0'</span>,</span><br><span class="line">                                    <span class="string">'objective'</span>:<span class="string">'reg:linear'</span>,<span class="string">'eval_metric'</span>:<span class="string">'rmse'</span>,<span class="string">'inputTableName'</span>:input_train_name,<span class="string">'modelName'</span>:model_name,<span class="string">'subsample'</span>:<span class="string">'0.8'</span>,<span class="string">'gamma'</span>:<span class="string">'0'</span>,</span><br><span class="line">                                    <span class="string">'lambda'</span>:<span class="string">'50'</span>&#125;</span><br><span class="line">            <span class="keyword">elif</span> i == <span class="number">4</span>:</span><br><span class="line">                parameters_train = &#123;<span class="string">'eta'</span>:<span class="string">'0.02'</span>,<span class="string">'num_round'</span>:<span class="string">'1500'</span>,<span class="string">'featureColNames'</span>:predictors,<span class="string">'labelColName'</span>:kind,<span class="string">'max_depth'</span>:<span class="string">'8'</span>,<span class="string">'colsample_bytree'</span>:<span class="string">'0.6'</span>,<span class="string">'seed'</span>:<span class="string">'0'</span>,</span><br><span class="line">                                    <span class="string">'objective'</span>:<span class="string">'reg:linear'</span>,<span class="string">'eval_metric'</span>:<span class="string">'rmse'</span>,<span class="string">'inputTableName'</span>:input_train_name,<span class="string">'modelName'</span>:model_name,<span class="string">'subsample'</span>:<span class="string">'0.8'</span>,<span class="string">'gamma'</span>:<span class="string">'0'</span>,</span><br><span class="line">                                    <span class="string">'lambda'</span>:<span class="string">'50'</span>&#125;</span><br><span class="line">            parameters_test = &#123;<span class="string">'modelName'</span>:model_name,<span class="string">'inputTableName'</span>:input_test_name,</span><br><span class="line">                               <span class="string">'outputTableName'</span>:output_pred_name_cv,<span class="string">'featureColNames'</span>:predictors,</span><br><span class="line">                               <span class="string">'appendColNames'</span>:<span class="string">'vid'</span>,<span class="string">'resultColName'</span>:kind+<span class="string">'_pred'</span>&#125;   </span><br><span class="line">            parameters_pred = &#123;<span class="string">'modelName'</span>:model_name,<span class="string">'inputTableName'</span>:test_data_set,</span><br><span class="line">                               <span class="string">'outputTableName'</span>:output_pred_name,<span class="string">'featureColNames'</span>:predictors,</span><br><span class="line">                               <span class="string">'appendColNames'</span>:<span class="string">'vid'</span>,<span class="string">'resultColName'</span>:kind+<span class="string">'_pred'</span>&#125;      </span><br><span class="line">            <span class="comment"># use 4 fold data train model</span></span><br><span class="line">            inst_train = o.execute_xflow(<span class="string">'xgboost'</span>, <span class="string">'algo_public'</span>,parameters=parameters_train)</span><br><span class="line">            <span class="comment"># get 1 fold test data predictions</span></span><br><span class="line">            inst_predict = o.execute_xflow(<span class="string">'prediction'</span>,<span class="string">'algo_public'</span>,parameters=parameters_test)</span><br><span class="line">            predict = o.get_table(output_pred_name_cv,project=<span class="string">'prj_tc_231654_144430_w9xa66'</span>).to_df().to_pandas()</span><br><span class="line">            new_feat = new_feat.append(predict)</span><br><span class="line">            <span class="comment"># get test predictions</span></span><br><span class="line">            inst_predict_test = o.execute_xflow(<span class="string">'prediction'</span>,<span class="string">'algo_public'</span>,parameters=parameters_pred)</span><br><span class="line">            result[<span class="string">'result_xgb_'</span>+str(i)] = o.get_table(output_pred_name,project=<span class="string">'prj_tc_231654_144430_w9xa66'</span>).to_df().to_pandas()[kind+<span class="string">'_pred'</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">5</span>:</span><br><span class="line">            model_name = <span class="string">'GBDT_wxc_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            input_train_name = <span class="string">'train_all_data_cv_0606_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            input_test_name = <span class="string">'test_all_data_cv_0606_'</span>+kind+<span class="string">'_'</span>+str(i)</span><br><span class="line">            output_pred_name_cv = <span class="string">'GBDT_cv_'</span>+kind+str(i)</span><br><span class="line">            output_pred_name = <span class="string">'GBDT_pred_'</span>+kind+str(i)</span><br><span class="line">            o.execute_sql(<span class="string">'drop table if exists '</span>+output_pred_name+ <span class="string">';'</span>)  <span class="comment">#  同步的方式执行，会阻塞直到SQL执行完成</span></span><br><span class="line">            o.execute_sql(<span class="string">'drop table if exists '</span>+output_pred_name_cv+ <span class="string">';'</span>) </span><br><span class="line">            o.delete_offline_model(model_name,if_exists=<span class="keyword">True</span>)</span><br><span class="line">            parameters_train = &#123;<span class="string">'shrinkage'</span>:<span class="string">'0.02'</span>,<span class="string">'lossType'</span>:<span class="string">'3'</span>,<span class="string">'newtonStep'</span>:<span class="string">'1'</span>,<span class="string">'metricType'</span>:<span class="string">'0'</span>,<span class="string">'featureColNames'</span>:predictors,</span><br><span class="line">                                <span class="string">'labelColName'</span>:kind,<span class="string">'maxDepth'</span>:<span class="string">'7'</span>,<span class="string">'featureRatio'</span>:<span class="string">'1.0'</span>,<span class="string">'randSeed'</span>:<span class="string">'2'</span>,<span class="string">'minLeafSampleCount'</span>:<span class="string">'100'</span>,</span><br><span class="line">                                <span class="string">'sampleRatio'</span>:<span class="string">'1.0'</span>,<span class="string">'treeCount'</span>:<span class="string">'1000'</span>,<span class="string">'inputTableName'</span>:input_train_name,<span class="string">'modelName'</span>:model_name&#125;</span><br><span class="line">            parameters_test = &#123;<span class="string">'modelName'</span>:model_name,<span class="string">'inputTableName'</span>:input_test_name,</span><br><span class="line">                               <span class="string">'outputTableName'</span>:output_pred_name_cv,<span class="string">'featureColNames'</span>:predictors,</span><br><span class="line">                               <span class="string">'appendColNames'</span>:<span class="string">'vid'</span>,<span class="string">'resultColName'</span>:kind+<span class="string">'_pred'</span>&#125;  </span><br><span class="line">            parameters_pred = &#123;<span class="string">'modelName'</span>:model_name,<span class="string">'inputTableName'</span>:test_data_set,</span><br><span class="line">                               <span class="string">'outputTableName'</span>:output_pred_name,<span class="string">'featureColNames'</span>:predictors,</span><br><span class="line">                               <span class="string">'appendColNames'</span>:<span class="string">'vid'</span>,<span class="string">'resultColName'</span>:kind+<span class="string">'_pred'</span>&#125;     </span><br><span class="line">            <span class="comment"># use 4 fold data train model</span></span><br><span class="line">            inst_train = o.execute_xflow(<span class="string">'GBDT'</span>, <span class="string">'algo_public'</span>,parameters=parameters_train)</span><br><span class="line">            <span class="comment"># get 1 fold test data predictions and make new feature</span></span><br><span class="line">            inst_predict = o.execute_xflow(<span class="string">'prediction'</span>,<span class="string">'algo_public'</span>,parameters=parameters_test)</span><br><span class="line">            predict = o.get_table(output_pred_name_cv,project=<span class="string">'prj_tc_231654_144430_w9xa66'</span>).to_df().to_pandas()</span><br><span class="line">            new_feat = new_feat.append(predict)</span><br><span class="line">            <span class="comment"># get test predictions</span></span><br><span class="line">            inst_predict_test = o.execute_xflow(<span class="string">'prediction'</span>,<span class="string">'algo_public'</span>,parameters=parameters_pred)</span><br><span class="line">            result[<span class="string">'result_GBDT_'</span>+str(i)] = o.get_table(output_pred_name,project=<span class="string">'prj_tc_231654_144430_w9xa66'</span>).to_df().to_pandas()[kind+<span class="string">'_pred'</span>]    </span><br><span class="line">    col = [x <span class="keyword">for</span> x <span class="keyword">in</span> result.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'vid'</span>]]</span><br><span class="line">    result[kind] = result[col].mean(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">print</span> result</span><br><span class="line">    <span class="keyword">return</span> result[[<span class="string">'vid'</span>,kind]],new_feat[[<span class="string">'vid'</span>,kind+<span class="string">'_pred'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_sec_floor</span><span class="params">(kind)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> kind == <span class="string">'sys'</span>:</span><br><span class="line">        baseScore = <span class="number">100</span></span><br><span class="line">    <span class="keyword">elif</span> kind == <span class="string">'dia'</span>:</span><br><span class="line">        baseScore = <span class="number">70</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        baseScore = <span class="number">0.5</span></span><br><span class="line">    model_name = <span class="string">'ps_smart_wxc_'</span>+kind+<span class="string">'_sec'</span></span><br><span class="line">    input_train_name = <span class="string">'train_all_data_newfeat_'</span>+kind</span><br><span class="line">    input_test_name = <span class="string">'test_all_data_newfeat_'</span>+kind</span><br><span class="line">    output_pred_name = <span class="string">'pssmart_pred_'</span>+kind+<span class="string">'_final'</span></span><br><span class="line">    o.execute_sql(<span class="string">'drop table if exists '</span>+output_pred_name+ <span class="string">';'</span>)  <span class="comment">#  同步的方式执行，会阻塞直到SQL执行完成</span></span><br><span class="line">    o.delete_offline_model(model_name,if_exists=<span class="keyword">True</span>)</span><br><span class="line">    parameters_train = &#123;<span class="string">'objective'</span>:<span class="string">'reg:gamma'</span>,<span class="string">'featureColNames'</span>:predictors,<span class="string">'metric'</span>:<span class="string">'rmse'</span>,<span class="string">'treeCount'</span>:<span class="string">'1000'</span>,<span class="string">'shrinkage'</span>:<span class="string">'0.02'</span>,</span><br><span class="line">                        <span class="string">'l1'</span>:<span class="string">'0'</span>,<span class="string">'l2'</span>:<span class="string">'0'</span>,<span class="string">'maxDepth'</span>:<span class="string">'5'</span>,<span class="string">'baseScore'</span>:baseScore,</span><br><span class="line">                        <span class="string">'labelColName'</span>:kind,<span class="string">'inputTableName'</span>:<span class="string">'prj_tc_231654_144430_w9xa66.'</span>+input_train_name,</span><br><span class="line">                        <span class="string">'modelName'</span>:<span class="string">r'algo_public/offlinemodels/'</span>+model_name&#125;</span><br><span class="line">    parameters_test = &#123;<span class="string">'modelName'</span>:<span class="string">r'algo_public/offlinemodels/'</span>+model_name,<span class="string">'inputTableName'</span>:input_test_name,</span><br><span class="line">                       <span class="string">'outputTableName'</span>:output_pred_name,<span class="string">'featureColNames'</span>:predictors,</span><br><span class="line">                       <span class="string">'appendColNames'</span>:<span class="string">'vid'</span>,<span class="string">'resultColName'</span>:kind+<span class="string">'_pred'</span>&#125;      </span><br><span class="line">    <span class="comment"># use all data train model                  </span></span><br><span class="line">    inst_train = o.execute_xflow(<span class="string">'ps_smart'</span>, <span class="string">'algo_public'</span>,parameters=parameters_train)</span><br><span class="line">    <span class="comment"># get final predictions</span></span><br><span class="line">    inst_predict = o.execute_xflow(<span class="string">'prediction'</span>,<span class="string">'algo_public'</span>,parameters=parameters_test)</span><br><span class="line">    final_result = o.get_table(output_pred_name,project=<span class="string">'prj_tc_231654_144430_w9xa66'</span>).to_df().to_pandas()</span><br><span class="line">    <span class="keyword">return</span> final_result[[<span class="string">'vid'</span>,kind+<span class="string">'_pred'</span>]]</span><br></pre></td></tr></table></figure><p>​    这仅仅是两层的stacking，如果要做更多层的融合，需要写一个class来做了，不然代码太难维护了，之前在GitHub上看到过一个这样的实例，后面会贴出来。</p><h2 id="资料整理"><a href="#资料整理" class="headerlink" title="资料整理"></a>资料整理</h2><p>blending实现方案：<a href="https://github.com/vandesa003/vertebral/blob/master/stacked_generalization.py" target="_blank" rel="noopener">blending</a></p><p>stacking实现方案：可以自己做。</p><p>平均融合实现方案：没有连接，只有代码。</p><p>贪婪特征组合方案：<a href="https://github.com/vandesa003/tianchi-diabetes-top12" target="_blank" rel="noopener">贪婪特征组合</a></p><h1 id="复赛"><a href="#复赛" class="headerlink" title="复赛"></a>复赛</h1><p>​    复赛的思路跟初赛差不多，加入了基因序列，但是相关性好像不大。数据量骤减到3000个样本。主要讲一下阿里的机器学习平台吧。平台限制很多，只有GBDT、RF、PS-SMART<br>（阿里自己开发的树模型，比较类似lgb）、Xgboost（因为版权问题，平台隐藏了，但是可以用PAI命令调用，然而没有文档，仅靠之前的一个前辈的帖子加自己摸索），可以安装外部<br>package，前提是纯python的package，不能调用C++接口。具体平台的使用方法可以直接查看官方文档。平台提供了pyodps的python接口，可以用python写所有的代码，其他没什么好说的了，<br>可以直接看代码（我们还找到了ps-smart的python接口的一个bug，还好最后官方给了替代的解决方案）。</p><h2 id="SMOT"><a href="#SMOT" class="headerlink" title="SMOT"></a>SMOT</h2><p>复赛的数据量太少了，模型非常容易过拟合导致线上成绩很不理想。因此我们用SMOT做了数据集扩充。具体的方案是这样的：</p><table><thead><tr><th>sample</th><th>feat1</th><th>feat2</th><th>label</th></tr></thead><tbody><tr><td>1</td><td><em>feat1_1</em></td><td><em>feat2_1</em></td><td>1</td></tr><tr><td>1</td><td><strong>feat1_2</strong></td><td><strong>feat2_2</strong></td><td>1</td></tr><tr><td>fake sample</td><td><em>feat1_1</em></td><td><strong>feat2_2</strong></td><td>1</td></tr></tbody></table><p>​    通过交换相同label的样本的不同特征，可以得到一个新的人造样本fake sample。通过这样的做法我们把3000数据集扩充到了4000。但是很遗憾线上并没有带来提升，但是我依然觉得这是<br>小数据集上的一个可行的方案。</p><p>​    另外还有一些过采样的方案可以应用在小数据集上，但是过采样方案只是单纯的复制样本，更容易造成不平衡的数据集，因此我们觉得没有SMOT合理，没有尝试。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>数据分布很重要，不管是特征的分布还是label的分布。有时候我们判断线上分数是否会上升就是直接从预测数据的分布入手的。</li><li>模型参数中的basescore很重要，初赛的时候一直忽视了这个点（跟我们迭代次数少，学习率较大可能也有关系）。</li><li>特征需要做更细化的处理，我们的模型都采用了跟前几名相同的模型，参数也都大同小异，在跟一些成绩不错的选手交流后，发现他们的特征工程都做的非常细致，而我们的特征做的有<br>点粗糙，这点应该是我们后期被卡住的主要原因（重做特征时间成本有点大，就没有再做了）。如果下次做比赛，一定要把重心放在特征工程上。</li></ol><h1 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h1><p>​    这两个月过得还是挺充实的，在比赛的过程中学习到了很多知识，也认识了很多大佬。其实非常感谢我的两个靠谱队友（茹宝、师兄）。一起刷了2次夜，开了无数次会，写了无数行<br>代码，队内的气氛都非常欢乐。自己会觉得有点愧疚没有能拿到更好的成绩，对不起大家两个月以来的付出。不过，时间还有很多，剩下的一年里足够野蛮生长，成为大佬。希望我们都能从这次<br>比赛中收获自己想要的东西，结果嘛，不是太重要~</p>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 数据预处理 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Introduction to ML with Python</title>
      <link href="/2018/06/05/Introduction-to-ML-with-Python/"/>
      <url>/2018/06/05/Introduction-to-ML-with-Python/</url>
      <content type="html"><![CDATA[<h1 id="sklearn中的交叉验证"><a href="#sklearn中的交叉验证" class="headerlink" title="sklearn中的交叉验证"></a>sklearn中的交叉验证</h1><h2 id="标准交叉验证"><a href="#标准交叉验证" class="headerlink" title="标准交叉验证"></a>标准交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">cancer = load_breast_cancer()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">logerg = LogisticRegression()</span><br><span class="line"></span><br><span class="line">scores = cross_val_score(logerg, iris.data, iris.target)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores: &#123;&#125;"</span>.format(scores)</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores: [ 0.96078431  0.92156863  0.95833333]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scores = cross_val_score(logerg, iris.data, iris.target, cv=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores: &#123;&#125;"</span>.format(scores)</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores: [ 1.          0.96666667  0.93333333  0.9         1.        ]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Average cross-validation score: &#123;:.2f&#125;"</span>.format(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>Average cross-validation score: 0.96</code></pre><a id="more"></a>    <h2 id="对交叉验证的更多控制"><a href="#对交叉验证的更多控制" class="headerlink" title="对交叉验证的更多控制"></a>对交叉验证的更多控制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(</span><br><span class="line">       cross_val_score(logerg, iris.data, iris.target, cv=kfold))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 1.          0.93333333  0.43333333  0.96666667  0.43333333]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kfold = KFold(n_splits=<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(</span><br><span class="line">       cross_val_score(logerg, iris.data, iris.target, cv=kfold))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 0.  0.  0.]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kfold = KFold(n_splits=<span class="number">3</span>, shuffle=<span class="keyword">True</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(</span><br><span class="line">       cross_val_score(logerg, iris.data, iris.target, cv=kfold))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 0.9   0.96  0.96]</code></pre><h2 id="留一法交叉验证"><a href="#留一法交叉验证" class="headerlink" title="留一法交叉验证"></a>留一法交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> LeaveOneOut</span><br><span class="line">loo = LeaveOneOut()</span><br><span class="line">scores = cross_val_score(logerg, iris.data, iris.target, cv=loo)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Number of cv iterations: "</span>, len(scores)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Mean accuracy: &#123;:.2f&#125;"</span>.format(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>Number of cv iterations:  150Mean accuracy: 0.95</code></pre><h2 id="打乱划分交叉验证"><a href="#打乱划分交叉验证" class="headerlink" title="打乱划分交叉验证"></a>打乱划分交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line">shuffle_split = ShuffleSplit(test_size=<span class="number">.5</span>, train_size=<span class="number">.5</span>, n_splits=<span class="number">10</span>)</span><br><span class="line">scores = cross_val_score(logerg, iris.data, iris.target, cv=shuffle_split)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(scores)</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 0.97333333  0.97333333  0.92        0.88        0.97333333  0.96  0.94666667  0.96        0.77333333  0.89333333]</code></pre><h2 id="分组交叉验证"><a href="#分组交叉验证" class="headerlink" title="分组交叉验证"></a>分组交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GroupKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="comment"># 创建模拟数据集</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">12</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 假设前3个样本属于同一组，接下来的4个样本属于同一组，以此类推</span></span><br><span class="line">groups = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">scores = cross_val_score(logerg, X, y, groups, cv=GroupKFold(n_splits=<span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(scores)</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 0.75        0.8         0.66666667]</code></pre><h1 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h1><h2 id="简单网格搜索"><a href="#简单网格搜索" class="headerlink" title="简单网格搜索"></a>简单网格搜索</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    iris.data, iris.target, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Size of training set: &#123;&#125;  size of the test set: &#123;&#125;"</span>.format(</span><br><span class="line">    X_train.shape[<span class="number">0</span>], X_test.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">best_score = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> gamma <span class="keyword">in</span> [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]:</span><br><span class="line">    <span class="keyword">for</span> C <span class="keyword">in</span> [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]:</span><br><span class="line">        svm = SVC(gamma=gamma,C=C)</span><br><span class="line">        svm.fit(X_train, y_train)</span><br><span class="line">        score = svm.score(X_test, y_test)</span><br><span class="line">        <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            best_parameters = &#123;<span class="string">'C'</span>: C, <span class="string">'gamma'</span>: gamma&#125;</span><br><span class="line">            </span><br><span class="line"><span class="keyword">print</span> <span class="string">"Best score: &#123;:.2f&#125;"</span>.format(best_score)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Best parameters: &#123;&#125;"</span>.format(best_parameters)</span><br></pre></td></tr></table></figure><pre><code>Size of training set: 112  size of the test set: 38Best score: 0.97Best parameters: {&apos;C&apos;: 100, &apos;gamma&apos;: 0.001}</code></pre><ul><li>training set: Model fitting</li><li>validation set: Patameter selection</li><li>test set: Evaluation</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line">mglearn.plots.plot_grid_search_overview()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">450</span>, n_features=<span class="number">2</span>, centers=<span class="number">2</span>, cluster_std=[<span class="number">7.0</span>, <span class="number">2</span>],</span><br><span class="line">                random_state=<span class="number">22</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">svc = SVC(gamma=<span class="number">.05</span>).fit(X_train, y_train)</span><br><span class="line">precision, recall, thresholds = precision_recall_curve(</span><br><span class="line">    y_test, svc.decision_function(X_test))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mglearn.plots.plot_decision_threshold()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test, svc.predict(X_test))</span><br></pre></td></tr></table></figure><pre><code>             precision    recall  f1-score   support          0       0.96      0.80      0.87        60          1       0.81      0.96      0.88        53avg / total       0.89      0.88      0.88       113</code></pre><h1 id="预处理与缩放"><a href="#预处理与缩放" class="headerlink" title="预处理与缩放"></a>预处理与缩放</h1><h2 id="应用数据变换"><a href="#应用数据变换" class="headerlink" title="应用数据变换"></a>应用数据变换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,</span><br><span class="line">                                                   random_state=<span class="number">1</span>)</span><br><span class="line">print(X_train.shape)</span><br><span class="line">print(X_test.shape)</span><br></pre></td></tr></table></figure><pre><code>(426L, 30L)(143L, 30L)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">scaler.fit(X_train)</span><br><span class="line">MinMaxScaler(copy=<span class="keyword">True</span>, feature_range=(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#变换数据</span></span><br><span class="line">X_train_scaled = scaler.transform(X_train)</span><br><span class="line">X_test_scaled = scaler.transform(X_test)</span><br></pre></td></tr></table></figure><h1 id="降维、特征提取和流形学习"><a href="#降维、特征提取和流形学习" class="headerlink" title="降维、特征提取和流形学习"></a>降维、特征提取和流形学习</h1><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span>  sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(cancer.data)</span><br><span class="line">X_scaled = scaler.transform(cancer.data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X_scaled)</span><br><span class="line"></span><br><span class="line">X_pca = pca.transform(X_scaled)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Original shape: &#123;&#125;"</span>.format(X_scaled.shape)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Reduced shape: &#123;&#125;"</span>.format(X_pca.shape)</span><br></pre></td></tr></table></figure><pre><code>Original shape: (569L, 30L)Reduced shape: (569L, 2L)</code></pre><h2 id="非负矩阵分解-NMF"><a href="#非负矩阵分解-NMF" class="headerlink" title="非负矩阵分解 NMF"></a>非负矩阵分解 NMF</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line">mglearn.plots.plot_nmf_illustration()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> NMF</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</span><br><span class="line"></span><br><span class="line">people = fetch_lfw_people(min_faces_per_person=<span class="number">20</span>, resize=<span class="number">0.7</span>)</span><br><span class="line">X_people = people.data[mask]</span><br><span class="line">y_people = people.target[mask]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_people, y_people, stratify=y_people, random_state=<span class="number">0</span>)</span><br><span class="line">nmf = NMF(n_cpmponents=<span class="number">15</span>, random_state=<span class="number">0</span>)</span><br><span class="line">nmf.fit(X_train)</span><br><span class="line">X_train_nmf = nmf.transform(X_train)</span><br><span class="line">X_test_nmf = nmf.transform(X_test)</span><br><span class="line"></span><br><span class="line">fix, axes = plt.subplots(<span class="number">3</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>,<span class="number">12</span>),</span><br><span class="line">                        subplot_kw=&#123;<span class="string">'xticks'</span>:(), <span class="string">'yticks'</span>:()&#125;)</span><br><span class="line"><span class="keyword">for</span> i, (component,ax) <span class="keyword">in</span> enumerate(zip(nmf.components_, axes.ravel())):</span><br><span class="line">    ax.imshow(component.reshape(image_shape))</span><br><span class="line">    ax.set_title(<span class="string">"&#123;&#125;.component"</span>.format(i))</span><br></pre></td></tr></table></figure><h2 id="用t-SNE进行流形学习"><a href="#用t-SNE进行流形学习" class="headerlink" title="用t-SNE进行流形学习"></a>用t-SNE进行流形学习</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line">digits = load_digits()</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">10</span>,<span class="number">5</span>),</span><br><span class="line">                        subplot_kw=&#123;<span class="string">'xticks'</span>:(), <span class="string">'yticks'</span>:()&#125;)</span><br><span class="line"><span class="keyword">for</span> ax, img <span class="keyword">in</span> zip(axes.ravel(), digits.images):</span><br><span class="line">    ax.imshow(img)</span><br></pre></td></tr></table></figure><h1 id="数据表示与特征工程"><a href="#数据表示与特征工程" class="headerlink" title="数据表示与特征工程"></a>数据表示与特征工程</h1><h2 id="分箱、离散化、线性模型与树"><a href="#分箱、离散化、线性模型与树" class="headerlink" title="分箱、离散化、线性模型与树"></a>分箱、离散化、线性模型与树</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">X, y = mglearn.datasets.make_wave(n_samples=<span class="number">100</span>)</span><br><span class="line">line = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>, endpoint=<span class="keyword">False</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">reg = DecisionTreeRegressor(min_samples_split=<span class="number">3</span>).fit(X, y)</span><br><span class="line">plt.plot(line, reg.predict(line), label=<span class="string">"decision tree"</span>)</span><br><span class="line"></span><br><span class="line">reg = LinearRegression().fit(X, y)</span><br><span class="line">plt.plot(line, reg.predict(line), label=<span class="string">"linear regression"</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(X[:,<span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Regression output"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Input feature"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x103c4da0&gt;</code></pre><h2 id="自动化特征选择"><a href="#自动化特征选择" class="headerlink" title="自动化特征选择"></a>自动化特征选择</h2><h3 id="单变量统计"><a href="#单变量统计" class="headerlink" title="单变量统计"></a>单变量统计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">citibike = mglearn.datasets.load_citibike()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Citi Bike data:\n&#123;&#125;"</span>.format(citibike.head())</span><br></pre></td></tr></table></figure><pre><code>Citi Bike data:starttime2015-08-01 00:00:00     3.02015-08-01 03:00:00     0.02015-08-01 06:00:00     9.02015-08-01 09:00:00    41.02015-08-01 12:00:00    39.0Freq: 3H, Name: one, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">xticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(),</span><br><span class="line">                      freq=<span class="string">'D'</span>)</span><br><span class="line">plt.plot(citibike, linewidth=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Date'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Rentals'</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0,0.5,u&apos;Rentals&apos;)</code></pre><h1 id="算法链与管道"><a href="#算法链与管道" class="headerlink" title="算法链与管道"></a>算法链与管道</h1><h2 id="举例说明信息泄露"><a href="#举例说明信息泄露" class="headerlink" title="举例说明信息泄露"></a>举例说明信息泄露</h2><p>考虑一个假象的回归任务，包含从高斯分布中独立采样的100个样本和10000个特征。还从高斯分布中对响应进行采样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">rnd = np.random.RandomState(seed=<span class="number">0</span>)</span><br><span class="line">X = rnd.normal(size=(<span class="number">100</span>,<span class="number">10000</span>))</span><br><span class="line">y = rnd.normal(size=(<span class="number">100</span>,))</span><br></pre></td></tr></table></figure><p>考虑到创建数据集的方式，数据X与目标y之间没有任何关系，所以应该不可能从这个数据集中学到任何内容。现在首先利用SelectPercentile特征选择从10000个特征中选择信息量最大的特征，然后使用交叉验证对Ridge回归进行评估：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectPercentile, f_regression</span><br><span class="line"></span><br><span class="line">select = SelectPercentile(score_func=f_regression, percentile=<span class="number">5</span>).fit(X, y)</span><br><span class="line">X_selected = select.transform(X)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"X_selected.shape:&#123;&#125;"</span>.format(X_selected.shape)</span><br></pre></td></tr></table></figure><pre><code>X_selected.shape:(100L, 500L)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation accuracy(cv only on ridge):&#123;:.2f&#125;"</span>.format(np.mean(cross_val_score(Ridge(), X_selected, y, cv=<span class="number">5</span>)))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation accuracy(cv only on ridge):0.91</code></pre><p>交叉验证计算得到的平均R^2为0.91，表示这是一个非常线性的模型，但是这明显不对，因为数据是完全随机的。这里的特征选择从10000个随机特征中（碰巧）选出了与目标相关性非常好的一些特征。由于我们在交叉验证之外对特征选择进行拟合，所以能够找到在训练部分和测试部分都相关的特征。从测试部分泄露出去的信息包含的信息量非常大，导致得到非常不切实际的结果。将上面的结果和正确的交叉验证（使用管道）进行对比：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">pipe = Pipeline([(<span class="string">"select"</span>, SelectPercentile(score_func=f_regression,</span><br><span class="line">                                            percentile=<span class="number">5</span>)),</span><br><span class="line">                (<span class="string">"ridge"</span>,Ridge())])</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation accuracy (pipeline):&#123;:.2f&#125;"</span>.format(</span><br><span class="line">        np.mean(cross_val_score(pipe, X, y, cv=<span class="number">5</span>)))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation accuracy (pipeline):-0.25</code></pre><p>这一次得到了负的R^2分数，表示模型很差。利用管道，特征选择现在位于交叉验证循环内部，也就是说，仅适用数据的训练部分来选择特征，而不使用测试部分。特征选择找到的特征在训练集中与目标相关，但由于数据是完全随机的，这些特征在测试集中并不与目标相关。在这个例子中，修正特征选择中的数据泄露问题，结论也由“模型表现很好”变为“模型根本没有效果”。</p>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 特征处理 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>jupyter kernel error</title>
      <link href="/2018/06/05/jupyter-kernel-error/"/>
      <url>/2018/06/05/jupyter-kernel-error/</url>
      <content type="html"><![CDATA[<h2 id="问题出现"><a href="#问题出现" class="headerlink" title="问题出现"></a>问题出现</h2><p>之前因为Python2中混淆了编码问题，Python2的str默认是ascii编码，和unicode编码冲突，解决方法主要有两种，一种是用sys.setdefaultencoding(utf8)来进行强制转换，<br>还有一种是用区分了unicode str和byte array的Python3。<br>所以用Anaconda同时安装了Python2.7和Python3.6，但是jupyter notebook却报错如下：<br>    File”//anaconda/lib/python2.7/site-packages/jupyter_client/manager.py”, line 190, in _launch_kernel<br>    return launch_kernel(kernel_cmd, <strong>kw)<br>    File “//anaconda/lib/python2.7/site-packages/jupyter_client/launcher.py”, line 123, in launch_kernel<br>    proc = Popen(cmd, </strong>kwargs)<br>    File “//anaconda/lib/python2.7/subprocess.py”, line 710, in init<br>    errread, errwrite)<br>    File “//anaconda/lib/python2.7/subprocess.py”, line 1335, in _execute_child<br>    raise child_exception<br>    OSError: [Errno 2] No such file or director</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ol><li>首先使用jupyter kernelspec list查看安装的内核和位置</li><li>进入安装内核目录打开kernel.jason文件，查看Python编译器的路径是否正确</li><li>如果不正确python -m ipykernel install –user重新安装内核，如果有多个内核，如果你使用conda create -n python2 python=2,为Python2.7设置conda变量,那么在anacoda下使用activate pyhton2切换python环境，重新使用python -m ipykernel install –user安装内核</li><li>重启jupyter notebook即可</li></ol>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>数据分箱</title>
      <link href="/2018/05/20/%E6%95%B0%E6%8D%AE%E5%88%86%E7%AE%B1/"/>
      <url>/2018/05/20/%E6%95%B0%E6%8D%AE%E5%88%86%E7%AE%B1/</url>
      <content type="html"><![CDATA[<h1 id="数据分箱的适用情形"><a href="#数据分箱的适用情形" class="headerlink" title="数据分箱的适用情形"></a>数据分箱的适用情形</h1><p>数据分箱是下列情形下常用的方法：<br>1.某些数值自变量在测量时存在随机误差，需要对数值进行平滑以消除噪音。<br>2.有些数值自变量有大量不重复的取值，对于使用&lt;、&gt;、=等基本操作符的算法（如决策树）而言，如果能减少这些不重复取值的个数，就能提高算法的速度。<br>3.有些算法只能使用分类自变量，需要把数值变量离散化。<br>数据被归入几个分箱之后，可以用每个分箱内数值的均值、中位数或边界值来替代该分箱内各观测的数值，也可以把每个分箱作为离散化后的一个类别。例如，某个自变量的观测值为1，2.1，2.5，3.4，4，5.6，7，7.4，8.2.假设将它们分为三个分箱，（1，2.1，2.5），（3.4，4，5.6），（7，7.4，8.2），那么使用分箱均值替代后所得值为（1.87，1.87，1.87），（4.33，4.33，4.33），（7.53，7.53，7.53），使用分箱中位数替代后所得值为（2.1，2.1，2.1），（4，4，4），（7.4，7.4，7.4），使用边界值替代后所得值为（1，2.5，2.5），（3.4，3.4，5.6），（7，7，8.2）（每个观测值由其所属分箱的两个边界值中较近的值替代）。</p><h1 id="数据分箱的常用方法"><a href="#数据分箱的常用方法" class="headerlink" title="数据分箱的常用方法"></a>数据分箱的常用方法</h1><h2 id="有监督的卡方分箱法-ChiMerge"><a href="#有监督的卡方分箱法-ChiMerge" class="headerlink" title="有监督的卡方分箱法(ChiMerge)"></a>有监督的卡方分箱法(ChiMerge)</h2><p>自底向上的(即基于合并的)数据离散化方法。<br>它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。<br><a id="more"></a></p><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想:"></a>基本思想:</h3><p>对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p><p>这里需要注意初始化时需要对实例进行排序，在排序的基础上进行合并。</p><h3 id="卡方阈值的确定："><a href="#卡方阈值的确定：" class="headerlink" title="卡方阈值的确定："></a>卡方阈值的确定：</h3><p>根据显著性水平和自由度得到卡方值<br>自由度比类别数量小1。例如：有3类,自由度为2，则90%置信度(10%显著性水平)下，卡方的值为4.6。</p><h3 id="阈值的意义"><a href="#阈值的意义" class="headerlink" title="阈值的意义"></a>阈值的意义</h3><p>类别和属性独立时,有90%的可能性,计算得到的卡方值会小于4.6。 大于阈值4.6的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大,区间合并就会进行很多次,离散后的区间数量少、区间大。 </p><h3 id="注"><a href="#注" class="headerlink" title="注:"></a>注:</h3><p>1,ChiMerge算法推荐使用0.90、0.95、0.99置信度,最大区间数取10到15之间.<br>2,也可以不考虑卡方阈值,此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限,最多几个区间,最少几个区间。<br>3,对于类别型变量,需要分箱时需要按照某种方式进行排序。</p><h2 id="无监督分箱法"><a href="#无监督分箱法" class="headerlink" title="无监督分箱法:"></a>无监督分箱法:</h2><h3 id="等距划分、等频划分"><a href="#等距划分、等频划分" class="headerlink" title="等距划分、等频划分"></a>等距划分、等频划分</h3><h3 id="等距分箱"><a href="#等距分箱" class="headerlink" title="等距分箱"></a>等距分箱</h3><p>从最小值到最大值之间,均分为 N 等份, 这样, 如果 A,B 为最小最大值, 则每个区间的长度为 W=(B−A)/N , 则区间边界值为A+W,A+2W,….A+(N−1)W 。这里只考虑边界，每个等份里面的实例数量可能不等。 </p><h3 id="等频分箱"><a href="#等频分箱" class="headerlink" title="等频分箱"></a>等频分箱</h3><p>区间的边界值要经过选择,使得每个区间包含大致相等的实例数量。比如说 N=10 ,每个区间应该包含大约10%的实例。 </p><h3 id="以上两种算法的弊端"><a href="#以上两种算法的弊端" class="headerlink" title="以上两种算法的弊端"></a>以上两种算法的弊端</h3><p>比如,等宽区间划分,划分为5区间,最高工资为50000,则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反,所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型,落在正确区间里的偶然性很大。<br>我们对特征进行分箱后，需要对分箱后的每组（箱）进行woe编码，然后才能放进模型训练。</p>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何选取卷积生成序列中的有用部分</title>
      <link href="/2018/05/08/%E5%A6%82%E4%BD%95%E9%80%89%E5%8F%96%E5%8D%B7%E7%A7%AF%E7%94%9F%E6%88%90%E5%BA%8F%E5%88%97%E4%B8%AD%E7%9A%84%E6%9C%89%E7%94%A8%E9%83%A8%E5%88%86/"/>
      <url>/2018/05/08/%E5%A6%82%E4%BD%95%E9%80%89%E5%8F%96%E5%8D%B7%E7%A7%AF%E7%94%9F%E6%88%90%E5%BA%8F%E5%88%97%E4%B8%AD%E7%9A%84%E6%9C%89%E7%94%A8%E9%83%A8%E5%88%86/</url>
      <content type="html"><![CDATA[<h1 id="卷积原理"><a href="#卷积原理" class="headerlink" title="卷积原理"></a>卷积原理</h1><p>在信号与系统中，卷积积分是线性时不变系统分析的一个重要工具，具体是通过两个函数f和g生成第三个函数，表征函数f与g经过翻转和平移的重叠部分的面积。</p><p>卷积是两个变量在某范围内相乘后求和的结果。如果卷积的变量是序列x(n)和h(n)，则卷积的结果y(n)=x(n)<em>h(n)，其中星号</em>表示卷积。当时序n=0时，序列h(-i)是h(i)的时序i取反的结果；时序取反使得h(i)以纵轴为中心翻转180度，所以这种相乘后求和的计算法称为卷积和，简称卷积。另外，n是使h(-i)位移的量，不同的n对应不同的卷积结果。</p><p>如果卷积的变量是函数x(t)和h(t)，则卷积的计算变为y(t)=x(t)<em>h(t)，其中p是积分变量，积分也是求和，t是使函数h(-p)位移的量，星号</em>表示卷积。</p><p>已知信号长度为M的时间序列{x(i), i=1,M}与长度为N的近似理想脉冲响应滤波器{h(i),i=1,N}的卷积长度为M+N-1的序列{y(i),i=1,M+N-1}。实际上只有中间的M-N+1的长度是有效卷积的内容。而两端各有N/2的长度,是部分{h(i)}和{x(i)}乘积求和的结果，是两个脉冲函数，这两端的部分不是我们想要的。</p><p>在实际应用中，我们希望得到的{y(i)}，不仅能够在长度上与{x(i)}一致，而且在内容上也全部是有效的。MATLAB中conv(x,h,flag)的函数flag有三个选项“full”,”same”和“valid”。在默认情况下是“full”全部长度即M+N-1,完整的调用格式为conv(x,h,’full’)。 ‘valid’选项的长度只M-N+1, 其内容就是’same’和‘full’的中间M-N+1的部分。而‘same’中的前首尾两端各N/2不是我们想要的，’full’首尾两端各N的长度也不是我们想要的。<br><a id="more"></a></p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="1-周期延拓"><a href="#1-周期延拓" class="headerlink" title="1.周期延拓"></a>1.周期延拓</h2><p>将原始的{x(i)}中尾部N/2长度的数据接在其前面，并且将原始{x(i)}中头部的数据接在其后面，即完成了周期延拓。再使用conv(x, h, ‘valid’)就可以得到与原始{x(i)}在长度上相同，重要的是有效地卷积序列。</p><h2 id="2-多条数据首尾相接法"><a href="#2-多条数据首尾相接法" class="headerlink" title="2.多条数据首尾相接法"></a>2.多条数据首尾相接法</h2><p>如果{x(i)}是一条数据的长度，那么可将前条数据末尾的N/2长度接在当条数据的前面，将下一条数据头部的N/2长度接在当条的尾部，再进行conv(x, h, ‘valid’)就可以得到与原始{x(i)}在长度上相同，重要的是有效地卷积序列。<br>两种方法的差别在于有效部分开始的少量结果有一致，到中间有效部分的长度就是完全一样的了。</p><h2 id="matlab实现代码"><a href="#matlab实现代码" class="headerlink" title="matlab实现代码"></a>matlab实现代码</h2><p><code>`</code>matlab</p><pre><code>h=load(&apos;hfilter.dat&apos;); N=length(h);  d1=load(&apos;MZL3_20080101Hx.dat&apos;); d2=load(&apos;MZL3_20080102Hx.dat&apos;); d3=load(&apos;MZL3_20080103Hx.dat&apos;); M=length(d1);    Figure   % 用当条的数据周期延拓  dd1=[d2(M-N/2+1:end); d2; d2(1:N/2)]; % 使用三条的数据接起来  dd2=[d1(M-N/2+1:end); d2; d3(1:N/2)]; plot(dd1,&apos;r&apos;);hold on; plot(dd2);   figure  y1=conv(dd1,h,&apos;valid&apos;); y2=conv(dd2,h,&apos;valid&apos;);  plot(y1(1:N/2),&apos;ro&apos;);hold on; plot(y2(1:N/2),&apos;*&apos;);    figure   y11=conv(dd1,h,&apos;same&apos;); y22=conv(dd2,h,&apos;same&apos;); plot(y11,&apos;ro&apos;);hold on; plot(y22,&apos;*&apos;); figure  y111=conv(d2,h,&apos;same&apos;);  yy111=[zeros(N/2,1); y111;zeros(N/2,1)]; y222=conv(d3,h,&apos;full&apos;); plot(yy111 ,&apos;r&apos;);hold on; plot(y222);</code></pre><h2 id="C语言实现"><a href="#C语言实现" class="headerlink" title="C语言实现"></a>C语言实现</h2><p>在用c语言来实现带通滤波器时我们遇到了这个问题，最终使用的是只计算两个序列对齐时的卷积值当作总的卷积值，代码如下：</p><p><code>`</code>c</p><pre><code>int coefficient_bp[length_bp] ={165, 115, 121, 101, 56, -6, -71, -129, -173, -204, -231, -268, -321, -393, -467,-517, -510, -417, -226, 54, 386, 715, 981, 1129, 1129, 981, 715, 386, 54, -226,-417, -510, -517, -467, -393, -321, -268, -231, -204, -173, -129, -71, -6, 56,101, 121, 115, 165};int data_before_filter_array[length_bp];int num_fir_bp=0;int get_num_fir_bp(){    return num_fir_bp;}</code></pre><p>//<strong><strong><strong><strong><strong>*</strong></strong></strong></strong></strong>带通滤波函数<strong><strong><strong><strong><strong>**</strong></strong></strong></strong></strong></p><pre><code>int fir_bp(int data_before_filter){    int data_filtered_bp = 0;    if (num_fir_bp &lt; length_bp)      //输入数据数组未填满时，不计算结果    {        data_before_filter_array[num_fir_bp] = data_before_filter;        num_fir_bp++;        if (num_fir_bp == length_bp)  //输入数据数组刚好填满时，计算第一个结果        {            for (int i = 0; i &lt; length_bp; i++)            {                data_filtered_bp = data_filtered_bp + coefficient_bp[i] * data_before_filter_array[i];            }            data_filtered_bp=data_filtered_bp&gt;&gt;13;  //恢复原来的大小            // chenhao0620 限幅            data_filtered_bp = data_filtered_bp&gt;32767 ? 32767:(data_filtered_bp&lt;-32768?-32767:data_filtered_bp);            return data_filtered_bp;        }        return 0;    }    else    {        for (int i = 1; i &lt; length_bp; i++) //输入数据数组填满之后更新与移动        {            data_before_filter_array[i - 1] = data_before_filter_array[i];        }     //原代码有bug，提前将新数据灌进去了，导致46和47(最后两个)是一样的        data_before_filter_array[length_bp-1] = data_before_filter;        for (int i = 0; i &lt; length_bp; i++) //卷积计算        {            data_filtered_bp = data_filtered_bp + coefficient_bp[i] * data_before_filter_array[i];        }        data_filtered_bp=data_filtered_bp&gt;&gt;13;  //因为滤波器函数为了取整扩大了2^13倍，现在移位来恢复原来的大小    //限幅        data_filtered_bp = data_filtered_bp&gt;32767 ? 32767:(data_filtered_bp&lt;-32768?-32767:data_filtered_bp);        return data_filtered_bp;    }}</code></pre>]]></content>
      
      <categories>
          
          <category> 信号处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号处理 </tag>
            
            <tag> 卷积序列 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello bear.</title>
      <link href="/2018/05/05/hello-world/"/>
      <url>/2018/05/05/hello-world/</url>
      <content type="html"><![CDATA[<p>May the force be with you.<br>　　　　　　　　　　　　　- xiaoxiaoai</p><p>###<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1923424&auto=1&height=66"></iframe></p>]]></content>
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
