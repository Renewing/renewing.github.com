<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>TensorFlow笔记</title>
      <link href="/2018/06/13/TensorFlow%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/06/13/TensorFlow%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="神经网络的基本概念"><a href="#神经网络的基本概念" class="headerlink" title="神经网络的基本概念"></a>神经网络的基本概念</h2><ol><li>张量是多维数组（列表），用‘阶’表示张量的维度</li><li>TensorFlow的数据类型有tf.float32、tf.int32等</li><li>计算图：是承载一个或多个计算节点的一张图，只搭建网络，不运算</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>],[<span class="number">4.0</span>]])</span><br><span class="line">y = tf.matmul(x,w)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><pre><code>Tensor(&quot;MatMul_3:0&quot;, shape=(1, 1), dtype=float32)</code></pre><p>可以看到，print的结构显示y是一个张量，只搭建承载计算过程的计算图，并没有运算。<br><a id="more"></a></p><ol start="4"><li>会话：执行计算图中的节点运算<br>用with结构实现，语法如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure><pre><code>[[11.]]</code></pre><h2 id="神经网络的搭建"><a href="#神经网络的搭建" class="headerlink" title="神经网络的搭建"></a>神经网络的搭建</h2><ol><li>准备数据集，提取特征，作为输入喂给NN</li><li>搭建NN结构，从输入到输出（先搭建计算图，再用会话执行）<br> （NN前向传播算法–&gt;计算输出）</li><li>大量特征数据喂给NN，迭代优化NN参数<br> （NN反向传播算法–&gt;优化参数训练模型）</li><li>使用训练好的模型预测和分类</li></ol><h3 id="前向传播的tensorflow描述"><a href="#前向传播的tensorflow描述" class="headerlink" title="前向传播的tensorflow描述"></a>前向传播的tensorflow描述</h3><p>变量初始化、计算图节点运算都要用会话（with结构）实现</p><pre><code>with tf.Session() as sess:    sess.run()</code></pre><p>变量初始化：在sess.run函数中用tf.global_variables_initializer()汇总所有待优化变量</p><pre><code>init_op = tf.global_variables_initializer()sess.run(init_op)</code></pre><p>计算图节点运算：在sess.run函数中写入待运算的节点</p><pre><code>sess.run(y)</code></pre><p>用tf.placeholder占位，在sess.run函数中用feed_dict喂数据<br>喂一组数据：</p><pre><code>x = tf.placeholder(tf.float32, shape=(1,2))sess.run(y,feed_dict={x: [[0.5,0.6]]})</code></pre><p>喂多组数据：</p><pre><code>x = tf.placeholder(tf.float32, shape=(None,2))sess.run(y,feed_dict={x: [[0.5,0.6]],[[0.1,0.5]],[[0.4,0.2]]，[[0.8,0.7]]})</code></pre><h3 id="反向传播的tensorflow描述"><a href="#反向传播的tensorflow描述" class="headerlink" title="反向传播的tensorflow描述"></a>反向传播的tensorflow描述</h3><ul><li><p>反向传播：训练模型参数，在所有参数上用梯度下降，使NN模型在训练数据上的损失函数最小</p></li><li><p>损失函数（loss）：计算得到的预测值y与已知y_的差距</p></li><li><p>均方误差MSE：常用的损失函数计算方法，是求前向传播计算结果与已知答案之差的平方再求平均。</p><p>  loss_mse = tf.reduce_mean(tf.square(y_ - y)) </p></li><li><p>反向传播训练方法：以减小 loss 值为优化目标，有梯度下降、momentum 优化器、adam 优化器等优化方法。</p><p>  train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</p><p>  train_step=tf.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)</p><p>  train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)</p></li><li><p>学习率：决定每次参数更新的幅度。</p></li></ul><h2 id="神经网络优化"><a href="#神经网络优化" class="headerlink" title="神经网络优化"></a>神经网络优化</h2><ul><li>交叉熵(Cross Entropy)：表示两个概率分布之间的距离。交叉熵越大，两个概率分布距离越远，两个概率分布越相异；交叉熵越小，两个概率分布距离越近，两个概率分布越相似。交叉熵计算公式：𝐇(𝐲_ , 𝐲) = −∑𝐲_ ∗ 𝒍𝒐𝒈 𝒚</li></ul><p>用 Tensorflow 函数表示为</p><pre><code>ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0))) </code></pre><ul><li>softmax函数：将 n 分类的 n 个输出（y1,y2…yn）变为满足以下概率分布要求的函数。</li></ul><p>在 Tensorflow 中，一般让模型的输出经过 sofemax 函数，以获得输出分类的概率分布，再与标准<br>答案对比，求出交叉熵，得到损失函数，用如下函数实现：</p><pre><code>ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))cem = tf.reduce_mean(ce)</code></pre><ul><li>学习率 learning_rate：表示了每次参数更新的幅度大小。学习率过大，会导致待优化的参数在最小值附近波动，不收敛；学习率过小，会导致待优化的参数收敛缓慢。在训练过程中，参数的更新向着损失函数梯度下降的方向。</li></ul><p>参数的更新公式为：</p><pre><code>𝒘𝒏+𝟏 = 𝒘𝒏 − 𝒍𝒆𝒂𝒓𝒏𝒊𝒏𝒈_𝒓𝒂𝒕𝒆delta</code></pre><ul><li>指数衰减学习率：学习率随着训练轮数变化而动态更新</li></ul><p>用 Tensorflow 的函数表示为：</p><pre><code>global_step = tf.Variable(0, trainable=False)learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP, LEARNING_RATE_DECAY,staircase=True/False)</code></pre><p>   注：staircase为True时，表示global_step/learning rate step取整数，学习率阶梯型衰减</p><pre><code>staircase为False时，学习率是一条平滑下降的曲线</code></pre><ul><li>滑动平均：记录了一段时间内模型中所有参数 w 和 b 各自的平均值。利用滑动平均值可以增强模型的泛化能力。</li></ul><p>滑动平均值（影子）计算公式：</p><p>影子 = 衰减率 <em> 影子 +（1 - 衰减率）</em> 参数</p><p>其中，衰减率 = 𝐦𝐢𝐧 {𝑴𝑶𝑽𝑰𝑵𝑮𝑨𝑽𝑬𝑹𝑨𝑮𝑬𝑫𝑬𝑪𝑨𝒀,(𝟏+轮数)/(𝟏𝟎+轮数)}，影子初值=参数初值   </p><p>用 Tensorflow 函数表示为：</p><pre><code>ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY，global_step)</code></pre><p>其中，MOVING_AVERAGE_DECAY 表示滑动平均衰减率，一般会赋接近 1 的值，global_step 表示当前训练了多少轮。</p><pre><code>ema_op = ema.apply(tf.trainable_variables())</code></pre><p>其中，ema.apply()函数实现对括号内参数求滑动平均，tf.trainable_variables()函数实现把所有待训练参数汇总为列表。</p><pre><code>with tf.control_dependencies([train_step, ema_op]):     train_op = tf.no_op(name=&apos;train&apos;) </code></pre><ul><li><p>过拟合：神经网络模型在训练数据集上的准确率较高，在新的数据进行预测或分类时准确率较低，说明模型的泛化能力差。 </p></li><li><p>正则化：在损失函数中给每个参数 w 加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。 </p></li></ul><p>正则化计算方法：<br>① L1 正则化： 𝒍𝒐𝒔𝒔𝑳𝟏 = ∑𝒊|𝒘𝒊|</p><p>用 Tensorflow 函数表示:</p><pre><code>loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w)</code></pre><p>② L2 正则化： 𝒍𝒐𝒔𝒔𝑳𝟐 = ∑𝒊|𝒘𝒊|^𝟐</p><p>用 Tensorflow 函数表示:</p><pre><code>loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w)</code></pre><p>用 Tensorflow 函数实现正则化：</p><pre><code>tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w)loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;)) </code></pre><p>利用L1经过训练后，会让权重得到稀疏解，即权重中的一部分项为0，这种作用相当于对原始数据进行了特征选择；利用L2进行训练后，会让权重更趋于0，但不会得到稀疏结，这样做可以避免某些权重过大；两种正则做法都可以减轻过拟合，使训练结果更加具有鲁棒性。</p>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>梯度提升树(GBDT)概述及sklearn调参</title>
      <link href="/2018/06/11/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-GBDT-%E6%A6%82%E8%BF%B0%E5%8F%8Asklearn%E8%B0%83%E5%8F%82/"/>
      <url>/2018/06/11/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-GBDT-%E6%A6%82%E8%BF%B0%E5%8F%8Asklearn%E8%B0%83%E5%8F%82/</url>
      <content type="html"><![CDATA[<h1 id="GBDT概述"><a href="#GBDT概述" class="headerlink" title="GBDT概述"></a>GBDT概述</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>   GBDT隶属于Boosting，但不同于Adaboost，GBDT限定了弱学习器只能使用CART回归树模型。</p><h1 id="1-GBDT原理概述"><a href="#1-GBDT原理概述" class="headerlink" title="1.GBDT原理概述"></a>1.GBDT原理概述</h1><p>　　　　在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$, 损失函数是$L(y,f_{t-1}(x))$, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y,f_{t}(x) =L(y,f_{t-1}(x)+ h_t(x))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p><br><p>　　　　GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。</p><br><p>　　　　从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？</p><br><a id="more"></a><br><br><br># 2. GBDT的负梯度拟合<br><p>　　　　在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]<em>{f(x) = f</em>{t-1}\;\; (x)}$$</p><br><p>　　　　利用$(x_i,r_{ti})\;\; (i=1,2,..m)$,我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域$R_{tj}, j =1,2,…, J$。其中J为叶子节点的个数。</p><br><p>　　　　针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值$c_{tj}$如下：$$c_{tj} = \underbrace{arg\; min}<em>{c}\sum\limits</em>{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$$</p><br><p>　　　　这样我们就得到了本轮的决策树拟合函数如下：$$h_t(x) = \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p><br><p>　　　　从而本轮最终得到的强学习器的表达式如下：$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p><br><p>　　　　通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p><h1 id="3-GBDT回归算法"><a href="#3-GBDT回归算法" class="headerlink" title="3.GBDT回归算法"></a>3.GBDT回归算法</h1><p>　　　　好了，有了上面的思路，下面我们总结下GBDT的回归算法。为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们在下一节讲。</p><br><p>　　　　输入是训练集样本$T={(x_,y_1),(x_2,y_2), …(x_m,y_m)}$， 最大迭代次数T, 损失函数L。</p><br><p>　　　　输出是强学习器f(x)</p><br><p>　　　　1) 初始化弱学习器$$f_0(x) = \underbrace{arg\; min}<em>{c}\sum\limits</em>{i=1}^{m}L(y_i, c)$$</p><br><p>　　　　2) 对迭代轮数t=1,2,…T有：</p><br><p>　　　　　　a)对样本i=1,2，…m，计算负梯度$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]<em>{f(x) = f</em>{t-1}\;\; (x)}$$</p><br><p>　　　　　　b)利用$(x_i,r_{ti})\;\; (i=1,2,..m)$, 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为$R_{tj}, j =1,2,…, J$。其中J为回归树t的叶子节点的个数。</p><br><p>　　　　　　c) 对叶子区域j =1,2,..J,计算最佳拟合值$$c_{tj} = \underbrace{arg\; min}<em>{c}\sum\limits</em>{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$$</p><br><p>　　　　　　d) 更新强学习器$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p><br><p>　　　　3) 得到强学习器f(x)的表达式$$f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p><h1 id="4-GBDT分类算法"><a href="#4-GBDT分类算法" class="headerlink" title="4. GBDT分类算法"></a>4. GBDT分类算法</h1><p>　　　　这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。</p><br><p>　　　　为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。</p><h2 id="4-1-二元GBDT分类算法"><a href="#4-1-二元GBDT分类算法" class="headerlink" title="4.1 二元GBDT分类算法"></a>4.1 二元GBDT分类算法</h2><p>　　　　对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：$$L(y, f(x)) = log(1+ exp(-yf(x)))$$</p><br><p>　　　　其中$y \in{-1, +1}$。则此时的负梯度误差为$$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]<em>{f(x) = f</em>{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$$</p><br><p>　　　　对于生成的决策树，我们各个叶子节点的最佳残差拟合值为$$c_{tj} = \underbrace{arg\; min}<em>{c}\sum\limits</em>{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$$</p><br><p>　　　　由于上式比较难优化，我们一般使用近似值代替$$c_{tj} =\sum\limits_{x_i \in R_{tj}}r_{ti}\bigg /\sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$$</p><br><p>　　　　除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。</p><h2 id="4-2-多元GBDT分类算法"><a href="#4-2-多元GBDT分类算法" class="headerlink" title="4.2 多元GBDT分类算法"></a>4.2 多元GBDT分类算法</h2><p>　　　　多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：$$L(y, f(x)) = - \sum\limits_{k=1}^{K}y_klog\;p_k(x)$$</p><br><p>　　　　其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为：$$p_k(x) = exp(f_k(x)) \bigg / \sum\limits_{l=1}^{K}exp(f_l(x))$$</p><br><p>　　　　集合上两式，我们可以计算出第$t$轮的第$i$个样本对应类别$l$的负梯度误差为$$r_{til} =-\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1}\;\; (x)} = y_{il} - p_{l, t-1}(x_i)$$</p><br><p>　　　　观察上式可以看出，其实这里的误差就是样本$i$对应类别$l$的真实概率和$t-1$轮预测概率的差值。</p><br><p>　　　　对于生成的决策树，我们各个叶子节点的最佳残差拟合值为$$c_{tjl} = \underbrace{arg\; min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K}L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$$</p><br><p>　　　　由于上式比较难优化，我们一般使用近似值代替$$c_{tjl} = \frac{K-1}{K} \; \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$$</p><br><p>　　　　除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p><h1 id="5-GBDT常用损失函数"><a href="#5-GBDT常用损失函数" class="headerlink" title="5. GBDT常用损失函数"></a>5. GBDT常用损失函数</h1><p>　　　　这里我们再对常用的GBDT损失函数做一个总结。</p><br><p>　　　　对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种:</p><br><p>　　　　a) 如果是指数损失函数，则损失函数表达式为$$L(y, f(x)) = exp(-yf(x))$$</p><br><p>　　　　其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理篇。</p><br><p>　　　　b)如果是对数损失函数，分为二元分类和多元分类两种，参见4.1节和4.2节。</p><br><p>　　　　</p><br><p>　　　　对于回归算法，常用损失函数有如下4种:</p><br><p>　　　　a)均方差，这个是最常见的回归损失函数了$$L(y, f(x)) =(y-f(x))^2$$</p><br><p>　　　　b)绝对损失，这个损失函数也很常见$$L(y, f(x)) =|y-f(x)|$$</p><br><p>　　　　　　对应负梯度误差为：$$sign(y_i-f(x_i))$$</p><br><p>　　　　c)Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：</p><br><p>$$L(y, f(x))=\begin{cases}\frac{1}{2}(y-f(x))^2 \qquad {|y-f(x)| \leq \delta}\ \delta(|y-f(x)| - \frac{\delta}{2}) \qquad {|y-f(x)| \geq \delta} \end{cases}$$</p><br><p>　　　　对应的负梯度误差为：</p><br><p>$$r(y_i, f(x_i))= \begin{cases} y_i-f(x_i) \qquad {|y_i-f(x_i)| \leq \delta}\ \delta sign(y_i-f(x_i)) \qquad {|y_i-f(x_i)| \geq\ \delta} \end{cases}$$</p><br><p>　　　　d) 分位数损失。它对应的是分位数回归的损失函数，表达式为$$L(y, f(x)) =\sum\limits_{y \geq f(x)}\theta|y - f(x)| + \sum\limits_{y   \leq f(x)}(1-\theta)|y - f(x)|$$</p><br><p>　　　　　　其中$\theta$为分位数，需要我们在回归前指定。对应的负梯度误差为：</p><br><p>$$r(y_i, f(x_i))= \begin{cases} \theta \qquad { y_i \geq f(x_i)}\ \theta - 1 \qquad {y_i \leq f(x_i) } \end{cases}$$</p><br><p>　　　　对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p><h1 id="6-GBDT的正则化"><a href="#6-GBDT的正则化" class="headerlink" title="6. GBDT的正则化"></a>6. GBDT的正则化</h1><p>　　　　和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。</p><br><p>　　　　第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为$\nu$,对于前面的弱学习器的迭代$$f_{k}(x) = f_{k-1}(x) + h_k(x) $$</p><br><p>　　　　如果我们加上了正则化项，则有$$f_{k}(x) = f_{k-1}(x) + \nu h_k(x) $$</p><br><p>　　　　$\nu$的取值范围为$0 \leq \nu \leq 1 $。对于同样的训练集学习效果，较小的$\nu$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p><br><p></p><br><p>　　　　第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。</p><br><p>　　　　使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</p><br><p></p><br><p>　　　　第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了。</p><h1 id="7-GBDT小结"><a href="#7-GBDT小结" class="headerlink" title="7. GBDT小结"></a>7. GBDT小结</h1><p>　　　　由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。</p><br><p>　　　　最后总结下GBDT的优缺点。</p><br><p>　　　　GBDT主要的优点有：</p><br><p>　　　　1) 可以灵活处理各种类型的数据，包括连续值和离散值。</p><br><p>　　　　2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。</p><br><p>　　　　3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p><br><p>　　　　GBDT的主要缺点有：</p><br><p>　　　　1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p><br><p></p><h1 id="8-GBDT-sklearn调参"><a href="#8-GBDT-sklearn调参" class="headerlink" title="8.GBDT sklearn调参"></a>8.GBDT sklearn调参</h1><h2 id="8-1-GBDT类Boosting框架参数"><a href="#8-1-GBDT类Boosting框架参数" class="headerlink" title="8.1 GBDT类Boosting框架参数"></a>8.1 GBDT类Boosting框架参数</h2><p>　　　　1)<strong>n_estimators</strong>: 弱学习器的最大迭代次数，或者说最大的弱学习器的个数。太小，容易欠拟合，太大，容易过拟合。默认是100。实际调参过程中常常将n_estimators和learning_rate一起考虑</p><br><p>　　　　2)<strong>learning_rate</strong>: 即每个弱学习器的权重缩减系数$\nu$，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为$f_{k}(x) = f_{k-1}(x) + \nu h_k(x)$。$\nu$的取值范围为$0 \leq \nu \leq 1 $。对于同样的训练集拟合效果，较小的$\nu$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的$\nu$开始调参，默认是1。</p><br><p>　　　　3)<strong>subsample</strong>: 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。</p><br><p>　　　　4)<strong>init</strong>: 即我们的初始化的时候的弱学习器，拟合对应原理篇里面的$f_{0}(x)$，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。</p><br><p>　　　　5)<strong>loss: </strong>即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。</p><br><p>　　　　　　对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。</p><br><p>　　　　　　对于回归模型，有均方差”ls”,绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。</p><br><p>　　　　6)<strong>alpha：</strong>这个参数只有GradientBoostingRegressor有，当我们使用Huber损失”huber”和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。</p><h2 id="8-2-GBDT类弱学习器参数"><a href="#8-2-GBDT类弱学习器参数" class="headerlink" title="8.2 GBDT类弱学习器参数"></a>8.2 GBDT类弱学习器参数</h2><p>　　　　这里我们再对GBDT的类库弱学习器的重要参数做一个总结。由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似。如果你已经很熟悉决策树算法的调参，那么这一节基本可以跳过。不熟悉的朋友可以继续看下去。</p><br><p>　　　　1)划分时考虑的最大特征数<strong>max_features</strong>:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数；如果是”log2”意味着划分时最多考虑$log_2N$个特征；如果是”sqrt”或者”auto”意味着划分时最多考虑$\sqrt{N}$个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p><br><p>　　　　2)决策树最大深度<strong>max_depth</strong>:默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</p><br><p>　　　　3)内部节点再划分所需最小样本数<strong>min_samples_split</strong>:这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p><br><p>　　　　4)叶子节点最少样本数<strong>min_samples_leaf</strong>:这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p><br><p>　　　　5）叶子节点最小的样本权重和<strong>min_weight_fraction_leaf</strong>：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p><br><p>　　　　6)最大叶子节点数<strong>max_leaf_nodes</strong>:通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</p><br><p>　　　　7)节点划分最小不纯度<strong>min_impurity_split:</strong>这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点。一般不推荐改动默认值1e-7。</p>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Introduction to ML with Python</title>
      <link href="/2018/06/05/Introduction-to-ML-with-Python/"/>
      <url>/2018/06/05/Introduction-to-ML-with-Python/</url>
      <content type="html"><![CDATA[<h1 id="sklearn中的交叉验证"><a href="#sklearn中的交叉验证" class="headerlink" title="sklearn中的交叉验证"></a>sklearn中的交叉验证</h1><h2 id="标准交叉验证"><a href="#标准交叉验证" class="headerlink" title="标准交叉验证"></a>标准交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">cancer = load_breast_cancer()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">logerg = LogisticRegression()</span><br><span class="line"></span><br><span class="line">scores = cross_val_score(logerg, iris.data, iris.target)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores: &#123;&#125;"</span>.format(scores)</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores: [ 0.96078431  0.92156863  0.95833333]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scores = cross_val_score(logerg, iris.data, iris.target, cv=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores: &#123;&#125;"</span>.format(scores)</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores: [ 1.          0.96666667  0.93333333  0.9         1.        ]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Average cross-validation score: &#123;:.2f&#125;"</span>.format(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>Average cross-validation score: 0.96</code></pre><a id="more"></a>    <h2 id="对交叉验证的更多控制"><a href="#对交叉验证的更多控制" class="headerlink" title="对交叉验证的更多控制"></a>对交叉验证的更多控制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(</span><br><span class="line">       cross_val_score(logerg, iris.data, iris.target, cv=kfold))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 1.          0.93333333  0.43333333  0.96666667  0.43333333]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kfold = KFold(n_splits=<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(</span><br><span class="line">       cross_val_score(logerg, iris.data, iris.target, cv=kfold))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 0.  0.  0.]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kfold = KFold(n_splits=<span class="number">3</span>, shuffle=<span class="keyword">True</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(</span><br><span class="line">       cross_val_score(logerg, iris.data, iris.target, cv=kfold))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 0.9   0.96  0.96]</code></pre><h2 id="留一法交叉验证"><a href="#留一法交叉验证" class="headerlink" title="留一法交叉验证"></a>留一法交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> LeaveOneOut</span><br><span class="line">loo = LeaveOneOut()</span><br><span class="line">scores = cross_val_score(logerg, iris.data, iris.target, cv=loo)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Number of cv iterations: "</span>, len(scores)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Mean accuracy: &#123;:.2f&#125;"</span>.format(scores.mean())</span><br></pre></td></tr></table></figure><pre><code>Number of cv iterations:  150Mean accuracy: 0.95</code></pre><h2 id="打乱划分交叉验证"><a href="#打乱划分交叉验证" class="headerlink" title="打乱划分交叉验证"></a>打乱划分交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line">shuffle_split = ShuffleSplit(test_size=<span class="number">.5</span>, train_size=<span class="number">.5</span>, n_splits=<span class="number">10</span>)</span><br><span class="line">scores = cross_val_score(logerg, iris.data, iris.target, cv=shuffle_split)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(scores)</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 0.97333333  0.97333333  0.92        0.88        0.97333333  0.96  0.94666667  0.96        0.77333333  0.89333333]</code></pre><h2 id="分组交叉验证"><a href="#分组交叉验证" class="headerlink" title="分组交叉验证"></a>分组交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GroupKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="comment"># 创建模拟数据集</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">12</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 假设前3个样本属于同一组，接下来的4个样本属于同一组，以此类推</span></span><br><span class="line">groups = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">scores = cross_val_score(logerg, X, y, groups, cv=GroupKFold(n_splits=<span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation scores:\n&#123;&#125;"</span>.format(scores)</span><br></pre></td></tr></table></figure><pre><code>Cross-validation scores:[ 0.75        0.8         0.66666667]</code></pre><h1 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h1><h2 id="简单网格搜索"><a href="#简单网格搜索" class="headerlink" title="简单网格搜索"></a>简单网格搜索</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    iris.data, iris.target, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Size of training set: &#123;&#125;  size of the test set: &#123;&#125;"</span>.format(</span><br><span class="line">    X_train.shape[<span class="number">0</span>], X_test.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">best_score = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> gamma <span class="keyword">in</span> [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]:</span><br><span class="line">    <span class="keyword">for</span> C <span class="keyword">in</span> [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]:</span><br><span class="line">        svm = SVC(gamma=gamma,C=C)</span><br><span class="line">        svm.fit(X_train, y_train)</span><br><span class="line">        score = svm.score(X_test, y_test)</span><br><span class="line">        <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            best_parameters = &#123;<span class="string">'C'</span>: C, <span class="string">'gamma'</span>: gamma&#125;</span><br><span class="line">            </span><br><span class="line"><span class="keyword">print</span> <span class="string">"Best score: &#123;:.2f&#125;"</span>.format(best_score)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Best parameters: &#123;&#125;"</span>.format(best_parameters)</span><br></pre></td></tr></table></figure><pre><code>Size of training set: 112  size of the test set: 38Best score: 0.97Best parameters: {&apos;C&apos;: 100, &apos;gamma&apos;: 0.001}</code></pre><ul><li>training set: Model fitting</li><li>validation set: Patameter selection</li><li>test set: Evaluation</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line">mglearn.plots.plot_grid_search_overview()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">450</span>, n_features=<span class="number">2</span>, centers=<span class="number">2</span>, cluster_std=[<span class="number">7.0</span>, <span class="number">2</span>],</span><br><span class="line">                random_state=<span class="number">22</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">svc = SVC(gamma=<span class="number">.05</span>).fit(X_train, y_train)</span><br><span class="line">precision, recall, thresholds = precision_recall_curve(</span><br><span class="line">    y_test, svc.decision_function(X_test))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mglearn.plots.plot_decision_threshold()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> classification_report(y_test, svc.predict(X_test))</span><br></pre></td></tr></table></figure><pre><code>             precision    recall  f1-score   support          0       0.96      0.80      0.87        60          1       0.81      0.96      0.88        53avg / total       0.89      0.88      0.88       113</code></pre><h1 id="预处理与缩放"><a href="#预处理与缩放" class="headerlink" title="预处理与缩放"></a>预处理与缩放</h1><h2 id="应用数据变换"><a href="#应用数据变换" class="headerlink" title="应用数据变换"></a>应用数据变换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,</span><br><span class="line">                                                   random_state=<span class="number">1</span>)</span><br><span class="line">print(X_train.shape)</span><br><span class="line">print(X_test.shape)</span><br></pre></td></tr></table></figure><pre><code>(426L, 30L)(143L, 30L)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">scaler.fit(X_train)</span><br><span class="line">MinMaxScaler(copy=<span class="keyword">True</span>, feature_range=(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#变换数据</span></span><br><span class="line">X_train_scaled = scaler.transform(X_train)</span><br><span class="line">X_test_scaled = scaler.transform(X_test)</span><br></pre></td></tr></table></figure><h1 id="降维、特征提取和流形学习"><a href="#降维、特征提取和流形学习" class="headerlink" title="降维、特征提取和流形学习"></a>降维、特征提取和流形学习</h1><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span>  sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(cancer.data)</span><br><span class="line">X_scaled = scaler.transform(cancer.data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X_scaled)</span><br><span class="line"></span><br><span class="line">X_pca = pca.transform(X_scaled)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Original shape: &#123;&#125;"</span>.format(X_scaled.shape)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Reduced shape: &#123;&#125;"</span>.format(X_pca.shape)</span><br></pre></td></tr></table></figure><pre><code>Original shape: (569L, 30L)Reduced shape: (569L, 2L)</code></pre><h2 id="非负矩阵分解-NMF"><a href="#非负矩阵分解-NMF" class="headerlink" title="非负矩阵分解 NMF"></a>非负矩阵分解 NMF</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line">mglearn.plots.plot_nmf_illustration()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> NMF</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</span><br><span class="line"></span><br><span class="line">people = fetch_lfw_people(min_faces_per_person=<span class="number">20</span>, resize=<span class="number">0.7</span>)</span><br><span class="line">X_people = people.data[mask]</span><br><span class="line">y_people = people.target[mask]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_people, y_people, stratify=y_people, random_state=<span class="number">0</span>)</span><br><span class="line">nmf = NMF(n_cpmponents=<span class="number">15</span>, random_state=<span class="number">0</span>)</span><br><span class="line">nmf.fit(X_train)</span><br><span class="line">X_train_nmf = nmf.transform(X_train)</span><br><span class="line">X_test_nmf = nmf.transform(X_test)</span><br><span class="line"></span><br><span class="line">fix, axes = plt.subplots(<span class="number">3</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>,<span class="number">12</span>),</span><br><span class="line">                        subplot_kw=&#123;<span class="string">'xticks'</span>:(), <span class="string">'yticks'</span>:()&#125;)</span><br><span class="line"><span class="keyword">for</span> i, (component,ax) <span class="keyword">in</span> enumerate(zip(nmf.components_, axes.ravel())):</span><br><span class="line">    ax.imshow(component.reshape(image_shape))</span><br><span class="line">    ax.set_title(<span class="string">"&#123;&#125;.component"</span>.format(i))</span><br></pre></td></tr></table></figure><h2 id="用t-SNE进行流形学习"><a href="#用t-SNE进行流形学习" class="headerlink" title="用t-SNE进行流形学习"></a>用t-SNE进行流形学习</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line">digits = load_digits()</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">10</span>,<span class="number">5</span>),</span><br><span class="line">                        subplot_kw=&#123;<span class="string">'xticks'</span>:(), <span class="string">'yticks'</span>:()&#125;)</span><br><span class="line"><span class="keyword">for</span> ax, img <span class="keyword">in</span> zip(axes.ravel(), digits.images):</span><br><span class="line">    ax.imshow(img)</span><br></pre></td></tr></table></figure><h1 id="数据表示与特征工程"><a href="#数据表示与特征工程" class="headerlink" title="数据表示与特征工程"></a>数据表示与特征工程</h1><h2 id="分箱、离散化、线性模型与树"><a href="#分箱、离散化、线性模型与树" class="headerlink" title="分箱、离散化、线性模型与树"></a>分箱、离散化、线性模型与树</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">X, y = mglearn.datasets.make_wave(n_samples=<span class="number">100</span>)</span><br><span class="line">line = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>, endpoint=<span class="keyword">False</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">reg = DecisionTreeRegressor(min_samples_split=<span class="number">3</span>).fit(X, y)</span><br><span class="line">plt.plot(line, reg.predict(line), label=<span class="string">"decision tree"</span>)</span><br><span class="line"></span><br><span class="line">reg = LinearRegression().fit(X, y)</span><br><span class="line">plt.plot(line, reg.predict(line), label=<span class="string">"linear regression"</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(X[:,<span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Regression output"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Input feature"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x103c4da0&gt;</code></pre><h2 id="自动化特征选择"><a href="#自动化特征选择" class="headerlink" title="自动化特征选择"></a>自动化特征选择</h2><h3 id="单变量统计"><a href="#单变量统计" class="headerlink" title="单变量统计"></a>单变量统计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">citibike = mglearn.datasets.load_citibike()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Citi Bike data:\n&#123;&#125;"</span>.format(citibike.head())</span><br></pre></td></tr></table></figure><pre><code>Citi Bike data:starttime2015-08-01 00:00:00     3.02015-08-01 03:00:00     0.02015-08-01 06:00:00     9.02015-08-01 09:00:00    41.02015-08-01 12:00:00    39.0Freq: 3H, Name: one, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">xticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(),</span><br><span class="line">                      freq=<span class="string">'D'</span>)</span><br><span class="line">plt.plot(citibike, linewidth=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Date'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Rentals'</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0,0.5,u&apos;Rentals&apos;)</code></pre><h1 id="算法链与管道"><a href="#算法链与管道" class="headerlink" title="算法链与管道"></a>算法链与管道</h1><h2 id="举例说明信息泄露"><a href="#举例说明信息泄露" class="headerlink" title="举例说明信息泄露"></a>举例说明信息泄露</h2><p>考虑一个假象的回归任务，包含从高斯分布中独立采样的100个样本和10000个特征。还从高斯分布中对响应进行采样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">rnd = np.random.RandomState(seed=<span class="number">0</span>)</span><br><span class="line">X = rnd.normal(size=(<span class="number">100</span>,<span class="number">10000</span>))</span><br><span class="line">y = rnd.normal(size=(<span class="number">100</span>,))</span><br></pre></td></tr></table></figure><p>考虑到创建数据集的方式，数据X与目标y之间没有任何关系，所以应该不可能从这个数据集中学到任何内容。现在首先利用SelectPercentile特征选择从10000个特征中选择信息量最大的特征，然后使用交叉验证对Ridge回归进行评估：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectPercentile, f_regression</span><br><span class="line"></span><br><span class="line">select = SelectPercentile(score_func=f_regression, percentile=<span class="number">5</span>).fit(X, y)</span><br><span class="line">X_selected = select.transform(X)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"X_selected.shape:&#123;&#125;"</span>.format(X_selected.shape)</span><br></pre></td></tr></table></figure><pre><code>X_selected.shape:(100L, 500L)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation accuracy(cv only on ridge):&#123;:.2f&#125;"</span>.format(np.mean(cross_val_score(Ridge(), X_selected, y, cv=<span class="number">5</span>)))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation accuracy(cv only on ridge):0.91</code></pre><p>交叉验证计算得到的平均R^2为0.91，表示这是一个非常线性的模型，但是这明显不对，因为数据是完全随机的。这里的特征选择从10000个随机特征中（碰巧）选出了与目标相关性非常好的一些特征。由于我们在交叉验证之外对特征选择进行拟合，所以能够找到在训练部分和测试部分都相关的特征。从测试部分泄露出去的信息包含的信息量非常大，导致得到非常不切实际的结果。将上面的结果和正确的交叉验证（使用管道）进行对比：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">pipe = Pipeline([(<span class="string">"select"</span>, SelectPercentile(score_func=f_regression,</span><br><span class="line">                                            percentile=<span class="number">5</span>)),</span><br><span class="line">                (<span class="string">"ridge"</span>,Ridge())])</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Cross-validation accuracy (pipeline):&#123;:.2f&#125;"</span>.format(</span><br><span class="line">        np.mean(cross_val_score(pipe, X, y, cv=<span class="number">5</span>)))</span><br></pre></td></tr></table></figure><pre><code>Cross-validation accuracy (pipeline):-0.25</code></pre><p>这一次得到了负的R^2分数，表示模型很差。利用管道，特征选择现在位于交叉验证循环内部，也就是说，仅适用数据的训练部分来选择特征，而不使用测试部分。特征选择找到的特征在训练集中与目标相关，但由于数据是完全随机的，这些特征在测试集中并不与目标相关。在这个例子中，修正特征选择中的数据泄露问题，结论也由“模型表现很好”变为“模型根本没有效果”。</p>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 特征处理 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>jupyter kernel error</title>
      <link href="/2018/06/05/jupyter-kernel-error/"/>
      <url>/2018/06/05/jupyter-kernel-error/</url>
      <content type="html"><![CDATA[<h2 id="问题出现"><a href="#问题出现" class="headerlink" title="问题出现"></a>问题出现</h2><p>之前因为Python2中混淆了编码问题，Python2的str默认是ascii编码，和unicode编码冲突，解决方法主要有两种，一种是用sys.setdefaultencoding(utf8)来进行强制转换，<br>还有一种是用区分了unicode str和byte array的Python3。<br>所以用Anaconda同时安装了Python2.7和Python3.6，但是jupyter notebook却报错如下：<br>    File”//anaconda/lib/python2.7/site-packages/jupyter_client/manager.py”, line 190, in _launch_kernel<br>    return launch_kernel(kernel_cmd, <strong>kw)<br>    File “//anaconda/lib/python2.7/site-packages/jupyter_client/launcher.py”, line 123, in launch_kernel<br>    proc = Popen(cmd, </strong>kwargs)<br>    File “//anaconda/lib/python2.7/subprocess.py”, line 710, in init<br>    errread, errwrite)<br>    File “//anaconda/lib/python2.7/subprocess.py”, line 1335, in _execute_child<br>    raise child_exception<br>    OSError: [Errno 2] No such file or director</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ol><li>首先使用jupyter kernelspec list查看安装的内核和位置</li><li>进入安装内核目录打开kernel.jason文件，查看Python编译器的路径是否正确</li><li>如果不正确python -m ipykernel install –user重新安装内核，如果有多个内核，如果你使用conda create -n python2 python=2,为Python2.7设置conda变量,那么在anacoda下使用activate pyhton2切换python环境，重新使用python -m ipykernel install –user安装内核</li><li>重启jupyter notebook即可</li></ol>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>数据分箱</title>
      <link href="/2018/05/20/%E6%95%B0%E6%8D%AE%E5%88%86%E7%AE%B1/"/>
      <url>/2018/05/20/%E6%95%B0%E6%8D%AE%E5%88%86%E7%AE%B1/</url>
      <content type="html"><![CDATA[<h1 id="数据分箱的适用情形"><a href="#数据分箱的适用情形" class="headerlink" title="数据分箱的适用情形"></a>数据分箱的适用情形</h1><p>数据分箱是下列情形下常用的方法：<br>1.某些数值自变量在测量时存在随机误差，需要对数值进行平滑以消除噪音。<br>2.有些数值自变量有大量不重复的取值，对于使用&lt;、&gt;、=等基本操作符的算法（如决策树）而言，如果能减少这些不重复取值的个数，就能提高算法的速度。<br>3.有些算法只能使用分类自变量，需要把数值变量离散化。<br>数据被归入几个分箱之后，可以用每个分箱内数值的均值、中位数或边界值来替代该分箱内各观测的数值，也可以把每个分箱作为离散化后的一个类别。例如，某个自变量的观测值为1，2.1，2.5，3.4，4，5.6，7，7.4，8.2.假设将它们分为三个分箱，（1，2.1，2.5），（3.4，4，5.6），（7，7.4，8.2），那么使用分箱均值替代后所得值为（1.87，1.87，1.87），（4.33，4.33，4.33），（7.53，7.53，7.53），使用分箱中位数替代后所得值为（2.1，2.1，2.1），（4，4，4），（7.4，7.4，7.4），使用边界值替代后所得值为（1，2.5，2.5），（3.4，3.4，5.6），（7，7，8.2）（每个观测值由其所属分箱的两个边界值中较近的值替代）。</p><h1 id="数据分箱的常用方法"><a href="#数据分箱的常用方法" class="headerlink" title="数据分箱的常用方法"></a>数据分箱的常用方法</h1><h2 id="有监督的卡方分箱法-ChiMerge"><a href="#有监督的卡方分箱法-ChiMerge" class="headerlink" title="有监督的卡方分箱法(ChiMerge)"></a>有监督的卡方分箱法(ChiMerge)</h2><p>自底向上的(即基于合并的)数据离散化方法。<br>它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。<br><a id="more"></a></p><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想:"></a>基本思想:</h3><p>对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p><p>这里需要注意初始化时需要对实例进行排序，在排序的基础上进行合并。</p><h3 id="卡方阈值的确定："><a href="#卡方阈值的确定：" class="headerlink" title="卡方阈值的确定："></a>卡方阈值的确定：</h3><p>根据显著性水平和自由度得到卡方值<br>自由度比类别数量小1。例如：有3类,自由度为2，则90%置信度(10%显著性水平)下，卡方的值为4.6。</p><h3 id="阈值的意义"><a href="#阈值的意义" class="headerlink" title="阈值的意义"></a>阈值的意义</h3><p>类别和属性独立时,有90%的可能性,计算得到的卡方值会小于4.6。 大于阈值4.6的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大,区间合并就会进行很多次,离散后的区间数量少、区间大。 </p><h3 id="注"><a href="#注" class="headerlink" title="注:"></a>注:</h3><p>1,ChiMerge算法推荐使用0.90、0.95、0.99置信度,最大区间数取10到15之间.<br>2,也可以不考虑卡方阈值,此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限,最多几个区间,最少几个区间。<br>3,对于类别型变量,需要分箱时需要按照某种方式进行排序。</p><h2 id="无监督分箱法"><a href="#无监督分箱法" class="headerlink" title="无监督分箱法:"></a>无监督分箱法:</h2><h3 id="等距划分、等频划分"><a href="#等距划分、等频划分" class="headerlink" title="等距划分、等频划分"></a>等距划分、等频划分</h3><h3 id="等距分箱"><a href="#等距分箱" class="headerlink" title="等距分箱"></a>等距分箱</h3><p>从最小值到最大值之间,均分为 N 等份, 这样, 如果 A,B 为最小最大值, 则每个区间的长度为 W=(B−A)/N , 则区间边界值为A+W,A+2W,….A+(N−1)W 。这里只考虑边界，每个等份里面的实例数量可能不等。 </p><h3 id="等频分箱"><a href="#等频分箱" class="headerlink" title="等频分箱"></a>等频分箱</h3><p>区间的边界值要经过选择,使得每个区间包含大致相等的实例数量。比如说 N=10 ,每个区间应该包含大约10%的实例。 </p><h3 id="以上两种算法的弊端"><a href="#以上两种算法的弊端" class="headerlink" title="以上两种算法的弊端"></a>以上两种算法的弊端</h3><p>比如,等宽区间划分,划分为5区间,最高工资为50000,则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反,所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型,落在正确区间里的偶然性很大。<br>我们对特征进行分箱后，需要对分箱后的每组（箱）进行woe编码，然后才能放进模型训练。</p>]]></content>
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何选取卷积生成序列中的有用部分</title>
      <link href="/2018/05/08/%E5%A6%82%E4%BD%95%E9%80%89%E5%8F%96%E5%8D%B7%E7%A7%AF%E7%94%9F%E6%88%90%E5%BA%8F%E5%88%97%E4%B8%AD%E7%9A%84%E6%9C%89%E7%94%A8%E9%83%A8%E5%88%86/"/>
      <url>/2018/05/08/%E5%A6%82%E4%BD%95%E9%80%89%E5%8F%96%E5%8D%B7%E7%A7%AF%E7%94%9F%E6%88%90%E5%BA%8F%E5%88%97%E4%B8%AD%E7%9A%84%E6%9C%89%E7%94%A8%E9%83%A8%E5%88%86/</url>
      <content type="html"><![CDATA[<h1 id="卷积原理"><a href="#卷积原理" class="headerlink" title="卷积原理"></a>卷积原理</h1><p>在信号与系统中，卷积积分是线性时不变系统分析的一个重要工具，具体是通过两个函数f和g生成第三个函数，表征函数f与g经过翻转和平移的重叠部分的面积。</p><p>卷积是两个变量在某范围内相乘后求和的结果。如果卷积的变量是序列x(n)和h(n)，则卷积的结果y(n)=x(n)<em>h(n)，其中星号</em>表示卷积。当时序n=0时，序列h(-i)是h(i)的时序i取反的结果；时序取反使得h(i)以纵轴为中心翻转180度，所以这种相乘后求和的计算法称为卷积和，简称卷积。另外，n是使h(-i)位移的量，不同的n对应不同的卷积结果。</p><p>如果卷积的变量是函数x(t)和h(t)，则卷积的计算变为y(t)=x(t)<em>h(t)，其中p是积分变量，积分也是求和，t是使函数h(-p)位移的量，星号</em>表示卷积。</p><p>已知信号长度为M的时间序列{x(i), i=1,M}与长度为N的近似理想脉冲响应滤波器{h(i),i=1,N}的卷积长度为M+N-1的序列{y(i),i=1,M+N-1}。实际上只有中间的M-N+1的长度是有效卷积的内容。而两端各有N/2的长度,是部分{h(i)}和{x(i)}乘积求和的结果，是两个脉冲函数，这两端的部分不是我们想要的。</p><p>在实际应用中，我们希望得到的{y(i)}，不仅能够在长度上与{x(i)}一致，而且在内容上也全部是有效的。MATLAB中conv(x,h,flag)的函数flag有三个选项“full”,”same”和“valid”。在默认情况下是“full”全部长度即M+N-1,完整的调用格式为conv(x,h,’full’)。 ‘valid’选项的长度只M-N+1, 其内容就是’same’和‘full’的中间M-N+1的部分。而‘same’中的前首尾两端各N/2不是我们想要的，’full’首尾两端各N的长度也不是我们想要的。<br><a id="more"></a></p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="1-周期延拓"><a href="#1-周期延拓" class="headerlink" title="1.周期延拓"></a>1.周期延拓</h2><p>将原始的{x(i)}中尾部N/2长度的数据接在其前面，并且将原始{x(i)}中头部的数据接在其后面，即完成了周期延拓。再使用conv(x, h, ‘valid’)就可以得到与原始{x(i)}在长度上相同，重要的是有效地卷积序列。</p><h2 id="2-多条数据首尾相接法"><a href="#2-多条数据首尾相接法" class="headerlink" title="2.多条数据首尾相接法"></a>2.多条数据首尾相接法</h2><p>如果{x(i)}是一条数据的长度，那么可将前条数据末尾的N/2长度接在当条数据的前面，将下一条数据头部的N/2长度接在当条的尾部，再进行conv(x, h, ‘valid’)就可以得到与原始{x(i)}在长度上相同，重要的是有效地卷积序列。<br>两种方法的差别在于有效部分开始的少量结果有一致，到中间有效部分的长度就是完全一样的了。</p><h2 id="matlab实现代码"><a href="#matlab实现代码" class="headerlink" title="matlab实现代码"></a>matlab实现代码</h2><p><code>`</code>matlab</p><pre><code>h=load(&apos;hfilter.dat&apos;); N=length(h);  d1=load(&apos;MZL3_20080101Hx.dat&apos;); d2=load(&apos;MZL3_20080102Hx.dat&apos;); d3=load(&apos;MZL3_20080103Hx.dat&apos;); M=length(d1);    Figure   % 用当条的数据周期延拓  dd1=[d2(M-N/2+1:end); d2; d2(1:N/2)]; % 使用三条的数据接起来  dd2=[d1(M-N/2+1:end); d2; d3(1:N/2)]; plot(dd1,&apos;r&apos;);hold on; plot(dd2);   figure  y1=conv(dd1,h,&apos;valid&apos;); y2=conv(dd2,h,&apos;valid&apos;);  plot(y1(1:N/2),&apos;ro&apos;);hold on; plot(y2(1:N/2),&apos;*&apos;);    figure   y11=conv(dd1,h,&apos;same&apos;); y22=conv(dd2,h,&apos;same&apos;); plot(y11,&apos;ro&apos;);hold on; plot(y22,&apos;*&apos;); figure  y111=conv(d2,h,&apos;same&apos;);  yy111=[zeros(N/2,1); y111;zeros(N/2,1)]; y222=conv(d3,h,&apos;full&apos;); plot(yy111 ,&apos;r&apos;);hold on; plot(y222);</code></pre><h2 id="C语言实现"><a href="#C语言实现" class="headerlink" title="C语言实现"></a>C语言实现</h2><p>在用c语言来实现带通滤波器时我们遇到了这个问题，最终使用的是只计算两个序列对齐时的卷积值当作总的卷积值，代码如下：</p><p><code>`</code>c</p><pre><code>int coefficient_bp[length_bp] ={165, 115, 121, 101, 56, -6, -71, -129, -173, -204, -231, -268, -321, -393, -467,-517, -510, -417, -226, 54, 386, 715, 981, 1129, 1129, 981, 715, 386, 54, -226,-417, -510, -517, -467, -393, -321, -268, -231, -204, -173, -129, -71, -6, 56,101, 121, 115, 165};int data_before_filter_array[length_bp];int num_fir_bp=0;int get_num_fir_bp(){    return num_fir_bp;}</code></pre><p>//<strong><strong><strong><strong><strong>*</strong></strong></strong></strong></strong>带通滤波函数<strong><strong><strong><strong><strong>**</strong></strong></strong></strong></strong></p><pre><code>int fir_bp(int data_before_filter){    int data_filtered_bp = 0;    if (num_fir_bp &lt; length_bp)      //输入数据数组未填满时，不计算结果    {        data_before_filter_array[num_fir_bp] = data_before_filter;        num_fir_bp++;        if (num_fir_bp == length_bp)  //输入数据数组刚好填满时，计算第一个结果        {            for (int i = 0; i &lt; length_bp; i++)            {                data_filtered_bp = data_filtered_bp + coefficient_bp[i] * data_before_filter_array[i];            }            data_filtered_bp=data_filtered_bp&gt;&gt;13;  //恢复原来的大小            // chenhao0620 限幅            data_filtered_bp = data_filtered_bp&gt;32767 ? 32767:(data_filtered_bp&lt;-32768?-32767:data_filtered_bp);            return data_filtered_bp;        }        return 0;    }    else    {        for (int i = 1; i &lt; length_bp; i++) //输入数据数组填满之后更新与移动        {            data_before_filter_array[i - 1] = data_before_filter_array[i];        }     //原代码有bug，提前将新数据灌进去了，导致46和47(最后两个)是一样的        data_before_filter_array[length_bp-1] = data_before_filter;        for (int i = 0; i &lt; length_bp; i++) //卷积计算        {            data_filtered_bp = data_filtered_bp + coefficient_bp[i] * data_before_filter_array[i];        }        data_filtered_bp=data_filtered_bp&gt;&gt;13;  //因为滤波器函数为了取整扩大了2^13倍，现在移位来恢复原来的大小    //限幅        data_filtered_bp = data_filtered_bp&gt;32767 ? 32767:(data_filtered_bp&lt;-32768?-32767:data_filtered_bp);        return data_filtered_bp;    }}</code></pre>]]></content>
      
      <categories>
          
          <category> 信号处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号处理 </tag>
            
            <tag> 卷积序列 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello bear.</title>
      <link href="/2018/05/05/hello-world/"/>
      <url>/2018/05/05/hello-world/</url>
      <content type="html"><![CDATA[<p>May the force be with you.<br>　　　　　　　　　　　　　- Renewing</p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=545314781&auto=1&height=66"></iframe>]]></content>
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
